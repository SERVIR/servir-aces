{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACES (Agricultural Classification and Estimation Service)","text":"<p>ACES (Agricultural Classification and Estimation Service) is a Python module for generating training data and training machine learning models for remote sensing applications. It provides functionalities for data processing, data loading from Earth Engine, feature extraction, and model training.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Data loading and processing from Earth Engine.</li> <li>Generation of training data for machine learning models.</li> <li>Training and evaluation of machine learning models (DNN, CNN, UNET).</li> <li>Support for remote sensing feature extraction.</li> <li>Integration with Apache Beam for data processing.</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>Define all your configuration in <code>.env</code> file. An example of the file is provided as <code>.env.example</code>.</p> <p>Here's an example of how to use the ACES module:</p> <pre><code>from aces.config import Config\nfrom aces.model_trainer import ModelTrainer\n\nif __name__ == \"__main__\":\n    config = Config()\n    trainer = ModelTrainer(config)\n    trainer.train_model()\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions to ACES are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request on the GitHub repository.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GPL-3 License - see the LICENSE file for details.</p>"},{"location":"data_processor/","title":"data_processor module","text":""},{"location":"data_processor/#aces.data_processor","title":"<code>aces.data_processor</code>","text":"<p>ACES Data Processor Module This module provides functions for data input/output and preprocessing for the ACES project.</p>"},{"location":"data_processor/#aces.data_processor.DataProcessor","title":"<code>DataProcessor</code>","text":"<p>ACES Data processor Class</p> <p>This class provides functions for data input/output and preprocessing for the ACES project.</p> Source code in <code>aces/data_processor.py</code> <pre><code>class DataProcessor:\n\"\"\"\n    ACES Data processor Class\n\n    This class provides functions for data input/output and preprocessing for the ACES project.\n    \"\"\"\n\n    @staticmethod\n    @tf.autograph.experimental.do_not_convert\n    def create_tfrecord_from_file(filename: str) -&gt; tf.data.TFRecordDataset:\n\"\"\"\n        Create a TensorFlow Dataset from a TFRecord file.\n\n        Parameters:\n        filename (str): The filename of the TFRecord file.\n\n        Returns:\n        tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.\n        \"\"\"\n        return tf.data.TFRecordDataset(filename, compression_type=\"GZIP\")\n\n    @staticmethod\n    @tf.autograph.experimental.do_not_convert\n    def get_single_sample(x):\n\"\"\"\n        Get a single sample from a dataset.\n\n        Parameters:\n        x: The input tensor.\n\n        Returns:\n        tf.Tensor: The single sample.\n        \"\"\"\n        return tf.numpy_function(lambda _: 1, inp=[x], Tout=tf.int64)\n\n    @staticmethod\n    def filter_good_patches(patch):\n\"\"\"\n        Filter patches to remove those with NaN or infinite values.\n\n        Parameters:\n        patch (np.ndarray): The patch to filter.\n\n        Returns:\n        bool: True if the patch has no NaN or infinite values, False otherwise.\n        \"\"\"\n        # the getdownload url has field names so we're using view here\n        has_nan = np.isnan(np.sum(patch.view(np.float32)))\n        has_inf = np.isinf(np.sum(patch.view(np.float32)))\n        if has_nan or has_inf:\n            return False\n        return True\n\n    @staticmethod\n    def calculate_n_samples(**config):\n\"\"\"\n        Calculate the number of samples in the training, testing, and validation datasets.\n\n        Parameters:\n        **config: The configuration settings.\n\n        Returns:\n        int: The number of training samples.\n        int: The number of testing samples.\n        int: The number of validation samples.\n        \"\"\"\n        parser = partial(DataProcessor.parse_tfrecord_multi_label,\n                         patch_size=config.get(\"PATCH_SHAPE_SINGLE\"),\n                         features=config.get(\"FEATURES\"),\n                         labels=config.get(\"LABELS\"))\n        tupler = partial(DataProcessor.to_tuple_multi_label, depth=config.get(\"OUT_CLASS_NUM\"), x_only=True)\n\n        tf_training_records = tf.data.Dataset.list_files(f\"{str(config.get('TRAINING_DIR'))}/*\")\\\n                                             .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_training_records = tf_training_records.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_training_records = tf_training_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        d0_training_records = tf_training_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n        n_training_records = d0_training_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n        tf_testing_records = tf.data.Dataset.list_files(f\"{str(config.get('TESTING_DIR'))}/*\")\\\n                                            .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_testing_records = tf_testing_records.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_testing_records = tf_testing_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        d0_testing_records = tf_testing_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n        n_testing_records = d0_testing_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n        tf_validation_records = tf.data.Dataset.list_files(f\"{str(config.get('VALIDATION_DIR'))}/*\")\\\n                                               .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_validation_records = tf_validation_records.map(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        tf_validation_records = tf_validation_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        d0_validation_records = tf_validation_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        n_validation_records = d0_validation_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n        return n_training_records, n_testing_records, n_validation_records\n\n\n    @staticmethod\n    def print_dataset_info(dataset: tf.data.Dataset, dataset_name: str) -&gt; None:\n\"\"\"\n        Print information about a dataset.\n\n        Parameters:\n        dataset (tf.data.Dataset): The dataset to print information about.\n        dataset_name (str): The name of the dataset.\n        \"\"\"\n        logging.info(dataset_name)\n        for inputs, outputs in dataset.take(1):\n            try:\n                logging.info(f\"inputs: {inputs.dtype.name} {inputs.shape}\")\n                print(inputs)\n                logging.info(f\"outputs: {outputs.dtype.name} {outputs.shape}\")\n                print(outputs)\n            except:\n                logging.info(f\" &gt; inputs:\")\n                for name, values in inputs.items():\n                    logging.info(f\"    {name}: {values.dtype.name} {values.shape}\")\n                logging.info(f\" &gt; outputs: {outputs.dtype.name} {outputs.shape}\")\n\n    @staticmethod\n    @tf.function\n    def random_transform(dataset: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Apply random transformations to a dataset.\n\n        Parameters:\n        dataset (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The transformed dataset.\n        \"\"\"\n        x = tf.random.uniform(())\n\n        if x &lt; 0.14:\n            return tf.concat([dataset, tf.image.flip_left_right(dataset)], 0)\n        elif tf.math.logical_and(x &gt;= 0.14, x &lt; 0.28):\n            return tf.concat([dataset, tf.image.flip_left_right(dataset)], 0)\n        elif tf.math.logical_and(x &gt;= 0.28, x &lt; 0.42):\n            return tf.concat([dataset, tf.image.flip_left_right(tf.image.flip_up_down(dataset))], 0)\n        elif tf.math.logical_and(x &gt;= 0.42, x &lt; 0.56):\n            return tf.concat([dataset, tf.image.rot90(dataset, k=1)], 0)\n        elif tf.math.logical_and(x &gt;= 0.56, x &lt; 0.70):\n            return tf.concat([dataset, tf.image.rot90(dataset, k=2)], 0)\n        elif tf.math.logical_and(x &gt;= 0.70, x &lt; 0.84):\n            return tf.concat([dataset, tf.image.rot90(dataset, k=3)], 0)\n        else:\n            return tf.concat([dataset, tf.image.flip_left_right(tf.image.rot90(dataset, k=2))], 0)\n\n    @staticmethod\n    @tf.function\n    def flip_inputs_up_down(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Flip the inputs of a dataset vertically.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The dataset with flipped inputs.\n        \"\"\"\n        return tf.image.flip_up_down(inputs)\n\n    @staticmethod\n    @tf.function\n    def flip_inputs_left_right(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Flip the inputs of a dataset horizontally.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The dataset with flipped inputs.\n        \"\"\"\n        return tf.image.flip_left_right(inputs)\n\n    @staticmethod\n    @tf.function\n    def transpose_inputs(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Transpose the inputs of a dataset.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The transposed dataset.\n        \"\"\"\n        flip_up_down = tf.image.flip_up_down(inputs)\n        transpose = tf.image.flip_left_right(flip_up_down)\n        return transpose\n\n    @staticmethod\n    @tf.function\n    def rotate_inputs_90(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Rotate the inputs of a dataset by 90 degrees.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The dataset with rotated inputs.\n        \"\"\"\n        return tf.image.rot90(inputs, k=1)\n\n    @staticmethod\n    @tf.function\n    def rotate_inputs_180(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Rotate the inputs of a dataset by 180 degrees.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The dataset with rotated inputs.\n        \"\"\"\n        return tf.image.rot90(inputs, k=2)\n\n    @staticmethod\n    @tf.function\n    def rotate_inputs_270(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Rotate the inputs of a dataset by 270 degrees.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n\n        Returns:\n        tf.Tensor: The dataset with rotated inputs.\n        \"\"\"\n        return tf.image.rot90(inputs, k=3)\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n\"\"\"\n        Parse a TFRecord example.\n\n        Parameters:\n        example_proto (tf.Tensor): The example to parse.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n        tf.data.Dataset: The parsed dataset.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        inputs = tf.io.parse_single_example(example_proto, proto_struct)\n        inputs_list = [inputs.get(key) for key in keys]\n        stacked = tf.stack(inputs_list, axis=0)\n        stacked = tf.transpose(stacked, [1, 2, 0])\n        return tf.data.Dataset.from_tensors(stacked)\n\n    @staticmethod\n    @tf.function\n    def to_tuple(dataset: tf.Tensor, n_features: int = None, inverse_labels: bool = False) -&gt; tuple:\n\"\"\"\n        Convert a dataset to a tuple of features and labels.\n\n        Parameters:\n        dataset (tf.Tensor): The input dataset.\n        n_features (int, optional): The number of features. Default is None.\n        inverse_labels (bool, optional): Whether to inverse the labels. Default is False.\n\n        Returns:\n        tuple: A tuple containing the features and labels.\n        \"\"\"\n        features = dataset[:, :, :, :n_features]\n        labels = dataset[:, :, :, n_features:]\n        if inverse_labels:\n            labels_inverse = tf.math.abs(labels - 1)\n            labels = tf.concat([labels_inverse, labels], axis=-1)\n        return features, labels\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_with_name(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n\"\"\"\n        Parse a TFRecord example with named features.\n\n        Parameters:\n        example_proto (tf.Tensor): The example to parse.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n        tf.data.Dataset: The parsed dataset.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        return tf.io.parse_single_example(example_proto, proto_struct)\n\n    @staticmethod\n    @tf.function\n    def to_tuple_with_name(inputs: tf.Tensor, features: list = None, labels: list = None, n_classes: int = 1) -&gt; tuple:\n\"\"\"\n        Convert inputs with named features to a tuple of features and one-hot encoded labels.\n\n        Parameters:\n        inputs (tf.Tensor): The input dataset.\n        features (list, optional): The list of feature names. Default is None.\n        labels (list, optional): The list of label names. Default is None.\n        n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.\n\n        Returns:\n        tuple: A tuple containing the features and one-hot encoded labels.\n        \"\"\"\n        return (\n            {name: inputs[name] for name in features},\n            tf.one_hot(tf.cast(inputs[labels[0]], tf.uint8), n_classes)\n        )\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_dnn(example_proto: tf.Tensor, features: list = None, labels: list = None) -&gt; tuple:\n\"\"\"\n        Parse a TFRecord example for DNN models.\n\n        Parameters:\n        example_proto (tf.Tensor): The example to parse.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n        tuple: A tuple containing the parsed features and labels.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n        label = parsed_features.pop(labels[0])\n        label = tf.cast(label, tf.int32)\n        return parsed_features, label\n\n    @staticmethod\n    @tf.function\n    def to_tuple_dnn(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n\"\"\"\n        Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n        Parameters:\n        dataset (dict): The input dataset.\n        label (tf.Tensor): The label.\n        depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n        Returns:\n        tuple: A tuple containing the features and one-hot encoded labels.\n        \"\"\"\n        return tf.transpose(list(dataset.values())), tf.one_hot(indices=label, depth=depth)\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_multi_label(example_proto: tf.data.Dataset, patch_size: int, features: list = None, labels: list = None) -&gt; tuple:\n\"\"\"\n        Parse a TFRecord example with multiple labels.\n\n        Parameters:\n        example_proto (tf.data.Dataset): The example to parse.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n        tuple: A tuple containing the parsed features and labels.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n        label = parsed_features.pop(labels[0])\n        return parsed_features, label\n\n    @staticmethod\n    @tf.function\n    def to_tuple_multi_label(dataset: dict, label: tf.Tensor, depth: int = 1, x_only: bool = False) -&gt; tuple:\n\"\"\"\n        Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n        Parameters:\n        dataset (tuple): The input dataset.\n        n_labels (int, optional): The number of labels. Default is 1.\n\n        Returns:\n        tuple: A tuple containing the features and multi-hot encoded labels.\n        \"\"\"\n        label = tf.cast(label, tf.uint8)\n        label = tf.one_hot(indices=label, depth=depth)\n        parsed_dataset = tf.transpose(list(dataset.values()))\n        if x_only:\n            return parsed_dataset\n        return parsed_dataset, label\n\n    @staticmethod\n    def _get_dataset(files: list, features: list, labels: list, patch_shape: list, batch_size: int, buffer_size: int = 1000, training: bool = False, **kwargs) -&gt; tf.data.Dataset:\n\"\"\"\n        Get a TFRecord dataset.\n\n        Parameters:\n        filenames (list): The list of file names.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n        batch_size (int, optional): The batch size. Default is 1.\n        shuffle (bool, optional): Whether to shuffle the dataset. Default is False.\n        n_labels (int, optional): The number of labels. Default is 1.\n        num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n        drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n        cache (bool, optional): Whether to cache the dataset. Default is False.\n\n        Returns:\n        tf.data.Dataset: The TFRecord dataset.\n        \"\"\"\n        dnn = kwargs.get('dnn', False)\n        inverse_labels = kwargs.get('inverse_labels', False)\n        depth = kwargs.get('depth', len(labels))\n        multi_label_unet = kwargs.get('multi_label_unet', False)\n        dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')\n\n        if dnn:\n            parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n            split_data = partial(DataProcessor.to_tuple_dnn, depth=depth)\n            dataset = dataset.map(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.batch(batch_size)\n            return dataset\n\n        if multi_label_unet:\n            parser = partial(DataProcessor.parse_tfrecord_multi_label, features=features, labels=labels, patch_shape=patch_shape)\n            split_data = partial(DataProcessor.to_tuple_multi_label, n_features=len(features), depth=depth)\n            dataset = dataset.interleave(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            if training:\n                dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True).batch(batch_size) \\\n                                 .map(DataProcessor.random_transform, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n                                 .map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            else:\n                dataset = dataset.batch(batch_size).map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n            return dataset\n\n        parser = partial(DataProcessor.parse_tfrecord, features=features, labels=labels)\n        split_data = partial(DataProcessor.to_tuple, n_features=len(features), inverse_labels=inverse_labels)\n        dataset = dataset.interleave(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        if training:\n            dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True).batch(batch_size) \\\n                             .map(DataProcessor.random_transform, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n                             .map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        else:\n            dataset = dataset.batch(batch_size).map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        return dataset\n\n    @staticmethod\n    def get_dataset(pattern: str, features: list, labels: list, patch_size: int, batch_size: int, n_classes: int = 1) -&gt; tf.data.Dataset:\n\"\"\"\n        Get a TFRecord dataset.\n\n        Parameters:\n        filenames (list): The list of file names.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n        batch_size (int, optional): The batch size. Default is 1.\n        shuffle(bool, optional): Whether to shuffle the dataset. Default is False.\n        n_labels (int, optional): The number of labels. Default is 1.\n        num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n        drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n        cache (bool, optional): Whether to cache the dataset. Default is False.\n\n        Returns:\n        tf.data.Dataset: The TFRecord dataset.\n        \"\"\"\n        logging.info(f\"Loading dataset from {pattern}\")\n        logging.info(f\"list_files: {tf.data.Dataset.list_files(pattern)}\")\n\n        parser = partial(DataProcessor.parse_tfrecord_multi_label, patch_size=patch_size, features=features, labels=labels)\n        tupler = partial(DataProcessor.to_tuple_multi_label, depth=n_classes)\n\n        dataset = tf.data.Dataset.list_files(pattern).interleave(DataProcessor.create_tfrecord_from_file)\n        dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        # dataset = dataset.cache()\n        dataset = dataset.shuffle(512)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        return dataset\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.calculate_n_samples","title":"<code>calculate_n_samples(**config)</code>  <code>staticmethod</code>","text":"<p>Calculate the number of samples in the training, testing, and validation datasets.</p> <p>**config: The configuration settings.</p> <p>int: The number of training samples. int: The number of testing samples. int: The number of validation samples.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef calculate_n_samples(**config):\n\"\"\"\n    Calculate the number of samples in the training, testing, and validation datasets.\n\n    Parameters:\n    **config: The configuration settings.\n\n    Returns:\n    int: The number of training samples.\n    int: The number of testing samples.\n    int: The number of validation samples.\n    \"\"\"\n    parser = partial(DataProcessor.parse_tfrecord_multi_label,\n                     patch_size=config.get(\"PATCH_SHAPE_SINGLE\"),\n                     features=config.get(\"FEATURES\"),\n                     labels=config.get(\"LABELS\"))\n    tupler = partial(DataProcessor.to_tuple_multi_label, depth=config.get(\"OUT_CLASS_NUM\"), x_only=True)\n\n    tf_training_records = tf.data.Dataset.list_files(f\"{str(config.get('TRAINING_DIR'))}/*\")\\\n                                         .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_training_records = tf_training_records.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_training_records = tf_training_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    d0_training_records = tf_training_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    n_training_records = d0_training_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n    tf_testing_records = tf.data.Dataset.list_files(f\"{str(config.get('TESTING_DIR'))}/*\")\\\n                                        .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_testing_records = tf_testing_records.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_testing_records = tf_testing_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    d0_testing_records = tf_testing_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    n_testing_records = d0_testing_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n    tf_validation_records = tf.data.Dataset.list_files(f\"{str(config.get('VALIDATION_DIR'))}/*\")\\\n                                           .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_validation_records = tf_validation_records.map(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    tf_validation_records = tf_validation_records.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    d0_validation_records = tf_validation_records.map(DataProcessor.get_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    n_validation_records = d0_validation_records.reduce(np.int64(0), lambda x, y: x + y).numpy()\n\n    return n_training_records, n_testing_records, n_validation_records\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.create_tfrecord_from_file","title":"<code>create_tfrecord_from_file(filename)</code>  <code>staticmethod</code>","text":"<p>Create a TensorFlow Dataset from a TFRecord file.</p> <p>filename (str): The filename of the TFRecord file.</p> <p>tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.autograph.experimental.do_not_convert\ndef create_tfrecord_from_file(filename: str) -&gt; tf.data.TFRecordDataset:\n\"\"\"\n    Create a TensorFlow Dataset from a TFRecord file.\n\n    Parameters:\n    filename (str): The filename of the TFRecord file.\n\n    Returns:\n    tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.\n    \"\"\"\n    return tf.data.TFRecordDataset(filename, compression_type=\"GZIP\")\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.filter_good_patches","title":"<code>filter_good_patches(patch)</code>  <code>staticmethod</code>","text":"<p>Filter patches to remove those with NaN or infinite values.</p> <p>patch (np.ndarray): The patch to filter.</p> <p>bool: True if the patch has no NaN or infinite values, False otherwise.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef filter_good_patches(patch):\n\"\"\"\n    Filter patches to remove those with NaN or infinite values.\n\n    Parameters:\n    patch (np.ndarray): The patch to filter.\n\n    Returns:\n    bool: True if the patch has no NaN or infinite values, False otherwise.\n    \"\"\"\n    # the getdownload url has field names so we're using view here\n    has_nan = np.isnan(np.sum(patch.view(np.float32)))\n    has_inf = np.isinf(np.sum(patch.view(np.float32)))\n    if has_nan or has_inf:\n        return False\n    return True\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.flip_inputs_left_right","title":"<code>flip_inputs_left_right(inputs)</code>  <code>staticmethod</code>","text":"<p>Flip the inputs of a dataset horizontally.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The dataset with flipped inputs.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef flip_inputs_left_right(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Flip the inputs of a dataset horizontally.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The dataset with flipped inputs.\n    \"\"\"\n    return tf.image.flip_left_right(inputs)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.flip_inputs_up_down","title":"<code>flip_inputs_up_down(inputs)</code>  <code>staticmethod</code>","text":"<p>Flip the inputs of a dataset vertically.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The dataset with flipped inputs.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef flip_inputs_up_down(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Flip the inputs of a dataset vertically.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The dataset with flipped inputs.\n    \"\"\"\n    return tf.image.flip_up_down(inputs)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.get_dataset","title":"<code>get_dataset(pattern, features, labels, patch_size, batch_size, n_classes=1)</code>  <code>staticmethod</code>","text":"<p>Get a TFRecord dataset.</p> <p>filenames (list): The list of file names. patch_size (int): The size of the patch. features (list, optional): The list of feature names to include. Default is None. labels (list, optional): The list of label names to include. Default is None. batch_size (int, optional): The batch size. Default is 1. shuffle(bool, optional): Whether to shuffle the dataset. Default is False. n_labels (int, optional): The number of labels. Default is 1. num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE. drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False. cache (bool, optional): Whether to cache the dataset. Default is False.</p> <p>tf.data.Dataset: The TFRecord dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef get_dataset(pattern: str, features: list, labels: list, patch_size: int, batch_size: int, n_classes: int = 1) -&gt; tf.data.Dataset:\n\"\"\"\n    Get a TFRecord dataset.\n\n    Parameters:\n    filenames (list): The list of file names.\n    patch_size (int): The size of the patch.\n    features (list, optional): The list of feature names to include. Default is None.\n    labels (list, optional): The list of label names to include. Default is None.\n    batch_size (int, optional): The batch size. Default is 1.\n    shuffle(bool, optional): Whether to shuffle the dataset. Default is False.\n    n_labels (int, optional): The number of labels. Default is 1.\n    num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n    drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n    cache (bool, optional): Whether to cache the dataset. Default is False.\n\n    Returns:\n    tf.data.Dataset: The TFRecord dataset.\n    \"\"\"\n    logging.info(f\"Loading dataset from {pattern}\")\n    logging.info(f\"list_files: {tf.data.Dataset.list_files(pattern)}\")\n\n    parser = partial(DataProcessor.parse_tfrecord_multi_label, patch_size=patch_size, features=features, labels=labels)\n    tupler = partial(DataProcessor.to_tuple_multi_label, depth=n_classes)\n\n    dataset = tf.data.Dataset.list_files(pattern).interleave(DataProcessor.create_tfrecord_from_file)\n    dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    # dataset = dataset.cache()\n    dataset = dataset.shuffle(512)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.get_single_sample","title":"<code>get_single_sample(x)</code>  <code>staticmethod</code>","text":"<p>Get a single sample from a dataset.</p> <p>x: The input tensor.</p> <p>tf.Tensor: The single sample.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.autograph.experimental.do_not_convert\ndef get_single_sample(x):\n\"\"\"\n    Get a single sample from a dataset.\n\n    Parameters:\n    x: The input tensor.\n\n    Returns:\n    tf.Tensor: The single sample.\n    \"\"\"\n    return tf.numpy_function(lambda _: 1, inp=[x], Tout=tf.int64)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord","title":"<code>parse_tfrecord(example_proto, patch_size, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example.</p> <p>example_proto (tf.Tensor): The example to parse. patch_size (int): The size of the patch. features (list, optional): The list of feature names to include. Default is None. labels (list, optional): The list of label names to include. Default is None.</p> <p>tf.data.Dataset: The parsed dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n\"\"\"\n    Parse a TFRecord example.\n\n    Parameters:\n    example_proto (tf.Tensor): The example to parse.\n    patch_size (int): The size of the patch.\n    features (list, optional): The list of feature names to include. Default is None.\n    labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n    tf.data.Dataset: The parsed dataset.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    inputs = tf.io.parse_single_example(example_proto, proto_struct)\n    inputs_list = [inputs.get(key) for key in keys]\n    stacked = tf.stack(inputs_list, axis=0)\n    stacked = tf.transpose(stacked, [1, 2, 0])\n    return tf.data.Dataset.from_tensors(stacked)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_dnn","title":"<code>parse_tfrecord_dnn(example_proto, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example for DNN models.</p> <p>example_proto (tf.Tensor): The example to parse. features (list, optional): The list of feature names to include. Default is None. labels (list, optional): The list of label names to include. Default is None.</p> <p>tuple: A tuple containing the parsed features and labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_dnn(example_proto: tf.Tensor, features: list = None, labels: list = None) -&gt; tuple:\n\"\"\"\n    Parse a TFRecord example for DNN models.\n\n    Parameters:\n    example_proto (tf.Tensor): The example to parse.\n    features (list, optional): The list of feature names to include. Default is None.\n    labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n    tuple: A tuple containing the parsed features and labels.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n    label = parsed_features.pop(labels[0])\n    label = tf.cast(label, tf.int32)\n    return parsed_features, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_multi_label","title":"<code>parse_tfrecord_multi_label(example_proto, patch_size, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example with multiple labels.</p> <p>example_proto (tf.data.Dataset): The example to parse. patch_size (int): The size of the patch. features (list, optional): The list of feature names to include. Default is None. labels (list, optional): The list of label names to include. Default is None.</p> <p>tuple: A tuple containing the parsed features and labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_multi_label(example_proto: tf.data.Dataset, patch_size: int, features: list = None, labels: list = None) -&gt; tuple:\n\"\"\"\n    Parse a TFRecord example with multiple labels.\n\n    Parameters:\n    example_proto (tf.data.Dataset): The example to parse.\n    patch_size (int): The size of the patch.\n    features (list, optional): The list of feature names to include. Default is None.\n    labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n    tuple: A tuple containing the parsed features and labels.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n    label = parsed_features.pop(labels[0])\n    return parsed_features, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_with_name","title":"<code>parse_tfrecord_with_name(example_proto, patch_size, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example with named features.</p> <p>example_proto (tf.Tensor): The example to parse. patch_size (int): The size of the patch. features (list, optional): The list of feature names to include. Default is None. labels (list, optional): The list of label names to include. Default is None.</p> <p>tf.data.Dataset: The parsed dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_with_name(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n\"\"\"\n    Parse a TFRecord example with named features.\n\n    Parameters:\n    example_proto (tf.Tensor): The example to parse.\n    patch_size (int): The size of the patch.\n    features (list, optional): The list of feature names to include. Default is None.\n    labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n    tf.data.Dataset: The parsed dataset.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    return tf.io.parse_single_example(example_proto, proto_struct)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.print_dataset_info","title":"<code>print_dataset_info(dataset, dataset_name)</code>  <code>staticmethod</code>","text":"<p>Print information about a dataset.</p> <p>dataset (tf.data.Dataset): The dataset to print information about. dataset_name (str): The name of the dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef print_dataset_info(dataset: tf.data.Dataset, dataset_name: str) -&gt; None:\n\"\"\"\n    Print information about a dataset.\n\n    Parameters:\n    dataset (tf.data.Dataset): The dataset to print information about.\n    dataset_name (str): The name of the dataset.\n    \"\"\"\n    logging.info(dataset_name)\n    for inputs, outputs in dataset.take(1):\n        try:\n            logging.info(f\"inputs: {inputs.dtype.name} {inputs.shape}\")\n            print(inputs)\n            logging.info(f\"outputs: {outputs.dtype.name} {outputs.shape}\")\n            print(outputs)\n        except:\n            logging.info(f\" &gt; inputs:\")\n            for name, values in inputs.items():\n                logging.info(f\"    {name}: {values.dtype.name} {values.shape}\")\n            logging.info(f\" &gt; outputs: {outputs.dtype.name} {outputs.shape}\")\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.random_transform","title":"<code>random_transform(dataset)</code>  <code>staticmethod</code>","text":"<p>Apply random transformations to a dataset.</p> <p>dataset (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The transformed dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef random_transform(dataset: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Apply random transformations to a dataset.\n\n    Parameters:\n    dataset (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The transformed dataset.\n    \"\"\"\n    x = tf.random.uniform(())\n\n    if x &lt; 0.14:\n        return tf.concat([dataset, tf.image.flip_left_right(dataset)], 0)\n    elif tf.math.logical_and(x &gt;= 0.14, x &lt; 0.28):\n        return tf.concat([dataset, tf.image.flip_left_right(dataset)], 0)\n    elif tf.math.logical_and(x &gt;= 0.28, x &lt; 0.42):\n        return tf.concat([dataset, tf.image.flip_left_right(tf.image.flip_up_down(dataset))], 0)\n    elif tf.math.logical_and(x &gt;= 0.42, x &lt; 0.56):\n        return tf.concat([dataset, tf.image.rot90(dataset, k=1)], 0)\n    elif tf.math.logical_and(x &gt;= 0.56, x &lt; 0.70):\n        return tf.concat([dataset, tf.image.rot90(dataset, k=2)], 0)\n    elif tf.math.logical_and(x &gt;= 0.70, x &lt; 0.84):\n        return tf.concat([dataset, tf.image.rot90(dataset, k=3)], 0)\n    else:\n        return tf.concat([dataset, tf.image.flip_left_right(tf.image.rot90(dataset, k=2))], 0)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.rotate_inputs_180","title":"<code>rotate_inputs_180(inputs)</code>  <code>staticmethod</code>","text":"<p>Rotate the inputs of a dataset by 180 degrees.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The dataset with rotated inputs.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef rotate_inputs_180(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Rotate the inputs of a dataset by 180 degrees.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The dataset with rotated inputs.\n    \"\"\"\n    return tf.image.rot90(inputs, k=2)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.rotate_inputs_270","title":"<code>rotate_inputs_270(inputs)</code>  <code>staticmethod</code>","text":"<p>Rotate the inputs of a dataset by 270 degrees.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The dataset with rotated inputs.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef rotate_inputs_270(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Rotate the inputs of a dataset by 270 degrees.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The dataset with rotated inputs.\n    \"\"\"\n    return tf.image.rot90(inputs, k=3)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.rotate_inputs_90","title":"<code>rotate_inputs_90(inputs)</code>  <code>staticmethod</code>","text":"<p>Rotate the inputs of a dataset by 90 degrees.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The dataset with rotated inputs.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef rotate_inputs_90(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Rotate the inputs of a dataset by 90 degrees.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The dataset with rotated inputs.\n    \"\"\"\n    return tf.image.rot90(inputs, k=1)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple","title":"<code>to_tuple(dataset, n_features=None, inverse_labels=False)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset to a tuple of features and labels.</p> <p>dataset (tf.Tensor): The input dataset. n_features (int, optional): The number of features. Default is None. inverse_labels (bool, optional): Whether to inverse the labels. Default is False.</p> <p>tuple: A tuple containing the features and labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple(dataset: tf.Tensor, n_features: int = None, inverse_labels: bool = False) -&gt; tuple:\n\"\"\"\n    Convert a dataset to a tuple of features and labels.\n\n    Parameters:\n    dataset (tf.Tensor): The input dataset.\n    n_features (int, optional): The number of features. Default is None.\n    inverse_labels (bool, optional): Whether to inverse the labels. Default is False.\n\n    Returns:\n    tuple: A tuple containing the features and labels.\n    \"\"\"\n    features = dataset[:, :, :, :n_features]\n    labels = dataset[:, :, :, n_features:]\n    if inverse_labels:\n        labels_inverse = tf.math.abs(labels - 1)\n        labels = tf.concat([labels_inverse, labels], axis=-1)\n    return features, labels\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_dnn","title":"<code>to_tuple_dnn(dataset, label, depth=1)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.</p> <p>dataset (dict): The input dataset. label (tf.Tensor): The label. depth (int, optional): The depth of one-hot encoding. Default is 1.</p> <p>tuple: A tuple containing the features and one-hot encoded labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_dnn(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n\"\"\"\n    Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n    Parameters:\n    dataset (dict): The input dataset.\n    label (tf.Tensor): The label.\n    depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n    Returns:\n    tuple: A tuple containing the features and one-hot encoded labels.\n    \"\"\"\n    return tf.transpose(list(dataset.values())), tf.one_hot(indices=label, depth=depth)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_multi_label","title":"<code>to_tuple_multi_label(dataset, label, depth=1, x_only=False)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.</p> <p>dataset (tuple): The input dataset. n_labels (int, optional): The number of labels. Default is 1.</p> <p>tuple: A tuple containing the features and multi-hot encoded labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_multi_label(dataset: dict, label: tf.Tensor, depth: int = 1, x_only: bool = False) -&gt; tuple:\n\"\"\"\n    Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n    Parameters:\n    dataset (tuple): The input dataset.\n    n_labels (int, optional): The number of labels. Default is 1.\n\n    Returns:\n    tuple: A tuple containing the features and multi-hot encoded labels.\n    \"\"\"\n    label = tf.cast(label, tf.uint8)\n    label = tf.one_hot(indices=label, depth=depth)\n    parsed_dataset = tf.transpose(list(dataset.values()))\n    if x_only:\n        return parsed_dataset\n    return parsed_dataset, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_with_name","title":"<code>to_tuple_with_name(inputs, features=None, labels=None, n_classes=1)</code>  <code>staticmethod</code>","text":"<p>Convert inputs with named features to a tuple of features and one-hot encoded labels.</p> <p>inputs (tf.Tensor): The input dataset. features (list, optional): The list of feature names. Default is None. labels (list, optional): The list of label names. Default is None. n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.</p> <p>tuple: A tuple containing the features and one-hot encoded labels.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_with_name(inputs: tf.Tensor, features: list = None, labels: list = None, n_classes: int = 1) -&gt; tuple:\n\"\"\"\n    Convert inputs with named features to a tuple of features and one-hot encoded labels.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n    features (list, optional): The list of feature names. Default is None.\n    labels (list, optional): The list of label names. Default is None.\n    n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.\n\n    Returns:\n    tuple: A tuple containing the features and one-hot encoded labels.\n    \"\"\"\n    return (\n        {name: inputs[name] for name in features},\n        tf.one_hot(tf.cast(inputs[labels[0]], tf.uint8), n_classes)\n    )\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.transpose_inputs","title":"<code>transpose_inputs(inputs)</code>  <code>staticmethod</code>","text":"<p>Transpose the inputs of a dataset.</p> <p>inputs (tf.Tensor): The input dataset.</p> <p>tf.Tensor: The transposed dataset.</p> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef transpose_inputs(inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Transpose the inputs of a dataset.\n\n    Parameters:\n    inputs (tf.Tensor): The input dataset.\n\n    Returns:\n    tf.Tensor: The transposed dataset.\n    \"\"\"\n    flip_up_down = tf.image.flip_up_down(inputs)\n    transpose = tf.image.flip_left_right(flip_up_down)\n    return transpose\n</code></pre>"},{"location":"ee_utils/","title":"ee_utils module","text":""},{"location":"ee_utils/#aces.ee_utils","title":"<code>aces.ee_utils</code>","text":""},{"location":"ee_utils/#aces.ee_utils.EEUtils","title":"<code>EEUtils</code>","text":"<p>EEUtils: Earth Engine Utility Class</p> <p>This class provides utility functions to handle Earth Engine API information and make authenticated requests.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>class EEUtils:\n\"\"\"\n    EEUtils: Earth Engine Utility Class\n\n    This class provides utility functions to handle Earth Engine API information and make authenticated requests.\n    \"\"\"\n    @staticmethod\n    def get_credentials_by_service_account_key(key):\n\"\"\"\n        Helper function to retrieve credentials using a service account key.\n\n        Parameters:\n        key (str): The path to the service account key JSON file.\n\n        Returns:\n        ee.ServiceAccountCredentials: The authenticated credentials.\n        \"\"\"\n        import json\n        service_account = json.load(open(key))\n        credentials = ee.ServiceAccountCredentials(service_account['client_email'], key)\n        return credentials\n\n    @staticmethod\n    def initialize_session(use_highvolume : bool = False, key : Union[str, None] = None):\n\"\"\"\n        Initialize the Earth Engine session.\n\n        Parameters:\n        use_highvolume (bool): Whether to use the high-volume Earth Engine API.\n        key (str or None): The path to the service account key JSON file. If None, the default credentials will be used.\n        \"\"\"\n        if key is None:\n            if use_highvolume:\n                ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\")\n            else:\n                ee.Initialize()\n        else:\n            credentials = EEUtils.get_credentials_by_service_account_key(key)\n            if use_highvolume:\n                ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n            else:\n                ee.Initialize(credentials)\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.get_credentials_by_service_account_key","title":"<code>get_credentials_by_service_account_key(key)</code>  <code>staticmethod</code>","text":"<p>Helper function to retrieve credentials using a service account key.</p> <p>key (str): The path to the service account key JSON file.</p> <p>ee.ServiceAccountCredentials: The authenticated credentials.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef get_credentials_by_service_account_key(key):\n\"\"\"\n    Helper function to retrieve credentials using a service account key.\n\n    Parameters:\n    key (str): The path to the service account key JSON file.\n\n    Returns:\n    ee.ServiceAccountCredentials: The authenticated credentials.\n    \"\"\"\n    import json\n    service_account = json.load(open(key))\n    credentials = ee.ServiceAccountCredentials(service_account['client_email'], key)\n    return credentials\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.initialize_session","title":"<code>initialize_session(use_highvolume=False, key=None)</code>  <code>staticmethod</code>","text":"<p>Initialize the Earth Engine session.</p> <p>use_highvolume (bool): Whether to use the high-volume Earth Engine API. key (str or None): The path to the service account key JSON file. If None, the default credentials will be used.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef initialize_session(use_highvolume : bool = False, key : Union[str, None] = None):\n\"\"\"\n    Initialize the Earth Engine session.\n\n    Parameters:\n    use_highvolume (bool): Whether to use the high-volume Earth Engine API.\n    key (str or None): The path to the service account key JSON file. If None, the default credentials will be used.\n    \"\"\"\n    if key is None:\n        if use_highvolume:\n            ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\")\n        else:\n            ee.Initialize()\n    else:\n        credentials = EEUtils.get_credentials_by_service_account_key(key)\n        if use_highvolume:\n            ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n        else:\n            ee.Initialize(credentials)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#coming-soon","title":"Coming soon","text":""},{"location":"metrics/","title":"metrics module","text":""},{"location":"metrics/#aces.metrics","title":"<code>aces.metrics</code>","text":"<p>metrics.py: Custom Metrics for Model Evaluation and Utility Functions for Model Visualization</p> <p>This module provides a collection of custom metrics that can be used for evaluating model performance in tasks such as image segmentation. It includes metrics like recall, precision, F1-score, Dice coefficient, Tversky index, and more. Additionally, it contains utility functions for plotting and visualizing model metrics during training.</p>"},{"location":"metrics/#aces.metrics.Metrics","title":"<code>Metrics</code>","text":"<p>A class containing various metrics functions for model evaluation.</p> <p>This class provides a collection of static methods, each representing a different evaluation metric. These metrics can be used to evaluate the performance of classification or segmentation models during training or testing.</p> Methods <p>precision():     Computes the precision metric. recall():     Computes the recall metric. dice_coef():     Computes the Dice coefficient metric. f1_m():     Computes the F1 score metric. one_hot_io_u(num_classes):     Computes the Intersection over Union (IoU) metric for each class. true_positives():     Computes the true positives metric. false_positives():     Computes the false positives metric. true_negatives():     Computes the true negatives metric. false_negatives():     Computes the false negatives metric. binary_accuracy():     Computes the binary accuracy metric. auc():     Computes the Area Under the Curve (AUC) metric. prc():     Computes the Precision-Recall Curve (PRC) metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>class Metrics:\n\"\"\"\n    A class containing various metrics functions for model evaluation.\n\n    This class provides a collection of static methods, each representing a different evaluation metric. These metrics can be used to\n    evaluate the performance of classification or segmentation models during training or testing.\n\n    Methods:\n        precision():\n            Computes the precision metric.\n        recall():\n            Computes the recall metric.\n        dice_coef():\n            Computes the Dice coefficient metric.\n        f1_m():\n            Computes the F1 score metric.\n        one_hot_io_u(num_classes):\n            Computes the Intersection over Union (IoU) metric for each class.\n        true_positives():\n            Computes the true positives metric.\n        false_positives():\n            Computes the false positives metric.\n        true_negatives():\n            Computes the true negatives metric.\n        false_negatives():\n            Computes the false negatives metric.\n        binary_accuracy():\n            Computes the binary accuracy metric.\n        auc():\n            Computes the Area Under the Curve (AUC) metric.\n        prc():\n            Computes the Precision-Recall Curve (PRC) metric.\n    \"\"\"\n\n    @staticmethod\n    def recall_m(y_true, y_pred):\n\"\"\"\n        Calculate the recall metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Recall metric value.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    @staticmethod\n    def precision_m(y_true, y_pred):\n\"\"\"\n        Calculate the precision metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Precision metric value.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    @staticmethod\n    def f1_m(y_true, y_pred):\n\"\"\"\n        Calculate the F1-score metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            F1-score metric value.\n        \"\"\"\n        precision = Metrics.precision_m(y_true, y_pred)\n        recall = Metrics.recall_m(y_true, y_pred)\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    @staticmethod\n    def dice_coef(y_true, y_pred, smooth=1):\n\"\"\"\n        Calculate the Dice coefficient metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n\n        Returns:\n            Dice coefficient metric value.\n        \"\"\"\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n    @staticmethod\n    def dice_loss(y_true, y_pred, smooth=1):\n\"\"\"\n        Calculate the Dice loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n\n        Returns:\n            Dice loss metric value.\n        \"\"\"\n        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n        true_sum = K.sum(K.square(y_true), -1)\n        pred_sum = K.sum(K.square(y_pred), -1)\n        return 1 - ((2. * intersection + smooth) / (true_sum + pred_sum + smooth))\n\n    @staticmethod\n    def bce_dice_loss(y_true, y_pred):\n\"\"\"\n        Calculate the BCE-Dice loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            BCE-Dice loss metric value.\n        \"\"\"\n        return Metrics.bce_loss(y_true, y_pred) + Metrics.dice_loss(y_true, y_pred)\n\n    @staticmethod\n    def bce_loss(y_true, y_pred):\n\"\"\"\n        Calculate the binary cross-entropy (BCE) loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            BCE loss metric value.\n        \"\"\"\n        return keras.losses.binary_crossentropy(y_true, y_pred, label_smoothing=0.2)\n\n    @staticmethod\n    def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n\"\"\"\n        Calculate the Tversky index metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n            alpha: Weighting factor.\n\n        Returns:\n            Tversky index metric value.\n        \"\"\"\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n        return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n\n    @staticmethod\n    def tversky_loss(y_true, y_pred):\n\"\"\"\n        Calculate the Tversky loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Tversky loss metric value.\n        \"\"\"\n        return 1 - Metrics.tversky(y_true, y_pred)\n\n    @staticmethod\n    def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n\"\"\"\n        Calculate the focal Tversky loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            gamma: Focusing parameter.\n\n        Returns:\n            Focal Tversky loss metric value.\n        \"\"\"\n        return K.pow((1 - Metrics.tversky(y_true, y_pred)), gamma)\n\n    @staticmethod\n    def true_positives():\n\"\"\"\n        Create a metric for counting true positives.\n\n        Returns:\n            True positives metric.\n        \"\"\"\n        return keras.metrics.TruePositives(name='tp')\n\n    @staticmethod\n    def false_positives():\n\"\"\"\n        Create a metric for counting false positives.\n\n        Returns:\n            False positives metric.\n        \"\"\"\n        return keras.metrics.FalsePositives(name='fp')\n\n    @staticmethod\n    def true_negatives():\n\"\"\"\n        Create a metric for counting true negatives.\n\n        Returns:\n            True negatives metric.\n        \"\"\"\n        return keras.metrics.TrueNegatives(name='tn')\n\n    @staticmethod\n    def false_negatives():\n\"\"\"\n        Create a metric for counting false negatives.\n\n        Returns:\n            False negatives metric.\n        \"\"\"\n        return keras.metrics.FalseNegatives(name='fn')\n\n    @staticmethod\n    def binary_accuracy():\n\"\"\"\n        Create a metric for calculating binary accuracy.\n\n        Returns:\n            Binary accuracy metric.\n        \"\"\"\n        return keras.metrics.BinaryAccuracy(name='accuracy')\n\n    # check difference between this and precision_m output\n    @staticmethod\n    def precision():\n\"\"\"\n        Create a metric for calculating precision.\n\n        Returns:\n            Precision metric.\n        \"\"\"\n        return keras.metrics.Precision(name='precision')\n\n    @staticmethod\n    def recall():\n\"\"\"\n        Create a metric for calculating recall.\n\n        Returns:\n            Recall metric.\n        \"\"\"\n        return keras.metrics.Recall(name='recall')\n\n    @staticmethod\n    def auc():\n\"\"\"\n        Create a metric for calculating Area Under the Curve (AUC).\n\n        Returns:\n            AUC metric.\n        \"\"\"\n        return keras.metrics.AUC(name='auc')\n\n    @staticmethod\n    def prc():\n\"\"\"\n        Create a metric for calculating Precision-Recall Curve (PRC).\n\n        Returns:\n            PRC metric.\n        \"\"\"\n        return keras.metrics.AUC(name='prc', curve='PR')\n\n    @staticmethod\n    def one_hot_io_u(num_classes, name='one_hot_io_u'):\n\"\"\"\n        Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.\n\n        Args:\n            num_classes: Number of classes.\n            name: Name of the metric.\n\n        Returns:\n            One-hot IoU metric.\n        \"\"\"\n        return keras.metrics.OneHotIoU(num_classes=num_classes, target_class_ids=list(range(num_classes)), name=name)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.auc","title":"<code>auc()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Area Under the Curve (AUC).</p> <p>Returns:</p> Type Description <p>AUC metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef auc():\n\"\"\"\n    Create a metric for calculating Area Under the Curve (AUC).\n\n    Returns:\n        AUC metric.\n    \"\"\"\n    return keras.metrics.AUC(name='auc')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.bce_dice_loss","title":"<code>bce_dice_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the BCE-Dice loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>BCE-Dice loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef bce_dice_loss(y_true, y_pred):\n\"\"\"\n    Calculate the BCE-Dice loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        BCE-Dice loss metric value.\n    \"\"\"\n    return Metrics.bce_loss(y_true, y_pred) + Metrics.dice_loss(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.bce_loss","title":"<code>bce_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the binary cross-entropy (BCE) loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>BCE loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef bce_loss(y_true, y_pred):\n\"\"\"\n    Calculate the binary cross-entropy (BCE) loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        BCE loss metric value.\n    \"\"\"\n    return keras.losses.binary_crossentropy(y_true, y_pred, label_smoothing=0.2)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.binary_accuracy","title":"<code>binary_accuracy()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating binary accuracy.</p> <p>Returns:</p> Type Description <p>Binary accuracy metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef binary_accuracy():\n\"\"\"\n    Create a metric for calculating binary accuracy.\n\n    Returns:\n        Binary accuracy metric.\n    \"\"\"\n    return keras.metrics.BinaryAccuracy(name='accuracy')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.dice_coef","title":"<code>dice_coef(y_true, y_pred, smooth=1)</code>  <code>staticmethod</code>","text":"<p>Calculate the Dice coefficient metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <p>Returns:</p> Type Description <p>Dice coefficient metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef dice_coef(y_true, y_pred, smooth=1):\n\"\"\"\n    Calculate the Dice coefficient metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n\n    Returns:\n        Dice coefficient metric value.\n    \"\"\"\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.dice_loss","title":"<code>dice_loss(y_true, y_pred, smooth=1)</code>  <code>staticmethod</code>","text":"<p>Calculate the Dice loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <p>Returns:</p> Type Description <p>Dice loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef dice_loss(y_true, y_pred, smooth=1):\n\"\"\"\n    Calculate the Dice loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n\n    Returns:\n        Dice loss metric value.\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    true_sum = K.sum(K.square(y_true), -1)\n    pred_sum = K.sum(K.square(y_pred), -1)\n    return 1 - ((2. * intersection + smooth) / (true_sum + pred_sum + smooth))\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.f1_m","title":"<code>f1_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the F1-score metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>F1-score metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef f1_m(y_true, y_pred):\n\"\"\"\n    Calculate the F1-score metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        F1-score metric value.\n    \"\"\"\n    precision = Metrics.precision_m(y_true, y_pred)\n    recall = Metrics.recall_m(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.false_negatives","title":"<code>false_negatives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting false negatives.</p> <p>Returns:</p> Type Description <p>False negatives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef false_negatives():\n\"\"\"\n    Create a metric for counting false negatives.\n\n    Returns:\n        False negatives metric.\n    \"\"\"\n    return keras.metrics.FalseNegatives(name='fn')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.false_positives","title":"<code>false_positives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting false positives.</p> <p>Returns:</p> Type Description <p>False positives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef false_positives():\n\"\"\"\n    Create a metric for counting false positives.\n\n    Returns:\n        False positives metric.\n    \"\"\"\n    return keras.metrics.FalsePositives(name='fp')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.focal_tversky_loss","title":"<code>focal_tversky_loss(y_true, y_pred, gamma=0.75)</code>  <code>staticmethod</code>","text":"<p>Calculate the focal Tversky loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>gamma</code> <p>Focusing parameter.</p> <code>0.75</code> <p>Returns:</p> Type Description <p>Focal Tversky loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n\"\"\"\n    Calculate the focal Tversky loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        gamma: Focusing parameter.\n\n    Returns:\n        Focal Tversky loss metric value.\n    \"\"\"\n    return K.pow((1 - Metrics.tversky(y_true, y_pred)), gamma)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.one_hot_io_u","title":"<code>one_hot_io_u(num_classes, name='one_hot_io_u')</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <p>Number of classes.</p> required <code>name</code> <p>Name of the metric.</p> <code>'one_hot_io_u'</code> <p>Returns:</p> Type Description <p>One-hot IoU metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef one_hot_io_u(num_classes, name='one_hot_io_u'):\n\"\"\"\n    Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.\n\n    Args:\n        num_classes: Number of classes.\n        name: Name of the metric.\n\n    Returns:\n        One-hot IoU metric.\n    \"\"\"\n    return keras.metrics.OneHotIoU(num_classes=num_classes, target_class_ids=list(range(num_classes)), name=name)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.prc","title":"<code>prc()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Precision-Recall Curve (PRC).</p> <p>Returns:</p> Type Description <p>PRC metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef prc():\n\"\"\"\n    Create a metric for calculating Precision-Recall Curve (PRC).\n\n    Returns:\n        PRC metric.\n    \"\"\"\n    return keras.metrics.AUC(name='prc', curve='PR')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.precision","title":"<code>precision()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating precision.</p> <p>Returns:</p> Type Description <p>Precision metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef precision():\n\"\"\"\n    Create a metric for calculating precision.\n\n    Returns:\n        Precision metric.\n    \"\"\"\n    return keras.metrics.Precision(name='precision')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.precision_m","title":"<code>precision_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the precision metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Precision metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef precision_m(y_true, y_pred):\n\"\"\"\n    Calculate the precision metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Precision metric value.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.recall","title":"<code>recall()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating recall.</p> <p>Returns:</p> Type Description <p>Recall metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef recall():\n\"\"\"\n    Create a metric for calculating recall.\n\n    Returns:\n        Recall metric.\n    \"\"\"\n    return keras.metrics.Recall(name='recall')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.recall_m","title":"<code>recall_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the recall metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Recall metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef recall_m(y_true, y_pred):\n\"\"\"\n    Calculate the recall metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Recall metric value.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.true_negatives","title":"<code>true_negatives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting true negatives.</p> <p>Returns:</p> Type Description <p>True negatives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef true_negatives():\n\"\"\"\n    Create a metric for counting true negatives.\n\n    Returns:\n        True negatives metric.\n    \"\"\"\n    return keras.metrics.TrueNegatives(name='tn')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.true_positives","title":"<code>true_positives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting true positives.</p> <p>Returns:</p> Type Description <p>True positives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef true_positives():\n\"\"\"\n    Create a metric for counting true positives.\n\n    Returns:\n        True positives metric.\n    \"\"\"\n    return keras.metrics.TruePositives(name='tp')\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.tversky","title":"<code>tversky(y_true, y_pred, smooth=1, alpha=0.7)</code>  <code>staticmethod</code>","text":"<p>Calculate the Tversky index metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <code>alpha</code> <p>Weighting factor.</p> <code>0.7</code> <p>Returns:</p> Type Description <p>Tversky index metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef tversky(y_true, y_pred, smooth=1, alpha=0.7):\n\"\"\"\n    Calculate the Tversky index metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n        alpha: Weighting factor.\n\n    Returns:\n        Tversky index metric value.\n    \"\"\"\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.tversky_loss","title":"<code>tversky_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the Tversky loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Tversky loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef tversky_loss(y_true, y_pred):\n\"\"\"\n    Calculate the Tversky loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Tversky loss metric value.\n    \"\"\"\n    return 1 - Metrics.tversky(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#aces.metrics.Utils","title":"<code>Utils</code>","text":"<p>Utils: Utility Functions for Model Visualization</p> <p>This class provides utility functions for plotting and visualizing model metrics during training.</p> Source code in <code>aces/metrics.py</code> <pre><code>class Utils:\n\"\"\"\n    Utils: Utility Functions for Model Visualization\n\n    This class provides utility functions for plotting and visualizing model metrics during training.\n    \"\"\"\n    @staticmethod\n    def plot_metrics(metrics, history, epoch, model_save_dir):\n\"\"\"\n        Plot the training and validation metrics over epochs.\n\n        Args:\n            metrics: List of metrics to plot.\n            history: Training history containing metric values.\n            epoch: List of epochs.\n            model_save_dir: Directory to save the plot.\n\n        Returns:\n            None.\n        \"\"\"\n        fig, ax = plt.subplots(nrows=len(metrics), sharex=True, figsize=(15, len(metrics) * 6))\n        colors = ['#1f77b4', '#ff7f0e', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n\n        for i, metric in enumerate(metrics):\n            ax[i].plot(history[metric], color=colors[i], label=f'Training {metric.upper()}')\n            ax[i].plot(history[f'val_{metric}'], linestyle=':', marker='o', markersize=3, color=colors[i], label=f'Validation {metric.upper()}')\n            ax[i].set_ylabel(metric.upper())\n            ax[i].legend()\n\n        ax[i].set_xticks(range(1, len(epoch) + 1, 4))\n        ax[i].set_xticklabels(range(1, len(epoch) + 1, 4))\n        ax[i].set_xlabel('Epoch')\n        fig.savefig(f\"{model_save_dir}/training.png\", dpi=700)\n</code></pre>"},{"location":"metrics/#aces.metrics.Utils.plot_metrics","title":"<code>plot_metrics(metrics, history, epoch, model_save_dir)</code>  <code>staticmethod</code>","text":"<p>Plot the training and validation metrics over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>List of metrics to plot.</p> required <code>history</code> <p>Training history containing metric values.</p> required <code>epoch</code> <p>List of epochs.</p> required <code>model_save_dir</code> <p>Directory to save the plot.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef plot_metrics(metrics, history, epoch, model_save_dir):\n\"\"\"\n    Plot the training and validation metrics over epochs.\n\n    Args:\n        metrics: List of metrics to plot.\n        history: Training history containing metric values.\n        epoch: List of epochs.\n        model_save_dir: Directory to save the plot.\n\n    Returns:\n        None.\n    \"\"\"\n    fig, ax = plt.subplots(nrows=len(metrics), sharex=True, figsize=(15, len(metrics) * 6))\n    colors = ['#1f77b4', '#ff7f0e', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n\n    for i, metric in enumerate(metrics):\n        ax[i].plot(history[metric], color=colors[i], label=f'Training {metric.upper()}')\n        ax[i].plot(history[f'val_{metric}'], linestyle=':', marker='o', markersize=3, color=colors[i], label=f'Validation {metric.upper()}')\n        ax[i].set_ylabel(metric.upper())\n        ax[i].legend()\n\n    ax[i].set_xticks(range(1, len(epoch) + 1, 4))\n    ax[i].set_xticklabels(range(1, len(epoch) + 1, 4))\n    ax[i].set_xlabel('Epoch')\n    fig.savefig(f\"{model_save_dir}/training.png\", dpi=700)\n</code></pre>"},{"location":"model_builder/","title":"model_builder module","text":""},{"location":"model_builder/#aces.model_builder","title":"<code>aces.model_builder</code>","text":"<p>model_builder.py: Model Builder Class for Creating and Compiling Neural Network Models</p> <p>This module provides a <code>ModelBuilder</code> class that is responsible for creating and compiling neural network models. It includes methods for building and compiling models of different types, such as DNN, CNN, and U-Net, based on the provided specifications. The class also contains utility methods for constructing custom layers and defining metrics.</p>"},{"location":"model_builder/#aces.model_builder.ModelBuilder","title":"<code>ModelBuilder</code>","text":"<p>ModelBuilder Class for Creating and Compiling Neural Network Models</p> <p>This class provides methods for building and compiling neural network models of different types, such as DNN, CNN, and U-Net, based on the provided specifications. It includes utility methods for constructing custom layers and defining metrics.</p> <p>Attributes:</p> Name Type Description <code>in_size</code> <code>int</code> <p>The input size of the models.</p> <code>out_classes</code> <code>int</code> <p>The number of output classes for classification models.</p> <code>optimizer</code> <code>tf.keras.optimizers.Optimizer</code> <p>The optimizer to use for model compilation.</p> <code>loss</code> <code>tf.keras.losses.Loss</code> <p>The loss function to use for model compilation.</p> Methods <p>build_model(model_type, kwargs):     Builds and compiles a neural network model based on the provided model type. build_and_compile_dnn_model(kwargs):     Builds and compiles a Deep Neural Network (DNN) model. build_and_compile_cnn_model(kwargs):     Builds and compiles a Convolutional Neural Network (CNN) model. build_and_compile_unet_model(kwargs):     Builds and compiles a U-Net model. _build_and_compile_unet_model(**kwargs):     Helper method for building and compiling a U-Net model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>class ModelBuilder:\n\"\"\"\n    ModelBuilder Class for Creating and Compiling Neural Network Models\n\n    This class provides methods for building and compiling neural network models of different types, such as DNN, CNN, and U-Net,\n    based on the provided specifications. It includes utility methods for constructing custom layers and defining metrics.\n\n    Attributes:\n        in_size (int): The input size of the models.\n        out_classes (int): The number of output classes for classification models.\n        optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n        loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n\n    Methods:\n        build_model(model_type, **kwargs):\n            Builds and compiles a neural network model based on the provided model type.\n        build_and_compile_dnn_model(**kwargs):\n            Builds and compiles a Deep Neural Network (DNN) model.\n        build_and_compile_cnn_model(**kwargs):\n            Builds and compiles a Convolutional Neural Network (CNN) model.\n        build_and_compile_unet_model(**kwargs):\n            Builds and compiles a U-Net model.\n        _build_and_compile_unet_model(**kwargs):\n            Helper method for building and compiling a U-Net model.\n    \"\"\"\n\n    def __init__(self, in_size, out_classes, optimizer, loss):\n\"\"\"\n        Initialize ModelBuilder with input size, output classes, optimizer, and loss.\n\n        Args:\n            in_size (int): The input size of the models.\n            out_classes (int): The number of output classes for classification models.\n            optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n            loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n        \"\"\"\n        self.in_size = in_size\n        self.out_classes = out_classes\n        self.optimizer = optimizer\n        self.loss = loss\n\n    def build_model(self, model_type, **kwargs):\n\"\"\"\n        Builds and compiles a neural network model based on the provided model type.\n\n        Args:\n            model_type (str): The type of the model to build ('dnn', 'cnn', 'unet').\n            **kwargs: Additional keyword arguments specific to the model type.\n\n        Returns:\n            keras.Model: The compiled neural network model.\n\n        Raises:\n            ValueError: If an invalid model type is provided.\n        \"\"\"\n        if model_type == 'dnn':\n            return self.build_and_compile_dnn_model(**kwargs)\n        elif model_type == 'cnn':\n            return self.build_and_compile_cnn_model(**kwargs)\n        elif model_type == 'unet':\n            return self.build_and_compile_unet_model(**kwargs)\n        else:\n            raise ValueError(f\"Invalid model type: {model_type}\")\n\n    def build_and_compile_dnn_model(self, **kwargs):\n\"\"\"\n        Builds and compiles a Deep Neural Network (DNN) model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled DNN model.\n        \"\"\"\n        FINAL_ACTIVATION = kwargs.get('FINAL_ACTIVATION', 'sigmoid')\n        INITIAL_BIAS = kwargs.get('INITIAL_BIAS', None)\n        # DNN_DURING_ONLY = kwargs.get('DURING_ONLY', False)\n\n        if INITIAL_BIAS is not None:\n            INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n\n        inputs = keras.Input(shape=(None, self.in_size), name='input_layer')\n\n        input_features = rs.concatenate_features_for_dnn(inputs)\n\n        # Create a custom input layer that accepts 4 channels\n        y = keras.layers.Conv1D(64, 3, activation='relu', padding='same', name='conv1')(input_features)\n        y = keras.layers.MaxPooling1D(2, padding='same')(y)\n        y = keras.layers.Conv1D(32, 3, activation='relu', padding='same', name='conv2')(y)\n        y = keras.layers.MaxPooling1D(2, padding='same')(y)\n        y = keras.layers.Conv1D(self.in_size, 2, activation='relu', padding='same', name='conv4')(y)\n\n        all_inputs = keras.layers.concatenate([inputs, y])\n\n        x = keras.layers.Dense(256, activation='relu')(all_inputs)\n        x = keras.layers.Dropout(0.2)(x)\n        x = keras.layers.Dense(128, activation='relu')(x)\n        x = keras.layers.Dropout(0.2)(x)\n        x = keras.layers.Dense(64, activation='relu')(x)\n        x = keras.layers.Dropout(0.2)(x)\n        x = keras.layers.Dense(32, activation='relu')(x)\n        x = keras.layers.Dropout(0.2)(x)\n        output = keras.layers.Dense(self.out_classes, activation=FINAL_ACTIVATION, bias_initializer=INITIAL_BIAS)(x)\n\n        model = keras.models.Model(inputs=inputs, outputs=output)\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.categorical_accuracy,\n            Metrics.dice_coef,\n            Metrics.f1_m,\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n\n    def build_and_compile_cnn_model(self, **kwargs):\n\"\"\"\n        Builds and compiles a Convolutional Neural Network (CNN) model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled CNN model.\n        \"\"\"\n        inputs = keras.Input(shape=(128, 128, self.in_size))\n        x = keras.layers.Conv2D(32, 3, activation='relu', name='convd-1')(inputs)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(0.15)(x)\n        x = keras.layers.Conv2D(64, 3, activation='relu', name='convd-2')(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(0.15)(x)\n        x = keras.layers.Conv2D(128, 3, activation='relu', name='convd-3')(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(0.15)(x)\n        x = keras.layers.Conv2D(128, 3, activation='relu', name='convd-4')(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(0.15)(x)\n        x = keras.layers.Conv2D(64, 3, activation='relu', name='convd-8')(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(0.15)(x)\n        x = keras.layers.MaxPooling2D((2, 2), name='maxpool-1')(x)\n        outputs = keras.layers.Conv2D(self.out_classes, (1, 1), activation='softmax', name='final_conv')(x)\n        model = keras.Model(inputs, outputs, name='cnn_model')\n\n        metrics_list = [\n            Metrics.true_positives(),\n            Metrics.false_positives(),\n            Metrics.true_negatives(),\n            Metrics.false_negatives(),\n            Metrics.binary_accuracy(),\n            Metrics.precision(),\n            Metrics.recall(),\n            Metrics.auc(),\n            Metrics.prc(),\n            Metrics.one_hot_io_u(self.out_classes),\n            Metrics.f1_m,\n            Metrics.dice_coef,\n        ]\n\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n\n    def build_and_compile_unet_model(self, **kwargs):\n\"\"\"\n        Builds and compiles a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        DISTRIBUTED_STRATEGY = kwargs.get(\"DISTRIBUTED_STRATEGY\", None)\n\n        if DISTRIBUTED_STRATEGY is not None:\n            with DISTRIBUTED_STRATEGY.scope():\n                return self._build_and_compile_unet_model(**kwargs)\n        else:\n            print(\"No distributed strategy found.\")\n            return self._build_and_compile_unet_model(**kwargs)\n\n    def _build_and_compile_unet_model(self, **kwargs):\n\"\"\"\n        Helper method for building and compiling a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        inputs = keras.Input(shape=(None, None, self.in_size))\n\n        input_features = rs.concatenate_features_for_cnn(inputs)\n\n        y = keras.layers.Conv2D(64, 3, activation='relu', padding='same', name='conv1')(input_features)\n        y = keras.layers.Conv2D(32, 3, activation='relu', padding='same', name='conv2')(y)\n        y = keras.layers.Conv2D(self.in_size, 3, activation='relu', padding='same', name='conv4')(y)\n\n        all_inputs = keras.layers.concatenate([inputs, y])\n\n        x = keras.layers.Conv2D(32, 3, strides=2, padding='same')(all_inputs)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Activation('relu')(x)\n\n        previous_block_activation = x\n\n        l2_regularizer = keras.regularizers.l2(0.001)\n\n        for filters in [64, 128]:\n            x = keras.layers.Activation('relu')(x)\n            x = keras.layers.SeparableConv2D(filters, 3, padding='same', depthwise_initializer='he_normal',\n                                              bias_initializer='he_normal', depthwise_regularizer=l2_regularizer)(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.Activation('relu')(x)\n            x = keras.layers.SeparableConv2D(filters, 3, padding='same', depthwise_initializer='he_normal',\n                                              bias_initializer='he_normal', depthwise_regularizer=l2_regularizer)(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n            residual = keras.layers.Conv2D(filters, 1, strides=2, padding='same', bias_initializer='he_normal',\n                                           kernel_initializer='he_normal', bias_regularizer=l2_regularizer,\n                                           kernel_regularizer=l2_regularizer)(\n                previous_block_activation\n            )\n            x = keras.layers.add([x, residual])\n            previous_block_activation = x\n\n        for filters in [128, 64, 32]:\n            x = keras.layers.Activation('relu')(x)\n            x = keras.layers.Conv2DTranspose(filters, 3, padding='same')(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.Activation('relu')(x)\n            x = keras.layers.Conv2DTranspose(filters, 3, padding='same')(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.UpSampling2D(2)(x)\n\n            residual = keras.layers.UpSampling2D(2)(previous_block_activation)\n            residual = keras.layers.Conv2D(filters, 1, padding='same')(residual)\n            x = keras.layers.add([x, residual])\n            previous_block_activation = x\n\n        outputs = keras.layers.Conv2D(self.out_classes, 3, activation=kwargs.get(\"ACTIVATION_FN\"), padding=\"same\", name=\"final_conv\")(x)\n\n        model = keras.Model(inputs=inputs, outputs=outputs, name='unet')\n\n        metrics_list = [\n            # Metrics.precision(),\n            # Metrics.recall(),\n            keras.metrics.categorical_accuracy,\n            # Metrics.dice_coef,\n            # Metrics.f1_m,\n            keras.metrics.Accuracy(),\n            # Metrics.one_hot_io_u(self.out_classes),\n        ]\n\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.__init__","title":"<code>__init__(in_size, out_classes, optimizer, loss)</code>","text":"<p>Initialize ModelBuilder with input size, output classes, optimizer, and loss.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>The input size of the models.</p> required <code>out_classes</code> <code>int</code> <p>The number of output classes for classification models.</p> required <code>optimizer</code> <code>tf.keras.optimizers.Optimizer</code> <p>The optimizer to use for model compilation.</p> required <code>loss</code> <code>tf.keras.losses.Loss</code> <p>The loss function to use for model compilation.</p> required Source code in <code>aces/model_builder.py</code> <pre><code>def __init__(self, in_size, out_classes, optimizer, loss):\n\"\"\"\n    Initialize ModelBuilder with input size, output classes, optimizer, and loss.\n\n    Args:\n        in_size (int): The input size of the models.\n        out_classes (int): The number of output classes for classification models.\n        optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n        loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n    \"\"\"\n    self.in_size = in_size\n    self.out_classes = out_classes\n    self.optimizer = optimizer\n    self.loss = loss\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_cnn_model","title":"<code>build_and_compile_cnn_model(**kwargs)</code>","text":"<p>Builds and compiles a Convolutional Neural Network (CNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>keras.Model: The compiled CNN model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_cnn_model(self, **kwargs):\n\"\"\"\n    Builds and compiles a Convolutional Neural Network (CNN) model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled CNN model.\n    \"\"\"\n    inputs = keras.Input(shape=(128, 128, self.in_size))\n    x = keras.layers.Conv2D(32, 3, activation='relu', name='convd-1')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.15)(x)\n    x = keras.layers.Conv2D(64, 3, activation='relu', name='convd-2')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.15)(x)\n    x = keras.layers.Conv2D(128, 3, activation='relu', name='convd-3')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.15)(x)\n    x = keras.layers.Conv2D(128, 3, activation='relu', name='convd-4')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.15)(x)\n    x = keras.layers.Conv2D(64, 3, activation='relu', name='convd-8')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.15)(x)\n    x = keras.layers.MaxPooling2D((2, 2), name='maxpool-1')(x)\n    outputs = keras.layers.Conv2D(self.out_classes, (1, 1), activation='softmax', name='final_conv')(x)\n    model = keras.Model(inputs, outputs, name='cnn_model')\n\n    metrics_list = [\n        Metrics.true_positives(),\n        Metrics.false_positives(),\n        Metrics.true_negatives(),\n        Metrics.false_negatives(),\n        Metrics.binary_accuracy(),\n        Metrics.precision(),\n        Metrics.recall(),\n        Metrics.auc(),\n        Metrics.prc(),\n        Metrics.one_hot_io_u(self.out_classes),\n        Metrics.f1_m,\n        Metrics.dice_coef,\n    ]\n\n    model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n    return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_dnn_model","title":"<code>build_and_compile_dnn_model(**kwargs)</code>","text":"<p>Builds and compiles a Deep Neural Network (DNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>keras.Model: The compiled DNN model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_dnn_model(self, **kwargs):\n\"\"\"\n    Builds and compiles a Deep Neural Network (DNN) model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled DNN model.\n    \"\"\"\n    FINAL_ACTIVATION = kwargs.get('FINAL_ACTIVATION', 'sigmoid')\n    INITIAL_BIAS = kwargs.get('INITIAL_BIAS', None)\n    # DNN_DURING_ONLY = kwargs.get('DURING_ONLY', False)\n\n    if INITIAL_BIAS is not None:\n        INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n\n    inputs = keras.Input(shape=(None, self.in_size), name='input_layer')\n\n    input_features = rs.concatenate_features_for_dnn(inputs)\n\n    # Create a custom input layer that accepts 4 channels\n    y = keras.layers.Conv1D(64, 3, activation='relu', padding='same', name='conv1')(input_features)\n    y = keras.layers.MaxPooling1D(2, padding='same')(y)\n    y = keras.layers.Conv1D(32, 3, activation='relu', padding='same', name='conv2')(y)\n    y = keras.layers.MaxPooling1D(2, padding='same')(y)\n    y = keras.layers.Conv1D(self.in_size, 2, activation='relu', padding='same', name='conv4')(y)\n\n    all_inputs = keras.layers.concatenate([inputs, y])\n\n    x = keras.layers.Dense(256, activation='relu')(all_inputs)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    output = keras.layers.Dense(self.out_classes, activation=FINAL_ACTIVATION, bias_initializer=INITIAL_BIAS)(x)\n\n    model = keras.models.Model(inputs=inputs, outputs=output)\n    metrics_list = [\n        Metrics.precision(),\n        Metrics.recall(),\n        keras.metrics.categorical_accuracy,\n        Metrics.dice_coef,\n        Metrics.f1_m,\n        Metrics.one_hot_io_u(self.out_classes),\n    ]\n\n    model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n    return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_unet_model","title":"<code>build_and_compile_unet_model(**kwargs)</code>","text":"<p>Builds and compiles a U-Net model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>keras.Model: The compiled U-Net model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_unet_model(self, **kwargs):\n\"\"\"\n    Builds and compiles a U-Net model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled U-Net model.\n    \"\"\"\n    DISTRIBUTED_STRATEGY = kwargs.get(\"DISTRIBUTED_STRATEGY\", None)\n\n    if DISTRIBUTED_STRATEGY is not None:\n        with DISTRIBUTED_STRATEGY.scope():\n            return self._build_and_compile_unet_model(**kwargs)\n    else:\n        print(\"No distributed strategy found.\")\n        return self._build_and_compile_unet_model(**kwargs)\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_model","title":"<code>build_model(model_type, **kwargs)</code>","text":"<p>Builds and compiles a neural network model based on the provided model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to build ('dnn', 'cnn', 'unet').</p> required <code>**kwargs</code> <p>Additional keyword arguments specific to the model type.</p> <code>{}</code> <p>Returns:</p> Type Description <p>keras.Model: The compiled neural network model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid model type is provided.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_model(self, model_type, **kwargs):\n\"\"\"\n    Builds and compiles a neural network model based on the provided model type.\n\n    Args:\n        model_type (str): The type of the model to build ('dnn', 'cnn', 'unet').\n        **kwargs: Additional keyword arguments specific to the model type.\n\n    Returns:\n        keras.Model: The compiled neural network model.\n\n    Raises:\n        ValueError: If an invalid model type is provided.\n    \"\"\"\n    if model_type == 'dnn':\n        return self.build_and_compile_dnn_model(**kwargs)\n    elif model_type == 'cnn':\n        return self.build_and_compile_cnn_model(**kwargs)\n    elif model_type == 'unet':\n        return self.build_and_compile_unet_model(**kwargs)\n    else:\n        raise ValueError(f\"Invalid model type: {model_type}\")\n</code></pre>"},{"location":"model_trainer/","title":"model_trainer module","text":""},{"location":"model_trainer/#aces.model_trainer","title":"<code>aces.model_trainer</code>","text":""},{"location":"model_trainer/#aces.model_trainer.ModelTrainer","title":"<code>ModelTrainer</code>","text":"<p>A class for training deep learning models.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>An object containing the configuration settings for model training.</p> <code>model_builder</code> <p>An instance of ModelBuilder for building the model.</p> <code>build_model</code> <p>A partial function for building the model with the specified model type.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>class ModelTrainer:\n\"\"\"\n    A class for training deep learning models.\n\n    Attributes:\n        config: An object containing the configuration settings for model training.\n        model_builder: An instance of ModelBuilder for building the model.\n        build_model: A partial function for building the model with the specified model type.\n    \"\"\"\n    def __init__(self, config):\n\"\"\"\n        Initialize the ModelTrainer object.\n\n        Args:\n            config: An object containing the configuration settings for model training.\n\n        Attributes:\n            config: The configuration settings for model training.\n            model_builder: An instance of ModelBuilder for building the model.\n            build_model: A partial function for building the model with the specified model type.\n        \"\"\"\n        self.config = config\n        self.model_builder = ModelBuilder(\n            in_size=len(self.config.FEATURES),\n            out_classes=self.config.OUT_CLASS_NUM,\n            optimizer=self.config.OPTIMIZER,\n            loss=self.config.LOSS\n        )\n        self.build_model = partial(self.model_builder.build_model, model_type=self.config.MODEL_TYPE)\n\n    def train_model(self) -&gt; None:\n\"\"\"\n        Train the model using the provided configuration settings.\n\n        This method performs the following steps:\n        1. Configures memory growth for TensorFlow.\n        2. Creates TensorFlow datasets for training, testing, and validation.\n        3. Builds and compiles the model.\n        4. Prepares the output directory for saving models and results.\n        5. Starts the training process.\n        6. Evaluates and prints validation metrics.\n        7. Saves training parameters, plots, and models.\n        \"\"\"\n        logging.info(\"****************************************************************************\")\n        print(f\"****************************** Configure memory growth... ************************\")\n        self.configure_memory_growth()\n        logging.info(\"****************************************************************************\")\n        logging.info(\"****************************** creating datasets... ************************\")\n        self.create_datasets(print_info=True)\n        logging.info(\"****************************************************************************\")\n        logging.info(\"************************ building and compiling model... *******************\")\n        self.build_and_compile_model(print_model_summary=True)\n        logging.info(\"****************************************************************************\")\n        logging.info(\"************************ preparing output directory... *********************\")\n        self.prepare_output_dir()\n        logging.info(\"****************************************************************************\")\n        logging.info(\"****************************** training model... ***************************\")\n        self.start_training()\n        self.evaluate_and_print_val()\n        self.save_parameters()\n        self.save_plots()\n        self.save_models()\n\n    def prepare_output_dir(self) -&gt; None:\n\"\"\"\n        Prepare the output directory for saving models and results.\n\n        Creates a directory with a timestamped name and increments the version number if necessary.\n        \"\"\"\n        today = datetime.date.today().strftime(\"%Y_%m_%d\")\n        iterator = 1\n        while True:\n            self.model_dir_name = f\"trial_{self.config.MODEL_TYPE}_{today}_V{iterator}\"\n            self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.model_dir_name\n            try:\n                os.mkdir(self.config.MODEL_SAVE_DIR)\n            except FileExistsError:\n                logging.info(f\"&gt; {self.config.MODEL_SAVE_DIR} exists, creating another version...\")\n                iterator += 1\n                continue\n            break\n\n    def create_datasets(self, print_info: bool = False) -&gt; None:\n\"\"\"\n        Create TensorFlow datasets for training, testing, and validation.\n\n        Args:\n            print_info: Flag indicating whether to print dataset information.\n\n        Prints information about the created datasets if print_info is set to True.\n        \"\"\"\n        self.TRAINING_DATASET = DataProcessor.get_dataset(\n            # self.config.TRAINING_FILES,\n            f\"{str(self.config.TRAINING_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            self.config.BATCH_SIZE,\n            self.config.OUT_CLASS_NUM,\n        ).repeat()\n        self.TESTING_DATASET = DataProcessor.get_dataset(\n            # self.config.TESTING_FILES,\n            f\"{str(self.config.TESTING_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            1,\n            self.config.OUT_CLASS_NUM,\n        ).repeat()\n        self.VALIDATION_DATASET = DataProcessor.get_dataset(\n            # self.config.VALIDATION_FILES,\n            f\"{str(self.config.VALIDATION_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            1,\n            self.config.OUT_CLASS_NUM,\n        )\n\n        if print_info:\n            logging.info(\"Printing dataset info:\")\n            DataProcessor.print_dataset_info(self.TRAINING_DATASET, \"Training\")\n            DataProcessor.print_dataset_info(self.TESTING_DATASET, \"Testing\")\n            DataProcessor.print_dataset_info(self.VALIDATION_DATASET, \"Validation\")\n\n    def configure_memory_growth(self) -&gt; None:\n\"\"\"\n        Configure TensorFlow to allocate GPU memory dynamically.\n\n        If GPUs are found, this method enables memory growth for each GPU.\n        \"\"\"\n        if self.config.physical_devices:\n            logging.info(f\" &gt; Found {len(self.config.physical_devices)} GPUs\")\n            try:\n                for device in self.config.physical_devices:\n                    tf.config.experimental.set_memory_growth(device, True)\n            except Exception as err:\n                logging.error(err)\n        else:\n            logging.info(\" &gt; No GPUs found\")\n\n    def build_and_compile_model(self, print_model_summary: bool = True) -&gt; None:\n\"\"\"\n        Build and compile the model.\n\n        Args:\n            print_model_summary: Flag indicating whether to print the model summary.\n\n        Builds and compiles the model using the provided configuration settings.\n        Prints the model summary if print_model_summary is set to True.\n        \"\"\"\n        self.model = self.build_model(**self.config.__dict__)\n        if print_model_summary:  logging.info(self.model.summary())\n\n    def start_training(self) -&gt; None:\n\"\"\"\n        Start the training process.\n\n        Trains the model using the provided configuration settings and callbacks.\n        \"\"\"\n        model_checkpoint = callbacks.ModelCheckpoint(\n            f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}.h5\",\n            monitor=self.config.CALLBACK_PARAMETER,\n            save_best_only=True,\n            mode=\"max\",\n            verbose=1,\n            save_weights_only=True,\n        )  # save best model\n\n        early_stopping = callbacks.EarlyStopping(\n            monitor=self.config.CALLBACK_PARAMETER,\n            patience=int(0.4 * self.config.EPOCHS),\n            verbose=1,\n            mode=\"max\",\n            restore_best_weights=True,\n        )\n        tensorboard = callbacks.TensorBoard(log_dir=str(self.config.MODEL_SAVE_DIR / \"logs\"), write_images=True)\n\n        def lr_scheduler(epoch):\n            if epoch &lt; self.config.RAMPUP_EPOCHS:\n                return self.config.MAX_LR\n            elif epoch &lt; self.config.RAMPUP_EPOCHS + self.config.SUSTAIN_EPOCHS:\n                return self.config.MID_LR\n            else:\n                return self.config.MIN_LR\n\n        lr_callback = callbacks.LearningRateScheduler(lambda epoch: lr_scheduler(epoch), verbose=True)\n\n        model_callbacks = [model_checkpoint, tensorboard]\n        if self.config.USE_ADJUSTED_LR:\n            model_callbacks.append(lr_callback)\n\n        self.model_callbacks = model_callbacks\n\n        self.history = self.model.fit(\n            x=self.TRAINING_DATASET,\n            epochs=self.config.EPOCHS,\n            steps_per_epoch=(self.config.TRAIN_SIZE // self.config.BATCH_SIZE),\n            validation_data=self.TESTING_DATASET,\n            validation_steps=(self.config.TEST_SIZE // self.config.BATCH_SIZE),\n            callbacks=model_callbacks,\n        )\n\n        # logging.info(self.model.summary())\n\n    def evaluate_and_print_val(self) -&gt; None:\n\"\"\"\n        Evaluate and print validation metrics.\n\n        Evaluates the model on the validation dataset and prints the metrics.\n        \"\"\"\n        logging.info(\"************************************************\")\n        logging.info(\"************************************************\")\n        logging.info(\"Validation\")\n        evaluate_results = self.model.evaluate(self.config.VALIDATION_DATASET)\n        for name, value in zip(self.model.metrics_names, evaluate_results):\n            logging.info(f\"{name}: {value}\")\n        logging.info()\n\n    def save_parameters(self) -&gt; None:\n\"\"\"\n        Save the training parameters to a text file.\n\n        Saves the training parameters used in the configuration settings to a text file.\n        \"\"\"\n        with open(f\"{str(self.config.MODEL_SAVE_DIR)}/parameters.txt\", \"w\") as f:\n            f.write(f\"MODEL_FUNCTION: {self.config.MODEL_FUNCTION}\\n\")\n            f.write(f\"TRAIN_SIZE: {self.config.TRAIN_SIZE}\\n\")\n            f.write(f\"TEST_SIZE: {self.config.TEST_SIZE}\\n\")\n            f.write(f\"VAL_SIZE: {self.config.VAL_SIZE}\\n\")\n            f.write(f\"BATCH_SIZE: {self.config.BATCH_SIZE}\\n\")\n            f.write(f\"EPOCHS: {self.config.EPOCHS}\\n\")\n            f.write(f\"LOSS: {self.config.LOSS_TXT}\\n\")\n            f.write(f\"BUFFER_SIZE: {self.config.BUFFER_SIZE}\\n\")\n            f.write(f\"LEARNING_RATE: {self.config.LEARNING_RATE}\\n\")\n            if self.config.USE_ADJUSTED_LR:\n                f.write(f\"USE_ADJUSTED_LR: {self.config.USE_ADJUSTED_LR}\\n\")\n                f.write(f\"MAX_LR: {self.config.MAX_LR}\\n\")\n                f.write(f\"MID_LR: {self.config.MID_LR}\\n\")\n                f.write(f\"MIN_LR: {self.config.MIN_LR}\\n\")\n                f.write(f\"RAMPUP_EPOCHS: {self.config.RAMPUP_EPOCHS}\\n\")\n                f.write(f\"SUSTAIN_EPOCHS: {self.config.SUSTAIN_EPOCHS}\\n\")\n            f.write(f\"DROPOUT_RATE: {self.config.DROPOUT_RATE}\\n\")\n            f.write(f\"ACTIVATION_FN: {self.config.ACTIVATION_FN}\\n\")\n            f.write(f\"FEATURES: {self.config.FEATURES}\\n\")\n            f.write(f\"LABELS: {self.config.LABELS}\\n\")\n            f.write(f\"PATCH_SHAPE: {self.config.PATCH_SHAPE}\\n\")\n            f.write(f\"CALLBACK_PARAMETER: {self.config.CALLBACK_PARAMETER}\\n\")\n            f.write(f\"MODEL_NAME: {self.config.MODEL_NAME}.h5\\n\")\n            f.write(f\"MODEL_CHECKPOINT_NAME: {self.config.MODEL_CHECKPOINT_NAME}.h5\\n\")\n\n        f.close()\n\n    def save_plots(self) -&gt; None:\n\"\"\"\n        Save plots and model visualization.\n\n        Saves the model architecture plot, training history plot, and model object.\n       \"\"\"\n        keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True)\n        with open(f\"{self.config.MODEL_SAVE_DIR}/model.pkl\", \"wb\") as f:\n            pickle.dump(self.history.history, f)\n        Utils.plot_metrics(self.config.METRICS, self.history.history, self.history.epoch, self.config.MODEL_SAVE_DIR)\n\n    def save_models(self) -&gt; None:\n\"\"\"\n        Save the trained models.\n\n        Saves the trained models in different formats: h5 and tf formats.\n        \"\"\"\n        self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n        self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n        self.model.save_weights(f\"{str(self.config.MODEL_SAVE_DIR)}/modelWeights.h5\", save_format=\"h5\")\n        self.model.save_weights(f\"{str(self.config.MODEL_SAVE_DIR)}/modelWeights.tf\", save_format=\"tf\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the ModelTrainer object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>An object containing the configuration settings for model training.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration settings for model training.</p> <code>model_builder</code> <p>An instance of ModelBuilder for building the model.</p> <code>build_model</code> <p>A partial function for building the model with the specified model type.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def __init__(self, config):\n\"\"\"\n    Initialize the ModelTrainer object.\n\n    Args:\n        config: An object containing the configuration settings for model training.\n\n    Attributes:\n        config: The configuration settings for model training.\n        model_builder: An instance of ModelBuilder for building the model.\n        build_model: A partial function for building the model with the specified model type.\n    \"\"\"\n    self.config = config\n    self.model_builder = ModelBuilder(\n        in_size=len(self.config.FEATURES),\n        out_classes=self.config.OUT_CLASS_NUM,\n        optimizer=self.config.OPTIMIZER,\n        loss=self.config.LOSS\n    )\n    self.build_model = partial(self.model_builder.build_model, model_type=self.config.MODEL_TYPE)\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.build_and_compile_model","title":"<code>build_and_compile_model(print_model_summary=True)</code>","text":"<p>Build and compile the model.</p> <p>Parameters:</p> Name Type Description Default <code>print_model_summary</code> <code>bool</code> <p>Flag indicating whether to print the model summary.</p> <code>True</code> <p>Builds and compiles the model using the provided configuration settings. Prints the model summary if print_model_summary is set to True.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def build_and_compile_model(self, print_model_summary: bool = True) -&gt; None:\n\"\"\"\n    Build and compile the model.\n\n    Args:\n        print_model_summary: Flag indicating whether to print the model summary.\n\n    Builds and compiles the model using the provided configuration settings.\n    Prints the model summary if print_model_summary is set to True.\n    \"\"\"\n    self.model = self.build_model(**self.config.__dict__)\n    if print_model_summary:  logging.info(self.model.summary())\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.configure_memory_growth","title":"<code>configure_memory_growth()</code>","text":"<p>Configure TensorFlow to allocate GPU memory dynamically.</p> <p>If GPUs are found, this method enables memory growth for each GPU.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def configure_memory_growth(self) -&gt; None:\n\"\"\"\n    Configure TensorFlow to allocate GPU memory dynamically.\n\n    If GPUs are found, this method enables memory growth for each GPU.\n    \"\"\"\n    if self.config.physical_devices:\n        logging.info(f\" &gt; Found {len(self.config.physical_devices)} GPUs\")\n        try:\n            for device in self.config.physical_devices:\n                tf.config.experimental.set_memory_growth(device, True)\n        except Exception as err:\n            logging.error(err)\n    else:\n        logging.info(\" &gt; No GPUs found\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.create_datasets","title":"<code>create_datasets(print_info=False)</code>","text":"<p>Create TensorFlow datasets for training, testing, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>print_info</code> <code>bool</code> <p>Flag indicating whether to print dataset information.</p> <code>False</code> <p>Prints information about the created datasets if print_info is set to True.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def create_datasets(self, print_info: bool = False) -&gt; None:\n\"\"\"\n    Create TensorFlow datasets for training, testing, and validation.\n\n    Args:\n        print_info: Flag indicating whether to print dataset information.\n\n    Prints information about the created datasets if print_info is set to True.\n    \"\"\"\n    self.TRAINING_DATASET = DataProcessor.get_dataset(\n        # self.config.TRAINING_FILES,\n        f\"{str(self.config.TRAINING_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        self.config.BATCH_SIZE,\n        self.config.OUT_CLASS_NUM,\n    ).repeat()\n    self.TESTING_DATASET = DataProcessor.get_dataset(\n        # self.config.TESTING_FILES,\n        f\"{str(self.config.TESTING_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        1,\n        self.config.OUT_CLASS_NUM,\n    ).repeat()\n    self.VALIDATION_DATASET = DataProcessor.get_dataset(\n        # self.config.VALIDATION_FILES,\n        f\"{str(self.config.VALIDATION_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        1,\n        self.config.OUT_CLASS_NUM,\n    )\n\n    if print_info:\n        logging.info(\"Printing dataset info:\")\n        DataProcessor.print_dataset_info(self.TRAINING_DATASET, \"Training\")\n        DataProcessor.print_dataset_info(self.TESTING_DATASET, \"Testing\")\n        DataProcessor.print_dataset_info(self.VALIDATION_DATASET, \"Validation\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.evaluate_and_print_val","title":"<code>evaluate_and_print_val()</code>","text":"<p>Evaluate and print validation metrics.</p> <p>Evaluates the model on the validation dataset and prints the metrics.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def evaluate_and_print_val(self) -&gt; None:\n\"\"\"\n    Evaluate and print validation metrics.\n\n    Evaluates the model on the validation dataset and prints the metrics.\n    \"\"\"\n    logging.info(\"************************************************\")\n    logging.info(\"************************************************\")\n    logging.info(\"Validation\")\n    evaluate_results = self.model.evaluate(self.config.VALIDATION_DATASET)\n    for name, value in zip(self.model.metrics_names, evaluate_results):\n        logging.info(f\"{name}: {value}\")\n    logging.info()\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.prepare_output_dir","title":"<code>prepare_output_dir()</code>","text":"<p>Prepare the output directory for saving models and results.</p> <p>Creates a directory with a timestamped name and increments the version number if necessary.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def prepare_output_dir(self) -&gt; None:\n\"\"\"\n    Prepare the output directory for saving models and results.\n\n    Creates a directory with a timestamped name and increments the version number if necessary.\n    \"\"\"\n    today = datetime.date.today().strftime(\"%Y_%m_%d\")\n    iterator = 1\n    while True:\n        self.model_dir_name = f\"trial_{self.config.MODEL_TYPE}_{today}_V{iterator}\"\n        self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.model_dir_name\n        try:\n            os.mkdir(self.config.MODEL_SAVE_DIR)\n        except FileExistsError:\n            logging.info(f\"&gt; {self.config.MODEL_SAVE_DIR} exists, creating another version...\")\n            iterator += 1\n            continue\n        break\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_models","title":"<code>save_models()</code>","text":"<p>Save the trained models.</p> <p>Saves the trained models in different formats: h5 and tf formats.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_models(self) -&gt; None:\n\"\"\"\n    Save the trained models.\n\n    Saves the trained models in different formats: h5 and tf formats.\n    \"\"\"\n    self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n    self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n    self.model.save_weights(f\"{str(self.config.MODEL_SAVE_DIR)}/modelWeights.h5\", save_format=\"h5\")\n    self.model.save_weights(f\"{str(self.config.MODEL_SAVE_DIR)}/modelWeights.tf\", save_format=\"tf\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_parameters","title":"<code>save_parameters()</code>","text":"<p>Save the training parameters to a text file.</p> <p>Saves the training parameters used in the configuration settings to a text file.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_parameters(self) -&gt; None:\n\"\"\"\n    Save the training parameters to a text file.\n\n    Saves the training parameters used in the configuration settings to a text file.\n    \"\"\"\n    with open(f\"{str(self.config.MODEL_SAVE_DIR)}/parameters.txt\", \"w\") as f:\n        f.write(f\"MODEL_FUNCTION: {self.config.MODEL_FUNCTION}\\n\")\n        f.write(f\"TRAIN_SIZE: {self.config.TRAIN_SIZE}\\n\")\n        f.write(f\"TEST_SIZE: {self.config.TEST_SIZE}\\n\")\n        f.write(f\"VAL_SIZE: {self.config.VAL_SIZE}\\n\")\n        f.write(f\"BATCH_SIZE: {self.config.BATCH_SIZE}\\n\")\n        f.write(f\"EPOCHS: {self.config.EPOCHS}\\n\")\n        f.write(f\"LOSS: {self.config.LOSS_TXT}\\n\")\n        f.write(f\"BUFFER_SIZE: {self.config.BUFFER_SIZE}\\n\")\n        f.write(f\"LEARNING_RATE: {self.config.LEARNING_RATE}\\n\")\n        if self.config.USE_ADJUSTED_LR:\n            f.write(f\"USE_ADJUSTED_LR: {self.config.USE_ADJUSTED_LR}\\n\")\n            f.write(f\"MAX_LR: {self.config.MAX_LR}\\n\")\n            f.write(f\"MID_LR: {self.config.MID_LR}\\n\")\n            f.write(f\"MIN_LR: {self.config.MIN_LR}\\n\")\n            f.write(f\"RAMPUP_EPOCHS: {self.config.RAMPUP_EPOCHS}\\n\")\n            f.write(f\"SUSTAIN_EPOCHS: {self.config.SUSTAIN_EPOCHS}\\n\")\n        f.write(f\"DROPOUT_RATE: {self.config.DROPOUT_RATE}\\n\")\n        f.write(f\"ACTIVATION_FN: {self.config.ACTIVATION_FN}\\n\")\n        f.write(f\"FEATURES: {self.config.FEATURES}\\n\")\n        f.write(f\"LABELS: {self.config.LABELS}\\n\")\n        f.write(f\"PATCH_SHAPE: {self.config.PATCH_SHAPE}\\n\")\n        f.write(f\"CALLBACK_PARAMETER: {self.config.CALLBACK_PARAMETER}\\n\")\n        f.write(f\"MODEL_NAME: {self.config.MODEL_NAME}.h5\\n\")\n        f.write(f\"MODEL_CHECKPOINT_NAME: {self.config.MODEL_CHECKPOINT_NAME}.h5\\n\")\n\n    f.close()\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_plots","title":"<code>save_plots()</code>","text":"<p>Save plots and model visualization.</p> <p>Saves the model architecture plot, training history plot, and model object.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_plots(self) -&gt; None:\n\"\"\"\n    Save plots and model visualization.\n\n    Saves the model architecture plot, training history plot, and model object.\n   \"\"\"\n    keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True)\n    with open(f\"{self.config.MODEL_SAVE_DIR}/model.pkl\", \"wb\") as f:\n        pickle.dump(self.history.history, f)\n    Utils.plot_metrics(self.config.METRICS, self.history.history, self.history.epoch, self.config.MODEL_SAVE_DIR)\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.start_training","title":"<code>start_training()</code>","text":"<p>Start the training process.</p> <p>Trains the model using the provided configuration settings and callbacks.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def start_training(self) -&gt; None:\n\"\"\"\n    Start the training process.\n\n    Trains the model using the provided configuration settings and callbacks.\n    \"\"\"\n    model_checkpoint = callbacks.ModelCheckpoint(\n        f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}.h5\",\n        monitor=self.config.CALLBACK_PARAMETER,\n        save_best_only=True,\n        mode=\"max\",\n        verbose=1,\n        save_weights_only=True,\n    )  # save best model\n\n    early_stopping = callbacks.EarlyStopping(\n        monitor=self.config.CALLBACK_PARAMETER,\n        patience=int(0.4 * self.config.EPOCHS),\n        verbose=1,\n        mode=\"max\",\n        restore_best_weights=True,\n    )\n    tensorboard = callbacks.TensorBoard(log_dir=str(self.config.MODEL_SAVE_DIR / \"logs\"), write_images=True)\n\n    def lr_scheduler(epoch):\n        if epoch &lt; self.config.RAMPUP_EPOCHS:\n            return self.config.MAX_LR\n        elif epoch &lt; self.config.RAMPUP_EPOCHS + self.config.SUSTAIN_EPOCHS:\n            return self.config.MID_LR\n        else:\n            return self.config.MIN_LR\n\n    lr_callback = callbacks.LearningRateScheduler(lambda epoch: lr_scheduler(epoch), verbose=True)\n\n    model_callbacks = [model_checkpoint, tensorboard]\n    if self.config.USE_ADJUSTED_LR:\n        model_callbacks.append(lr_callback)\n\n    self.model_callbacks = model_callbacks\n\n    self.history = self.model.fit(\n        x=self.TRAINING_DATASET,\n        epochs=self.config.EPOCHS,\n        steps_per_epoch=(self.config.TRAIN_SIZE // self.config.BATCH_SIZE),\n        validation_data=self.TESTING_DATASET,\n        validation_steps=(self.config.TEST_SIZE // self.config.BATCH_SIZE),\n        callbacks=model_callbacks,\n    )\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.train_model","title":"<code>train_model()</code>","text":"<p>Train the model using the provided configuration settings.</p> <p>This method performs the following steps: 1. Configures memory growth for TensorFlow. 2. Creates TensorFlow datasets for training, testing, and validation. 3. Builds and compiles the model. 4. Prepares the output directory for saving models and results. 5. Starts the training process. 6. Evaluates and prints validation metrics. 7. Saves training parameters, plots, and models.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def train_model(self) -&gt; None:\n\"\"\"\n    Train the model using the provided configuration settings.\n\n    This method performs the following steps:\n    1. Configures memory growth for TensorFlow.\n    2. Creates TensorFlow datasets for training, testing, and validation.\n    3. Builds and compiles the model.\n    4. Prepares the output directory for saving models and results.\n    5. Starts the training process.\n    6. Evaluates and prints validation metrics.\n    7. Saves training parameters, plots, and models.\n    \"\"\"\n    logging.info(\"****************************************************************************\")\n    print(f\"****************************** Configure memory growth... ************************\")\n    self.configure_memory_growth()\n    logging.info(\"****************************************************************************\")\n    logging.info(\"****************************** creating datasets... ************************\")\n    self.create_datasets(print_info=True)\n    logging.info(\"****************************************************************************\")\n    logging.info(\"************************ building and compiling model... *******************\")\n    self.build_and_compile_model(print_model_summary=True)\n    logging.info(\"****************************************************************************\")\n    logging.info(\"************************ preparing output directory... *********************\")\n    self.prepare_output_dir()\n    logging.info(\"****************************************************************************\")\n    logging.info(\"****************************** training model... ***************************\")\n    self.start_training()\n    self.evaluate_and_print_val()\n    self.save_parameters()\n    self.save_plots()\n    self.save_models()\n</code></pre>"},{"location":"remote_sensing/","title":"remote_sensing module","text":""},{"location":"remote_sensing/#aces.remote_sensing","title":"<code>aces.remote_sensing</code>","text":""},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures","title":"<code>RemoteSensingFeatures</code>","text":"<p>A class for generating remote sensing features using TensorFlow.</p> <p>This class provides static methods to compute various remote sensing indices and concatenate them into feature tensors.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>class RemoteSensingFeatures:\n\"\"\"\n    A class for generating remote sensing features using TensorFlow.\n\n    This class provides static methods to compute various remote sensing indices and concatenate them into feature tensors.\n    \"\"\"\n    @staticmethod\n    def normalized_difference(c1: tf.Tensor, c2: tf.Tensor, name: str = 'nd') -&gt; tf.Tensor:\n\"\"\"\n        Compute the normalized difference index between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the normalized difference index.\n\n        \"\"\"\n        nd_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1])), name=name)([c1, c2])\n        nd_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f'{name}_inf')([c1, c2])\n        return tf.where(tf.math.is_finite(nd_f), nd_f, nd_inf)\n\n    @staticmethod\n    def evi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'evi') -&gt; tf.Tensor:\n\"\"\"\n        Compute the enhanced vegetation index (EVI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the EVI.\n\n        \"\"\"\n        _evi = keras.layers.Lambda(lambda x: 2.5 * ((x[0] - x[1]) / (x[0] + 6 * x[1] - 7.5 * x[2] + 1)), name=name)([c1, c2, c3])\n        return _evi\n\n    @staticmethod\n    def savi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'savi') -&gt; tf.Tensor:\n\"\"\"\n        Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the SAVI.\n\n        \"\"\"\n        savi_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] + 0.5)) * 1.5, name=name)([c1, c2])\n        return savi_f\n\n    @staticmethod\n    def msavi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'msavi') -&gt; tf.Tensor:\n\"\"\"\n        Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the MSAVI.\n\n        \"\"\"\n        msavi_f = keras.layers.Lambda(lambda x: (((2 * x[0] + 1) - tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - 8 * (x[0] - x[1]))) / 2), name=name)([c1, c2])\n        return msavi_f\n\n    @staticmethod\n    def mtvi2(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'mtvi2') -&gt; tf.Tensor:\n\"\"\"\n        Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the MTVI2.\n\n        \"\"\"\n        mtvi2_f = keras.layers.Lambda(lambda x: (1.5 * (1.2 * (x[0] - x[2]) - 2.5 * (x[1] - x[2]))) / (tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - (6 * x[0] - 5 * tf.sqrt(x[1])) - 0.5)), name=name)([c1, c2, c3])\n        return mtvi2_f\n\n    @staticmethod\n    def vari(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'vari') -&gt; tf.Tensor:\n\"\"\"\n        Compute the visible atmospheric resistant index (VARI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the VARI.\n\n        \"\"\"\n        vari_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] - x[2])), name=name)([c1, c2, c3])\n        return vari_f\n\n    @staticmethod\n    def tgi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'tgi') -&gt; tf.Tensor:\n\"\"\"\n        Compute the triangular greenness index (TGI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the TGI.\n\n        \"\"\"\n        tgi_f = keras.layers.Lambda(lambda x: ((120 * (x[1] - x[2])) - (190 * (x[1] - x[0]))) / 2, name=name)([c1, c2, c3])\n        return tgi_f\n\n    @staticmethod\n    def ratio(c1: tf.Tensor, c2: tf.Tensor, name: str = 'ratio') -&gt; tf.Tensor:\n\"\"\"\n        Compute the ratio between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the ratio between the spectral bands.\n\n        \"\"\"\n        ratio_f = keras.layers.Lambda(lambda x: x[0] / x[1], name=name)([c1, c2])\n        ratio_inf = keras.layers.Lambda(lambda x: x[0], name=f'{name}_inf')([c1, c2])\n        return tf.where(tf.math.is_finite(ratio_f), ratio_f, ratio_inf)\n\n    @staticmethod\n    def nvi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'nvi') -&gt; tf.Tensor:\n\"\"\"\n        Compute the normalized vegetation index (NVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the NVI.\n\n        \"\"\"\n        nvi_f = keras.layers.Lambda(lambda x: x[0] / (x[0] + x[1]), name=name)([c1, c2])\n        nvi_inf = keras.layers.Lambda(lambda x: x[0], name=f'{name}_inf')([c1, c2])\n        return tf.where(tf.math.is_finite(nvi_f), nvi_f, nvi_inf)\n\n    @staticmethod\n    def diff_band(c1: tf.Tensor, c2: tf.Tensor, name: str = 'diff') -&gt; tf.Tensor:\n\"\"\"\n        Compute the difference between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the difference between the spectral bands.\n\n        \"\"\"\n        diff = keras.layers.Lambda(lambda x: x[0] - x[1], name=name)([c1, c2])\n        return diff\n\n    @staticmethod\n    def concatenate_features_for_cnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Concatenate remote sensing features for Convolutional Neural Network (CNN) input.\n\n        Args:\n            input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n        Returns:\n            A TensorFlow tensor representing the concatenated features for CNN input.\n\n        \"\"\"\n        red_before = input_tensor[:, :, :, 0:1]\n        green_before = input_tensor[:, :, :, 1:2]\n        blue_before = input_tensor[:, :, :, 2:3]\n        nir_before = input_tensor[:, :, :, 3:4]\n        red_during = input_tensor[:, :, :, 4:5]\n        green_during = input_tensor[:, :, :, 5:6]\n        blue_during = input_tensor[:, :, :, 6:7]\n        nir_during = input_tensor[:, :, :, 7:8]\n\n        ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name='ndvi_before')\n        ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name='ndvi_during')\n        evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name='evi_before')\n        evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name='evi_during')\n        ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name='ndwi_before')\n        ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name='ndwi_during')\n        savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name='savi_before')\n        savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name='savi_during')\n        msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name='msavi_before')\n        msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name='msavi_during')\n        mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name='mtvi2_before')\n        mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name='mtvi2_during')\n        vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name='vari_before')\n        vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name='vari_during')\n        tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name='tgi_before')\n        tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name='tgi_during')\n\n        red_diff1 = RemoteSensingFeatures.diff_band(red_before, red_during, name='diff1')\n        green_diff1 = RemoteSensingFeatures.diff_band(green_before, green_during, name='diff2')\n        blue_diff1 = RemoteSensingFeatures.diff_band(blue_before, blue_during, name='diff3')\n        nir_diff1 = RemoteSensingFeatures.diff_band(nir_before, nir_during, name='diff4')\n\n        return keras.layers.concatenate(\n            [ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n             msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during,\n             red_diff1, green_diff1, blue_diff1, nir_diff1],\n            name='input_features'\n        )\n\n    @staticmethod\n    def concatenate_features_for_dnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n        Args:\n            input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n        Returns:\n            A TensorFlow tensor representing the concatenated features for DNN input.\n\n        \"\"\"\n        red_before = input_tensor[:, :, 0:1]\n        green_before = input_tensor[:, :, 1:2]\n        blue_before = input_tensor[:, :, 2:3]\n        nir_before = input_tensor[:, :, 3:4]\n        red_during = input_tensor[:, :, 4:5]\n        green_during = input_tensor[:, :, 5:6]\n        blue_during = input_tensor[:, :, 6:7]\n        nir_during = input_tensor[:, :, 7:8]\n\n        ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name='ndvi_before')\n        ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name='ndvi_during')\n        evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name='evi_before')\n        evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name='evi_during')\n        ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name='ndwi_before')\n        ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name='ndwi_during')\n        savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name='savi_before')\n        savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name='savi_during')\n        msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name='msavi_before')\n        msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name='msavi_during')\n        mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name='mtvi2_before')\n        mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name='mtvi2_during')\n        vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name='vari_before')\n        vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name='vari_during')\n        tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name='tgi_before')\n        tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name='tgi_during')\n\n        return keras.layers.concatenate(\n            [ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n             msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during],\n            name='input_features'\n        )\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.concatenate_features_for_cnn","title":"<code>concatenate_features_for_cnn(input_tensor)</code>  <code>staticmethod</code>","text":"<p>Concatenate remote sensing features for Convolutional Neural Network (CNN) input.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the input remote sensing data.</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the concatenated features for CNN input.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef concatenate_features_for_cnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Concatenate remote sensing features for Convolutional Neural Network (CNN) input.\n\n    Args:\n        input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n    Returns:\n        A TensorFlow tensor representing the concatenated features for CNN input.\n\n    \"\"\"\n    red_before = input_tensor[:, :, :, 0:1]\n    green_before = input_tensor[:, :, :, 1:2]\n    blue_before = input_tensor[:, :, :, 2:3]\n    nir_before = input_tensor[:, :, :, 3:4]\n    red_during = input_tensor[:, :, :, 4:5]\n    green_during = input_tensor[:, :, :, 5:6]\n    blue_during = input_tensor[:, :, :, 6:7]\n    nir_during = input_tensor[:, :, :, 7:8]\n\n    ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name='ndvi_before')\n    ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name='ndvi_during')\n    evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name='evi_before')\n    evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name='evi_during')\n    ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name='ndwi_before')\n    ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name='ndwi_during')\n    savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name='savi_before')\n    savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name='savi_during')\n    msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name='msavi_before')\n    msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name='msavi_during')\n    mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name='mtvi2_before')\n    mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name='mtvi2_during')\n    vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name='vari_before')\n    vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name='vari_during')\n    tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name='tgi_before')\n    tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name='tgi_during')\n\n    red_diff1 = RemoteSensingFeatures.diff_band(red_before, red_during, name='diff1')\n    green_diff1 = RemoteSensingFeatures.diff_band(green_before, green_during, name='diff2')\n    blue_diff1 = RemoteSensingFeatures.diff_band(blue_before, blue_during, name='diff3')\n    nir_diff1 = RemoteSensingFeatures.diff_band(nir_before, nir_during, name='diff4')\n\n    return keras.layers.concatenate(\n        [ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n         msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during,\n         red_diff1, green_diff1, blue_diff1, nir_diff1],\n        name='input_features'\n    )\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.concatenate_features_for_dnn","title":"<code>concatenate_features_for_dnn(input_tensor)</code>  <code>staticmethod</code>","text":"<p>Concatenate remote sensing features for Deep Neural Network (DNN) input.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the input remote sensing data.</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the concatenated features for DNN input.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef concatenate_features_for_dnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n    Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n    Args:\n        input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n    Returns:\n        A TensorFlow tensor representing the concatenated features for DNN input.\n\n    \"\"\"\n    red_before = input_tensor[:, :, 0:1]\n    green_before = input_tensor[:, :, 1:2]\n    blue_before = input_tensor[:, :, 2:3]\n    nir_before = input_tensor[:, :, 3:4]\n    red_during = input_tensor[:, :, 4:5]\n    green_during = input_tensor[:, :, 5:6]\n    blue_during = input_tensor[:, :, 6:7]\n    nir_during = input_tensor[:, :, 7:8]\n\n    ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name='ndvi_before')\n    ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name='ndvi_during')\n    evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name='evi_before')\n    evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name='evi_during')\n    ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name='ndwi_before')\n    ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name='ndwi_during')\n    savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name='savi_before')\n    savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name='savi_during')\n    msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name='msavi_before')\n    msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name='msavi_during')\n    mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name='mtvi2_before')\n    mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name='mtvi2_during')\n    vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name='vari_before')\n    vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name='vari_during')\n    tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name='tgi_before')\n    tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name='tgi_during')\n\n    return keras.layers.concatenate(\n        [ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n         msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during],\n        name='input_features'\n    )\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.diff_band","title":"<code>diff_band(c1, c2, name='diff')</code>  <code>staticmethod</code>","text":"<p>Compute the difference between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'diff'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the difference between the spectral bands.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef diff_band(c1: tf.Tensor, c2: tf.Tensor, name: str = 'diff') -&gt; tf.Tensor:\n\"\"\"\n    Compute the difference between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the difference between the spectral bands.\n\n    \"\"\"\n    diff = keras.layers.Lambda(lambda x: x[0] - x[1], name=name)([c1, c2])\n    return diff\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.evi","title":"<code>evi(c1, c2, c3, name='evi')</code>  <code>staticmethod</code>","text":"<p>Compute the enhanced vegetation index (EVI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'evi'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the EVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef evi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'evi') -&gt; tf.Tensor:\n\"\"\"\n    Compute the enhanced vegetation index (EVI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the EVI.\n\n    \"\"\"\n    _evi = keras.layers.Lambda(lambda x: 2.5 * ((x[0] - x[1]) / (x[0] + 6 * x[1] - 7.5 * x[2] + 1)), name=name)([c1, c2, c3])\n    return _evi\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.msavi","title":"<code>msavi(c1, c2, name='msavi')</code>  <code>staticmethod</code>","text":"<p>Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'msavi'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the MSAVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef msavi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'msavi') -&gt; tf.Tensor:\n\"\"\"\n    Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the MSAVI.\n\n    \"\"\"\n    msavi_f = keras.layers.Lambda(lambda x: (((2 * x[0] + 1) - tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - 8 * (x[0] - x[1]))) / 2), name=name)([c1, c2])\n    return msavi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.mtvi2","title":"<code>mtvi2(c1, c2, c3, name='mtvi2')</code>  <code>staticmethod</code>","text":"<p>Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'mtvi2'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the MTVI2.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef mtvi2(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'mtvi2') -&gt; tf.Tensor:\n\"\"\"\n    Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the MTVI2.\n\n    \"\"\"\n    mtvi2_f = keras.layers.Lambda(lambda x: (1.5 * (1.2 * (x[0] - x[2]) - 2.5 * (x[1] - x[2]))) / (tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - (6 * x[0] - 5 * tf.sqrt(x[1])) - 0.5)), name=name)([c1, c2, c3])\n    return mtvi2_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.normalized_difference","title":"<code>normalized_difference(c1, c2, name='nd')</code>  <code>staticmethod</code>","text":"<p>Compute the normalized difference index between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'nd'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the normalized difference index.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef normalized_difference(c1: tf.Tensor, c2: tf.Tensor, name: str = 'nd') -&gt; tf.Tensor:\n\"\"\"\n    Compute the normalized difference index between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the normalized difference index.\n\n    \"\"\"\n    nd_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1])), name=name)([c1, c2])\n    nd_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f'{name}_inf')([c1, c2])\n    return tf.where(tf.math.is_finite(nd_f), nd_f, nd_inf)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.nvi","title":"<code>nvi(c1, c2, name='nvi')</code>  <code>staticmethod</code>","text":"<p>Compute the normalized vegetation index (NVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'nvi'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the NVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef nvi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'nvi') -&gt; tf.Tensor:\n\"\"\"\n    Compute the normalized vegetation index (NVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the NVI.\n\n    \"\"\"\n    nvi_f = keras.layers.Lambda(lambda x: x[0] / (x[0] + x[1]), name=name)([c1, c2])\n    nvi_inf = keras.layers.Lambda(lambda x: x[0], name=f'{name}_inf')([c1, c2])\n    return tf.where(tf.math.is_finite(nvi_f), nvi_f, nvi_inf)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.ratio","title":"<code>ratio(c1, c2, name='ratio')</code>  <code>staticmethod</code>","text":"<p>Compute the ratio between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'ratio'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the ratio between the spectral bands.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef ratio(c1: tf.Tensor, c2: tf.Tensor, name: str = 'ratio') -&gt; tf.Tensor:\n\"\"\"\n    Compute the ratio between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the ratio between the spectral bands.\n\n    \"\"\"\n    ratio_f = keras.layers.Lambda(lambda x: x[0] / x[1], name=name)([c1, c2])\n    ratio_inf = keras.layers.Lambda(lambda x: x[0], name=f'{name}_inf')([c1, c2])\n    return tf.where(tf.math.is_finite(ratio_f), ratio_f, ratio_inf)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.savi","title":"<code>savi(c1, c2, name='savi')</code>  <code>staticmethod</code>","text":"<p>Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'savi'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the SAVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef savi(c1: tf.Tensor, c2: tf.Tensor, name: str = 'savi') -&gt; tf.Tensor:\n\"\"\"\n    Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the SAVI.\n\n    \"\"\"\n    savi_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] + 0.5)) * 1.5, name=name)([c1, c2])\n    return savi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.tgi","title":"<code>tgi(c1, c2, c3, name='tgi')</code>  <code>staticmethod</code>","text":"<p>Compute the triangular greenness index (TGI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'tgi'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the TGI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef tgi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'tgi') -&gt; tf.Tensor:\n\"\"\"\n    Compute the triangular greenness index (TGI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the TGI.\n\n    \"\"\"\n    tgi_f = keras.layers.Lambda(lambda x: ((120 * (x[1] - x[2])) - (190 * (x[1] - x[0]))) / 2, name=name)([c1, c2, c3])\n    return tgi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.vari","title":"<code>vari(c1, c2, c3, name='vari')</code>  <code>staticmethod</code>","text":"<p>Compute the visible atmospheric resistant index (VARI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>tf.Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'vari'</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>A TensorFlow tensor representing the VARI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef vari(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = 'vari') -&gt; tf.Tensor:\n\"\"\"\n    Compute the visible atmospheric resistant index (VARI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the VARI.\n\n    \"\"\"\n    vari_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] - x[2])), name=name)([c1, c2, c3])\n    return vari_f\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#coming-soon","title":"Coming soon","text":""}]}