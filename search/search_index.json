{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACES (Agricultural Classification and Estimation Service)","text":"<p>ACES (Agricultural Classification and Estimation Service) is a Python module for generating training data and training machine learning models for remote sensing applications. It provides functionalities for data processing, data loading from Earth Engine, feature extraction, and model training.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -U servir-aces\n</code></pre> <p>For saving model plot in TensorFlow/Keras, you need <code>pydot</code> and Graphviz. To install, <code>pydot</code>, run:</p> <pre><code>pip install pydot\n</code></pre> <p>The installation process for Graphviz varies depending on your operating system.</p> <ul> <li>For macOS:\\ You can install Graphviz using Homebrew. If you don't have Homebrew installed, you can get it from https://brew.sh Once Homebrew is installed, run the following command:</li> </ul> <pre><code>brew install graphviz\n</code></pre> <ul> <li> <p>For Windows:\\ Download the installer from the Graphviz website (Graphviz Download Page), and follow the installation instructions. Make sure to add the path to the Graphviz bin folder to your system\u2019s PATH environment variable.</p> </li> <li> <p>For Linux (Ubuntu/Debian):\\ You can install Graphviz using apt:</p> </li> </ul> <pre><code>sudo apt-get install graphviz\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Data loading and processing from Earth Engine.</li> <li>Generation of training data for machine learning models.</li> <li>Training and evaluation of machine learning models (DNN, CNN, UNET).</li> <li>Inferences of the trained ML/DL models.</li> <li>Support for remote sensing feature extraction.</li> <li>Integration with Apache Beam for data processing.</li> </ul>"},{"location":"#usage","title":"Usage","text":""},{"location":"#setup","title":"Setup","text":"<p>ACES relies on environment variables defined in a <code>.env</code> file to configure various aspects of the training process. You can find an example <code>.env</code> file named <code>.env.example</code>. Copy this file and rename it to <code>config.env</code> in your project directory. Edit the variables within <code>config.env</code> to suit your specific needs. You will need to change several environment settings before running. Below are explained some of the environment configuration.</p>"},{"location":"#environment-configuration","title":"Environment Configuration","text":"<ul> <li> <p>BASEDIR (str): The base directory for the project. This is the root directory where all project-related files and subdirectories reside. It serves as the root directory for other folders containing data, outputs, etc.</p> </li> <li> <p>DATADIR (str): The directory for your data, and will be used to retrieve data necessary for training experiments. For this you can either specify the Google Cloud Storage (GCS) path starting with <code>gs://</code> or the path which can ben either an full absolute path or relative path to <code>BASEDIR</code>. This means, for example, the path to your data directory can be either <code>gs://aces-project/data</code> or the full absolute path as <code>/home/aces-project/data</code> or can be relative path to <code>BASEDIR</code>; so if <code>BASEDIR</code> is <code>/home/aces-project</code> and <code>DATADIR</code> is <code>data</code>, then the <code>DATADIR</code> is constructed as <code>/home/aces-project/data</code>. \\ Then your training, testing, and validation dataset should be placed as sub-directory inside the <code>DATADIR</code> with sub-directory named as \"training\", \"testing\", and \"validation\" respectively. This means, from above example, if <code>DATADIR</code> is <code>gs://aces-project/data</code> or <code>/home/aces-project/data</code>, the training, testing, and validation dataset should be made available at <code>gs://aces-project/data/training</code> or <code>/home/aces-project/data/training</code>, <code>gs://aces-project/data/testing</code> or <code>/home/aces-project/data/testing</code>, and <code>gs://aces-project/data/validation</code> or <code>/home/aces-project/data/validation</code> respectively.</p> </li> <li> <p>OUTPUT_DIR (str): This is the directory where all output files, such as trained models, evaluation results, model outpus, and other generated files during training, will be saved. Similar to <code>DATADIR</code> above, this can be either an absolute full path or a relative path to the <code>BASEDIR</code>. This means, for example, the path to your output directory can be either a full absolute path as <code>/home/aces-project/output</code> or can be relative path to <code>BASEDIR</code>; so if <code>BASEDIR</code> is <code>/home/aces-project</code> and <code>OUTPUT_DIR</code> is <code>data</code>, then the <code>OUTPUT_DIR</code> is constructed as <code>/home/aces-project/output</code>.</p> </li> <li> <p>MODEL_DIR_NAME (str): This is the sub-directory inside the <code>OUTPUT_DIR</code> where the output of the trained model and other relevant files. The rationale behind the <code>MODEL_DIR_NAME</code> is to provide a versioning mechanism. So, for example, if you're training a DNN model with different <code>BATCH_SIZE</code> say 32 and 64, you could run those two experiments with a <code>MODEL_DIR_NAME</code> as <code>dnn_batch_32</code> and <code>dnn_batch_64</code>, which saves those two experiments under that sub-directory name inside the <code>OUTPUT_DIR</code>. \\ Note: MODEL_DIR_NAME is used to construct the MODEL_DIR config parameter which can be accessed as config.MODEL_DIR.</p> </li> <li> <p>MODEL_TYPE (str): This variable defines the type of deep learning model you want to train The default is \"unet\". The available choices are \"dnn\", \"unet\", and \"cnn\". Note current version does not expose all the model intracacies through the environment file but future version may include those depending on the need.</p> </li> <li> <p>FEATURES (str): This variable specifies the feature names used in your training data. Each feature should correspond to a band or channel in your data. The features can be either specified as a comma-separated string, newline-separated string, or newline-with-comma separated string.\\ For example, <code>FEATURES</code> can be specified as comma-separated string as <code>FEATURES = \"red_before, green_before, blue_before, nir_before, red_during, green_during, blue_during, nir_during\"</code> or newline separated string as <pre><code>  FEATURES = \"red_before\n  green_before\n  blue_before\n  nir_before\n  red_during\n  green_during\n  blue_during\n  nir_during\"\n</code></pre> <pre><code>  or newline-with-comma separated string as\n</code></pre> <pre><code>  FEATURES = \"red_before,\n  green_before,\n  blue_before,\n  nir_before,\n  red_during,\n  green_during,\n  blue_during,\n  nir_during\"\n</code></pre></p> </li> <li> <p>LABELS (str): This variable specifies the target labels used in the training process. Similar to <code>FEATURES</code> above, you can specify this as either a comma-separated string, newline-separated string, or newline-with-comma separated string.</p> </li> <li> <p>PATCH_SHAPE (tuple): This variable specifies the shape of the single patch (WxH) used for training datasets. The number of channels are taken from <code>FEATURES</code>. This is useful for patch based algorithms like U-Net and CNN. This can be specified as a tuple of patch dimension. eg. <code>PATCH_SHAPE = (256, 256)</code></p> </li> <li> <p>TRAIN_SIZE, TEST_SIZE, VAL_SIZE (int): These variables define the number of samples used for training, testing, and validation, respectively. This can be speficied as the integer number. For example, <code>TRAIN_SIZE = 8531</code>, <code>TEST_SIZE = 1222</code>, <code>VAL_SIZE = 2404</code>.</p> </li> <li> <p>OUT_CLASS_NUM (int): This variable specifies the number of output classes in the classification model. This can be speficied as the integer number. For example, <code>OUT_CLASS_NUM = 5</code>.</p> </li> <li> <p>SCALE (int): This variable specifies the scale in which the analysis take place. This can be specified as the integer number. For example, <code>SCALE = 10</code>.</p> </li> <li> <p>DROPOUT_RATE (float): This variable specifies the dropout rate for the model. This should be specified as the float number between 0 and 1. For example, <code>DROPOUT_RATE = 0.2</code> Anything below zero would be set to 0 and anything above 1 would be set to 1. If not specified, the <code>DROPOUT_RATE</code> would be set to 0.</p> </li> <li> <p>LOSS (str): This variable specifies the loss function used for model training. Learn more about the loss function here. Use the named representation of the loss function. For example, in order to use <code>keras.losses.CategoricalCrossentropy</code>, you should specify <code>\"categorical_crossentropy\"</code>.</p> </li> <li> <p>ACTIVATION_FN (str): This variable specifies the activation function to be used for model training. Learn more about the activation function available here. Use the named representation of the activation function. For example, in order to use <code>keras.activations.softmax</code>, you should specify <code>\"softmax\"</code>.</p> </li> <li> <p>OPTIMIZER (str): This variable specifies the optimizer function to be used for model training. Learn more about the optimizer function available here. Use the named representation of the activation function. For example, in order to use <code>keras.optimizers.Adam</code>, you should specify <code>\"adam\"</code>. \\ Note current implementation does not expose all the parameters of the LOSS, ACTIVATION, OPTIMIZER but future version may include those depending on the need.</p> </li> <li> <p>MODEL_CHECKPOINT_NAME (str): This variable specifies the name to be used for the model checkpoints. Learn more about the model checkpoints here. This helps save the Keras model at some frequency.</p> </li> <li> <p>CALLBACK_PARAMETER (str): This variable specifies the call metric parameter that needs to be monitored. This is used as the monitor value in the ModelCheckPoint and EarlyStopping Callbacks. Learn more about callbacks here. If not specified, default value of <code>\"val_loss\"</code> is set.</p> </li> <li> <p>EARLY_STOPPING (bool): This variable specifies whether to use EarlyStopping callback or not. Learn more about callbacks here. The default value is False. If <code>EARLY_STOPPING</code> is set to True, the parameter to monitor is the <code>CALLBACK_PARAMETER</code> set above, and the <code>patience</code> argument is set to 30% of the total epochs defined by <code>EPOCHS</code> configuration.</p> </li> </ul>"},{"location":"#tutorial","title":"Tutorial","text":"<p>Note: We have a Jupyter Notebook Colab example that you can use to run without having to worry too much about setting up locally. You can find the relevant notebook here. Note, however, the resources especially GPU may not be fully available via Colab for unpaid version</p> <p>ACES relies on environment variables defined in a <code>.env</code> file to configure various aspects of the training process. You can find an example <code>.env</code> file named <code>.env.example</code>. Copy this file and rename it to <code>config.env</code> in your project directory. Edit the variables within <code>config.env</code> to suit your specific needs.</p> <p>Learn how to setup your <code>BASEDIR</code>, <code>DATADIR</code>, <code>OUTPUT_DIR</code> and <code>MODEL_DIR_NAME</code> from Usage above.</p> <p>For quickly running this, we have already prepared and exported the training datasets. They can be found at the Google Cloud Storage and we will use gsutil to get the dataset in our workspace. The dataset has training, testing, and validation subdirectory. Let's start by downloading these datasets in our workspace. Follow this link to install gsutil.</p> <p>Note: If you're looking to produce your own datasets, you can follow this notebook which was used to produce these training, testing, and validation datasets provided here.</p> <pre><code>mkdir path/to/your/project/data\ngsutil -m cp -r gs://dl-book/chapter-1/dnn_planet_wo_indices/* path/to/your/project/data\n</code></pre> <p>The parent folder (if you run <code>gsutil -m cp -r gs://dl-book/chapter-1/* DATADIR</code>) has several dataset inside it. We use <code>dnn_planet_wo_indices</code> here because it has lightweight data and would be much faster to run. If you want to test U-Net model, you can use <code>unet_256x256_planet_wo_indices</code> folder instead. Each of these have training, testing, and validation sub-folder inside them.</p> <p>We need to change some more parameters for this tutorial. We need to change the <code>MODEL_TYPE</code>. The default is \"unet\", let's change it to \"dnn\" since we will be training using that model.</p> <pre><code>MODEL_TYPE = \"dnn\"\n</code></pre> <p>Next define the <code>FEATURES</code> and the <code>LABELS</code> variables. You can refer to the Usage section above to learn more. But this dataset was prepared for the rice mapping application that uses before and during growing season information. So for this dataset here are <code>FEATURES</code>.</p> <pre><code>FEATURES = \"red_before\ngreen_before\nblue_before\nnir_before\nred_during\ngreen_during\nblue_during\nnir_during\"\n</code></pre> <p>Similarly, since the dataset has a single label, it is going to be as below.</p> <pre><code>LABELS = \"class\"\n</code></pre> <p>In addition, the training sizes (<code>TRAIN_SIZE</code>, <code>TEST_SIZE</code>, and <code>VAL_SIZE</code>) needs to be changed. For this dataset, we know our size before hand. <code>aces</code> also provides handful of functions that we can use to calculate this. See this notebook to learn more about how to do it. You should also change the <code>SCALE</code>, <code>DROPOUT_RATE</code>, <code>BATCH_SIZE</code>, <code>EPOCHS</code>, <code>LOSS</code>, <code>ACTIVATION</code>, <code>OUT_CLASS_NUM</code>, <code>OPTIMIZER</code>, <code>MODEL_CHECKPOINT_NAME</code> information as below.</p> <pre><code>TRAIN_SIZE = 8531\nTEST_SIZE = 1222\nVAL_SIZE = 2404\nSCALE = 10\nDROPOUT_RATE = 0.2\nBATCH_SIZE = 32\nEPOCHS = 30\nLOSS = \"categorical_crossentropy\"\nACTIVATION_FN = \"softmax\"\nOPTIMIZER = \"adam\"\nOUT_CLASS_NUM = 5\nMODEL_CHECKPOINT_NAME = \"modelCheckpoint\"\n</code></pre> <p>These settings should be good enough to get started. To view the complete settings and what they meant, you can view them here.</p>"},{"location":"#tutorial-configuration","title":"Tutorial Configuration","text":"<p>Here's the example configuration file for running this tutorial (<code>config.env</code>)</p> <pre><code>BASEDIR=\"/path/to/your/project\"\nDATADIR=\"data\"\nOUTPUT_DIR=\"output\"\nMODEL_DIR_NAME=\"dnn_v1\"\nMODEL_TYPE=\"dnn\"\nFEATURES=\"red_before\ngreen_before\nblue_before\nnir_before\nred_during\ngreen_during\nblue_during\nnir_during\"\nLABELS=\"class\"\nPATCH_SHAPE=(256, 256)\nTRAIN_SIZE=8531\nTEST_SIZE=1222\nVAL_SIZE=2404\nSCALE=10\nDROPOUT_RATE=0.2\nBATCH_SIZE=32\nEPOCHS=30\nLOSS=\"categorical_crossentropy\"\nACTIVATION_FN=\"softmax\"\nOPTIMIZER=\"adam\"\n# \"cropland_etc\", \"rice\", \"forest\", \"urban\", \"others_water_etc\"\nOUT_CLASS_NUM=5\nMODEL_CHECKPOINT_NAME=\"modelCheckpoint\"\n</code></pre>"},{"location":"#running-the-aces-module","title":"Running the ACES Module","text":"<p>After setting up your config.env file, you can use the ACES module as follows:</p> <pre><code>from aces.config import Config\nfrom aces.model_trainer import ModelTrainer\n\nconfig_file = \"config.env\"\nconfig = Config(config_file, override=True)\ntrainer = ModelTrainer(config)\ntrainer.train_model()\n</code></pre> <p>Once the model training is complete, it will save the trained model, evaluation results, plots, and other relevant files on the <code>MODEL_DIR</code> (sub-folder named <code>MODEL_DIR_NAME</code> inside the <code>OUTPUT_DIR</code>).</p>"},{"location":"#inference","title":"Inference","text":"<p>For inference, you would need to get the images in a right format. To export images in the right way, refer to this notebook, but for this, we already have the images prepared in the right way. You can download them in a similar manner you downloaded the training datasets. Change the IMAGEDIR to your appropriate directory.</p> <pre><code>mkdir IMAGEDIR\ngsutil -m cp -r gs://dl-book/chapter-1/images/* IMAGEDIR\n</code></pre> <p>There are few more settings that needs to be changed before we run the predictions. They are: - OUTPUT_NAME (str): This is the name of the output prediction for GEE asset, locally (in TF Format) and gcs output (in TFRecord format).</p> <ul> <li> <p>GCS_PROJECT (str): This is the name of the Google Cloud Project that will be used to push the output from GCS to GEE for the prediction.</p> </li> <li> <p>GCS_BUCKET (str): This is the name of the Google Cloud Bucket that will be used to store your prediction and should be inside the <code>GCS_PROJECT</code>.</p> </li> <li> <p>EE_OUTPUT_ASSET (str): This is the name of the variable that will be used as the output path to the asset in Google Earth Engine (GEE) that is used to push the predictions to.</p> </li> </ul> <p>So in addition to the already defined variables above, these are the additional example configuration for the prediction in the configuration file (<code>config.env</code>).</p> <pre><code>OUTPUT_NAME = \"prediction_dnn_v1\"\nGCS_PROJECT = \"your-gcs-project\"\nGCS_BUCKET = \"your-bucket\"\nEE_OUTPUT_ASSET = \"your-gee-output-asset-path\"\n</code></pre> <p>You will have to run the configuration settings again for the new configuration to take place.</p> <pre><code>from aces.config import Config\n\nconfig_file = \"config.env\"\nconfig = Config(config_file, override=True)\n</code></pre> <p>We can then start constructing the actual path for the output file using the <code>OUTPUT_NAME</code> as, and print it to view.</p> <pre><code>OUTPUT_IMAGE_FILE = str(config.MODEL_DIR / \"prediction\" / f\"{config.OUTPUT_NAME}.TFRecord\")\nprint(f\"OUTPUT_IMAGE_FILE: {OUTPUT_IMAGE_FILE}\")\n</code></pre> <p>Now let's get all the files inside the <code>IMAGEDIR</code>, and then separate out our actual image files and the JSON mixer file. The JSON mixer file inside the <code>IMAGEDIR</code> is generated when exporting images from GEE as TFRecords. This is a simple JSON file for defining the georeferencing of the patches.</p> <pre><code>import glob\n\nimage_files_list = []\njson_file = None\n\nfor f in glob.glob(f\"{IMAGEDIR}/*\"):\n    if f.endswith(\".tfrecord.gz\"):\n        image_files_list.append(f)\n    elif f.endswith(\".json\"):\n        json_file = f\n\n# Make sure the files are in the right order.\nimage_files_list.sort()\n</code></pre> <p>Next, we will load the trained model and look at the model summary. The trained model is stored within the trained-model subdirectory in the MODEL_DIR.</p> <pre><code>import tensorflow as tf\n\nprint(f\"Loading model from {str(config.MODEL_DIR)}/trained-model\")\nthis_model = tf.keras.models.load_model(f\"{str(config.MODEL_DIR)}/trained-model\")\n\nprint(this_model.summary())\n</code></pre> <p>Now let's get the relevant info from the JSON mixer file.</p> <pre><code>import json\n\nwith open(json_file, encoding='utf-8') as jm: mixer = json.load(jm)\n\n# Get relevant info from the JSON mixer file.\npatch_width = mixer[\"patchDimensions\"][0]\npatch_height = mixer[\"patchDimensions\"][1]\npatches = mixer[\"totalPatches\"]\npatch_dimensions_flat = [patch_width * patch_height, 1]\n</code></pre> <p>Next let's create a TFDataset from our images as:</p> <pre><code>def parse_image(example_proto):\n    columns = [\n        tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) for k in config.FEATURES\n    ]\n    image_features_dict = dict(zip(config.FEATURES, columns))\n    return tf.io.parse_single_example(example_proto, image_features_dict)\n\n\n# Create a dataset from the TFRecord file(s).\nimage_dataset = tf.data.TFRecordDataset(image_files_list, compression_type=\"GZIP\")\nimage_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n\n# Break our long tensors into many little ones.\nimage_dataset = image_dataset.flat_map(\n  lambda features: tf.data.Dataset.from_tensor_slices(features)\n)\n\n# Turn the dictionary in each record into a tuple without a label.\nimage_dataset = image_dataset.map(\n  lambda data_dict: (tf.transpose(list(data_dict.values())), )\n)\n\nimage_dataset = image_dataset.batch(patch_width * patch_height)\n</code></pre> <p>Finally, let's perform the prediction.</p> <pre><code>predictions = this_model.predict(image_dataset, steps=patches, verbose=1)\nprint(f\"predictions shape: {predictions.shape}\")\n</code></pre> <p>Now let's write this predictions on the file.</p> <pre><code>from pathlib import Path\nimport numpy as np\n\n# Create the target directory if it doesn't exist\nPath(OUTPUT_IMAGE_FILE).parent.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Writing predictions to {OUTPUT_IMAGE_FILE} ...\")\nwriter = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n\n# Every patch-worth of predictions we\"ll dump an example into the output\n# file with a single feature that holds our predictions. Since our predictions\n# are already in the order of the exported data, the patches we create here\n# will also be in the right order.\npatch = [[], [], [], [], [], []]\n\ncur_patch = 1\n\nfor i, prediction in enumerate(predictions):\n    patch[0].append(int(np.argmax(prediction)))\n    patch[1].append(prediction[0][0])\n    patch[2].append(prediction[0][1])\n    patch[3].append(prediction[0][2])\n    patch[4].append(prediction[0][3])\n    patch[5].append(prediction[0][4])\n\n\n    if i == 0:\n        print(f\"prediction.shape: {prediction.shape}\")\n\n    if (len(patch[0]) == patch_width * patch_height):\n        if cur_patch % 100 == 0:\n            print(\"Done with patch \" + str(cur_patch) + \" of \" + str(patches) + \"...\")\n\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature={\n                \"prediction\": tf.train.Feature(\n                    int64_list=tf.train.Int64List(\n                        value=patch[0])),\n                \"cropland_etc\": tf.train.Feature(\n                    float_list=tf.train.FloatList(\n                        value=patch[1])),\n                \"rice\": tf.train.Feature(\n                    float_list=tf.train.FloatList(\n                        value=patch[2])),\n                \"forest\": tf.train.Feature(\n                    float_list=tf.train.FloatList(\n                        value=patch[3])),\n                \"urban\": tf.train.Feature(\n                    float_list=tf.train.FloatList(\n                        value=patch[4])),\n                \"others_etc\": tf.train.Feature(\n                    float_list=tf.train.FloatList(\n                        value=patch[5])),\n                }\n            )\n        )\n\n        # Write the example to the file and clear our patch array so it\"s ready for\n        # another batch of class ids\n        writer.write(example.SerializeToString())\n        patch = [[], [], [], [], [], []]\n        cur_patch += 1\n\nwriter.close()\n</code></pre>"},{"location":"#uploading-predictions-to-gcp-and-gee","title":"Uploading Predictions to GCP and GEE","text":"<p>Now we have write the prediction to the <code>OUTPUT_IMAGE_FILE</code>. You can upload this to GEE for visualization. To do this, you will need to upload to GCP and then to GEE.</p> <p>You can upload to GCP using gsutil. The <code>OUTPUT_GCS_PATH</code> can be any path inside the <code>GCS_BUCKET</code> (e.g. <code>OUTPUT_GCS_PATH = f\"gs://{config.GCS_BUCKET}/{config.OUTPUT_NAME}.TFRecord\"</code>)</p> <pre><code>gsutil cp \"{OUTPUT_IMAGE_FILE}\" \"{OUTPUT_GCS_PATH}\"\n</code></pre> <p>Once the file is available on GCP, you can then upload to earthengine using <code>earthengine</code> command line. Learn more about earthengine command line tools options here.</p> <pre><code>earthengine upload image --asset_id={config.EE_OUTPUT_ASSET}/{config.OUTPUT_NAME} --pyramiding_policy=mode {OUTPUT_GCS_PATH} {json_file}\n</code></pre> <p>Note: Since this requires to use GCP and enable billing, we have hosted the prediction in the GCP so you don't have to. To use this prediction from GCP and upload to GEE, first make sure you are authenticated to earthengine using (learn more here to use different authentication techniques):</p> <pre><code>earthengine authenticate\n</code></pre> <p>You may need to set your project in earthengine to configure a default user project to be used for all API calls using the following command (replace <code>my-project</code> with your own project name). Learn more about authentication here.</p> <pre><code>earthengine set_project {my-project}\n</code></pre> <p>After that, let's say you want to upload it to your GEE asset to path say with <code>config.EE_OUTPUT_ASSET</code> as <code>users/biplov/aces_test</code> and <code>config.OUTPUT_NAME</code> as <code>prediction_dnn_v1</code> (you can directly use <code>{config.EE_OUTPUT_ASSET}</code> and <code>{config.OUTPUT_NAME}</code> as shown above), then do:</p> <pre><code>earthengine upload image --asset_id=users/biplov/aces_test/prediction_dnn_v1 --pyramiding_policy=mode gs://dl-book/chapter-1/prediction/prediction_dnn_v1.TFRecord gs://dl-book/chapter-1/images/image_2021mixer.json\n</code></pre> <p>This will give the message similar to below</p> <pre><code>Started upload task with ID: T3FMGOOXIXJKEAYXPS77MWDB\n</code></pre> <p>You can then go to GEE and check your <code>Tasks</code> tab to see that task with the given ID. Once complete, you can use this script to visualize your result. You can replace the asset with your own.</p> <p>Note: The inferencing is also available on this notebook, scroll to <code>Inference using Saved U-Net Model</code> or <code>Inference using Saved DNN Model</code> depending upon which model you're using.</p>"},{"location":"#tests","title":"Tests","text":"<p>Several tests are provided in the <code>tests/</code> folder covering the functionality of the package. Install the testing environment with <code>pytest</code>: <code>pip install pytest</code>.</p> <p>Then, run tests with pytest as:</p> <pre><code>pytest -v -s tests/\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions to ACES are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request on the GitHub repository.</p> <p>For Developers, to bump the version, refer here.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GNU General Public License v3.0.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"config/","title":"Config","text":"<p>ACES Configuration Module This module provides the configuration settings for the ACES project.</p>"},{"location":"config/#aces.config.Config","title":"<code> Config        </code>","text":"<p>ACES Configuration Class</p> <p>This class contains the configuration settings for the ACES project. These are generated from the .env file.</p> <p>Attributes:</p> Name Type Description <code>BASEDIR</code> <code>str</code> <p>The base directory for data I/O information.</p> <code>DATADIR</code> <code>str</code> <p>The directory for data collection experiments. DATADIR = BASEDIR / DATADIR</p> <code>MODEL_NAME</code> <code>str</code> <p>The name of the model.</p> <code>MODEL_CHECKPOINT_NAME</code> <code>str</code> <p>The name for model checkpoints.</p> <code>MODEL_DIR_NAME</code> <code>str</code> <p>The name of the model directory. MODEL_DIR = OUTPUT_DIR / MODEL_DIR_NAME</p> <code>AUTO_MODEL_DIR_NAME</code> <code>bool</code> <p>Flag to use automatic model directory naming.</p> <code>DATA_OUTPUT_DIR</code> <code>str</code> <p>The output directory for data export. Used in the workflow/v2/generate_training_patches script.</p> <code>FEATURES</code> <code>str</code> <p>The list of features used in the model.</p> <code>USE_ELEVATION</code> <code>bool</code> <p>Flag to use elevation data.</p> <code>USE_S1</code> <code>bool</code> <p>Flag to use Sentinel-1 data.</p> <code>LABELS</code> <code>list</code> <p>A list of labels used in the model.</p> <code>SCALE</code> <code>int</code> <p>The scale of the data.</p> <code>#</code> <code>Note</code> <p>The seeding componenet needs fixing. It is not working as expected.</p> <code>USE_SEED</code> <code>bool</code> <p>Flag to use a seed for reproducbility.</p> <code>SEED</code> <code>int</code> <p>The seed used for reproducibility.</p> <code>PATCH_SHAPE</code> <code>tuple</code> <p>The shape of training patches.</p> <code>KERNEL_BUFFER</code> <code>tuple</code> <p>The buffer for prediction purpose.</p> <code>TRAIN_SIZE</code> <code>int</code> <p>The size of the training dataset.</p> <code>TEST_SIZE</code> <code>int</code> <p>The size of the testing dataset.</p> <code>VAL_SIZE</code> <code>int</code> <p>The size of the validation dataset.</p> <code>BATCH_SIZE</code> <code>int</code> <p>The batch size for model training.</p> <code>EPOCHS</code> <code>int</code> <p>The number of epochs for model training.</p> <code>RAMPUP_EPOCHS</code> <code>int</code> <p>The number of ramp-up epochs.</p> <code>SUSTAIN_EPOCHS</code> <code>int</code> <p>The number of sustain epochs.</p> <code>USE_ADJUSTED_LR</code> <code>bool</code> <p>Flag to use adjusted learning rate.</p> <code>MAX_LR</code> <code>float</code> <p>The maximum learning rate.</p> <code>MID_LR</code> <code>float</code> <p>The intermediate learning rate.</p> <code>MIN_LR</code> <code>float</code> <p>The minimum learning rate.</p> <code>DROPOUT_RATE</code> <code>float</code> <p>The dropout rate for the model.</p> <code>CALLBACK_PARAMETER</code> <code>str</code> <p>The parameter used for callbacks.</p> <code>EARLY_STOPPING</code> <code>bool</code> <p>Flag to use early stopping.</p> <code>MODEL_TYPE</code> <code>str</code> <p>The type of model (current choices: cnn, dnn, unet).</p> <code>TRANSFORM_DATA</code> <code>bool</code> <p>Flag to transform data (rotation, flip) DNN</p> <code>ACTIVATION_FN</code> <code>str</code> <p>The activation function for the model.</p> <code>OPTIMIZER</code> <code>str</code> <p>The optimizer used for model training.</p> <code>#</code> <code>You can use any loss function available [here](https</code> <p>//www.tensorflow.org/api_docs/python/tf/keras/losses):</p> <code>#</code> <code>other available are</code> <p>custom_focal_tversky_loss</p> <code>LOSS</code> <code>str</code> <p>The loss function used for model training.,</p> <code>OUT_CLASS_NUM</code> <code>int</code> <p>The number of output classes.</p> <code>#</code> <code>This parameter is used in script [5. prediction_unet.py](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_unet.py)</p> <code>#</code> <code>and [host_vertex_ai.py](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/host_vertex_ai.py) files</p> <code>USE_BEST_MODEL_FOR_INFERENCE</code> <code>bool</code> <p>Flag to use the best model for inference.</p> <code>USE_SERVICE_ACCOUNT</code> <code>bool</code> <p>Flag to use a service account for Earth Engine.</p> <code>EE_SERVICE_CREDENTIALS</code> <code>str</code> <p>The path to the Earth Engine service account credentials.</p> <code>#</code> <code>Used in the script [5.prediction_unet.py](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_unet.py) and</p> <code>#</code> <code>[5.prediction_dnn](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_dnn.py)</p> <code>EE_OUPUT_ASSET</code> <code>str</code> <p>The Earth Engine output (prediction) asset path.</p> <code>OUTPUT_NAME</code> <code>str</code> <p>The name of the output prediction for GEE asset, locally (in TF Format) and gcs output (in TFRecord format).</p> <code>GCS_BUCKET</code> <code>str</code> <p>The Google Cloud Storage bucket name.</p> <code>#</code> <code>Exported to this dir from [4.export_image_for_prediction.py](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/4.export_image_for_prediction.py)</p> <code>GCS_IMAGE_DIR</code> <code>str</code> <p>The Google Cloud Storage image directory to store the image for prediction.</p> <code>GCS_IMAGE_PREFIX</code> <code>str</code> <p>The Google Cloud Storage image prefix.</p> <code>#</code> <code>Used in the script [host_vertex_ai.py](https</code> <p>//github.com/SERVIR/servir-aces/blob/main/workflow/v2/host_vertex_ai.py)</p> <code>GCS_PROJECT</code> <code>str</code> <p>The Google Cloud Storage project name.</p> <code>GCS_VERTEX_MODEL_SAVE_DIR</code> <code>str</code> <p>The Google Cloud Storage Vertex AI model save directory.</p> <code>GCS_REGION</code> <code>str</code> <p>The Google Cloud Storage region.</p> <code>GCS_VERTEX_CONTAINER_IMAGE</code> <code>str</code> <p>The Google Cloud Storage Vertex AI container image.</p> <code>USE_AI_PLATFORM</code> <code>bool</code> <p>Flag to use Google Cloud AI Platform.</p> <code>#</code> <code>Get machine type here</code> <code>GCP_MACHINE_TYPE</code> <code>str</code> <p>The Google Cloud Platform machine type.</p> Source code in <code>aces/config.py</code> <pre><code>class Config:\n    \"\"\"\n    ACES Configuration Class\n\n    This class contains the configuration settings for the ACES project. These are generated from the .env file.\n\n    Attributes:\n        BASEDIR (str): The base directory for data I/O information.\n        DATADIR (str): The directory for data collection experiments. DATADIR = BASEDIR / DATADIR\n        MODEL_NAME (str): The name of the model.\n        MODEL_CHECKPOINT_NAME (str): The name for model checkpoints.\n        MODEL_DIR_NAME (str): The name of the model directory. MODEL_DIR = OUTPUT_DIR / MODEL_DIR_NAME\n        AUTO_MODEL_DIR_NAME (bool): Flag to use automatic model directory naming.\n        DATA_OUTPUT_DIR (str): The output directory for data export. Used in the workflow/v2/generate_training_patches script.\n        # True generates as trial_MODELTYPE + datetime.now() + _v + version\n        # False uses the MODEL_DIR_NAME\n        FEATURES (str): The list of features used in the model.\n        USE_ELEVATION (bool): Flag to use elevation data.\n        USE_S1 (bool): Flag to use Sentinel-1 data.\n        LABELS (list): A list of labels used in the model.\n        SCALE (int): The scale of the data.\n        # Note: The seeding componenet needs fixing. It is not working as expected.\n        USE_SEED (bool): Flag to use a seed for reproducbility.\n        SEED (int): The seed used for reproducibility.\n        PATCH_SHAPE (tuple): The shape of training patches.\n        # buffer for prediction purpose\n        # Half this will extend on the sides of each patch.\n        # if zero; does not do buffer\n        # else specify the size as tuple (e.g. 128, 128)\n        KERNEL_BUFFER (tuple): The buffer for prediction purpose.\n        TRAIN_SIZE (int): The size of the training dataset.\n        TEST_SIZE (int): The size of the testing dataset.\n        VAL_SIZE (int): The size of the validation dataset.\n        BATCH_SIZE (int): The batch size for model training.\n        EPOCHS (int): The number of epochs for model training.\n        RAMPUP_EPOCHS (int): The number of ramp-up epochs.\n        SUSTAIN_EPOCHS (int): The number of sustain epochs.\n        USE_ADJUSTED_LR (bool): Flag to use adjusted learning rate.\n        MAX_LR (float): The maximum learning rate.\n        MID_LR (float): The intermediate learning rate.\n        MIN_LR (float): The minimum learning rate.\n        DROPOUT_RATE (float): The dropout rate for the model.\n        CALLBACK_PARAMETER (str): The parameter used for callbacks.\n        # patience for EARLY_STOPPING = int(0.3 * EPOCHS)\n        # monitors CALLBACK_PARAMETER\n        EARLY_STOPPING (bool): Flag to use early stopping.\n        MODEL_TYPE (str): The type of model (current choices: cnn, dnn, unet).\n        TRANSFORM_DATA (bool): Flag to transform data (rotation, flip) DNN\n        does not do augmentation.\n        ACTIVATION_FN (str): The activation function for the model.\n        OPTIMIZER (str): The optimizer used for model training.\n        # You can use any loss function available [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses):\n        # other available are: custom_focal_tversky_loss\n        LOSS (str): The loss function used for model training.,\n        OUT_CLASS_NUM (int): The number of output classes.\n        # This parameter is used in script [5. prediction_unet.py](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_unet.py)\n        # and [host_vertex_ai.py](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/host_vertex_ai.py) files\n        USE_BEST_MODEL_FOR_INFERENCE (bool): Flag to use the best model for inference.\n        USE_SERVICE_ACCOUNT (bool): Flag to use a service account for Earth Engine.\n        # This is used when USE_SERVICE_ACCOUNT is True\n        # This is used to\n        EE_SERVICE_CREDENTIALS (str): The path to the Earth Engine service account credentials.\n        # Used in the script [5.prediction_unet.py](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_unet.py) and\n        # [5.prediction_dnn](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/5.prediction_dnn.py)\n        EE_OUPUT_ASSET (str): The Earth Engine output (prediction) asset path.\n        OUTPUT_NAME (str): The name of the output prediction for GEE asset, locally (in TF Format) and gcs output (in TFRecord format).\n        GCS_BUCKET (str): The Google Cloud Storage bucket name.\n        # Exported to this dir from [4.export_image_for_prediction.py](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/4.export_image_for_prediction.py)\n        GCS_IMAGE_DIR (str): The Google Cloud Storage image directory to store the image for prediction.\n        # used as file_name_prefix = {GCS_IMAGE_DIR}/{GCS_IMAGE_PREFIX} for exporting image\n        GCS_IMAGE_PREFIX (str): The Google Cloud Storage image prefix.\n        # Used in the script [host_vertex_ai.py](https://github.com/SERVIR/servir-aces/blob/main/workflow/v2/host_vertex_ai.py)\n        GCS_PROJECT (str): The Google Cloud Storage project name.\n        GCS_VERTEX_MODEL_SAVE_DIR (str): The Google Cloud Storage Vertex AI model save directory.\n        GCS_REGION (str): The Google Cloud Storage region.\n        GCS_VERTEX_CONTAINER_IMAGE (str): The Google Cloud Storage Vertex AI container image.\n        # AI platform expects a need a serialized model, this parameter does this.\n        # See its uses in `data_processor.py` and `model_training.py`\n        USE_AI_PLATFORM (bool): Flag to use Google Cloud AI Platform.\n        # Get machine type here: https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n        GCP_MACHINE_TYPE (str): The Google Cloud Platform machine type.\n    \"\"\"\n\n    def __init__(self, config_file, override=False) -&gt; None:\n        \"\"\"\n        ACES Configuration Class Constructor\n\n        Args:\n            config_file (str): The path to the configuration file.\n            override (bool): Flag to override the configuration settings.\n        \"\"\"\n        load_dotenv(config_file, override=override)\n\n        self.BASEDIR = Path(os.getenv(\"BASEDIR\"))\n        _DATADIR = os.getenv(\"DATADIR\")\n        if _DATADIR.startswith(\"gs://\"):\n            self.DATADIR = _DATADIR\n            self.TRAINING_DIR = f\"{self.DATADIR}/training\"\n            self.TESTING_DIR = f\"{self.DATADIR}/testing\"\n            self.VALIDATION_DIR = f\"{self.DATADIR}/validation\"\n        else:\n            self.DATADIR = self.BASEDIR / os.getenv(\"DATADIR\")\n            self.TRAINING_DIR = self.DATADIR / \"training\"\n            self.TESTING_DIR = self.DATADIR / \"testing\"\n            self.VALIDATION_DIR = self.DATADIR / \"validation\"\n\n        print(f\"BASEDIR: {self.BASEDIR}\")\n        print(f\"checking if the DATADIR exists at: {self.DATADIR}\")\n        if not Path(f\"{self.DATADIR}\").is_dir():\n            raise FileNotFoundError(f\"The directory '{self.DATADIR}' does not exist.\")\n\n        self.OUTPUT_DIR = self.BASEDIR / os.getenv(\"OUTPUT_DIR\")\n\n        self.MODEL_NAME = os.getenv(\"MODEL_NAME\")\n        if self.MODEL_NAME is None:\n            self.MODEL_NAME = \"aces_model\"\n\n        self.MODEL_CHECKPOINT_NAME = os.getenv(\"MODEL_CHECKPOINT_NAME\")\n\n        self.MODEL_DIR_NAME = os.getenv(\"MODEL_DIR_NAME\")\n\n        self.MODEL_DIR = self.OUTPUT_DIR / self.MODEL_DIR_NAME\n        # Ensure MODEL_DIR directory exists, create if it doesn't\n        # this takes care of creating OUTPUT_DIR as well\n        print(f\"creating if MODEL_DIR does not exist at: {self.MODEL_DIR}\")\n        self.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n        self.AUTO_MODEL_DIR_NAME = os.getenv(\"AUTO_MODEL_DIR_NAME\") == \"True\"\n\n        self.DATA_OUTPUT_DIR = os.getenv(\"DATA_OUTPUT_DIR\")\n\n        self.SCALE = int(os.getenv(\"SCALE\"))\n\n        # self.FEATURES = os.getenv(\"FEATURES\").split(\"\\n\")\n        self.FEATURES = Utils.parse_params(\"FEATURES\")\n\n        self.USE_ELEVATION = os.getenv(\"USE_ELEVATION\") == \"True\"\n        self.USE_S1 = os.getenv(\"USE_S1\") == \"True\"\n\n        # self.LABELS = os.getenv(\"LABELS\").split(\"\\n\")\n        self.LABELS = Utils.parse_params(\"LABELS\")\n\n        if self.USE_ELEVATION:\n                self.FEATURES.extend([\"elevation\", \"slope\"])\n\n        if self.USE_S1:\n            self.FEATURES.extend([\"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n                                  \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"])\n\n        print(f\"using features: {self.FEATURES}\")\n        print(f\"using labels: {self.LABELS}\")\n\n        self.USE_SEED = os.getenv(\"USE_SEED\") == \"True\"\n        if self.USE_SEED:\n            self.SEED = int(os.getenv(\"SEED\"))\n\n        self.PRINT_INFO = os.getenv(\"PRINT_INFO\")\n        if self.PRINT_INFO is None:\n            self.PRINT_INFO = True\n        else:\n            self.PRINT_INFO = self.PRINT_INFO == \"True\"\n\n        # patch size for training\n        self.PATCH_SHAPE = ast.literal_eval(os.getenv(\"PATCH_SHAPE\"))\n        self.PATCH_SHAPE_SINGLE = self.PATCH_SHAPE[0]\n        KERNEL_BUFFER = os.getenv(\"KERNEL_BUFFER\")\n        if KERNEL_BUFFER == \"0\" or KERNEL_BUFFER is None:\n            self.KERNEL_BUFFER = None\n        else:\n            self.KERNEL_BUFFER = ast.literal_eval(KERNEL_BUFFER)\n\n        # Sizes of the testing, and evaluation datasets\n        self.TRAIN_SIZE = int(os.getenv(\"TRAIN_SIZE\"))\n        self.TEST_SIZE = int(os.getenv(\"TEST_SIZE\"))\n        self.VAL_SIZE = int(os.getenv(\"VAL_SIZE\"))\n\n        self.MODEL_TYPE = os.getenv(\"MODEL_TYPE\")\n        self.IS_DNN = True if self.MODEL_TYPE == \"dnn\" else False\n\n        self.BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\"))\n        self.EPOCHS = int(os.getenv(\"EPOCHS\"))\n\n        self.RAMPUP_EPOCHS = os.getenv(\"RAMPUP_EPOCHS\")\n        if self.RAMPUP_EPOCHS is not None:\n            self.RAMPUP_EPOCHS = int(self.RAMPUP_EPOCHS)\n\n        self.SUSTAIN_EPOCHS = os.getenv(\"SUSTAIN_EPOCHS\")\n        if self.RAMPUP_EPOCHS is not None and self.SUSTAIN_EPOCHS is None:\n            raise ValueError(\"SUSTAIN_EPOCHS must be set if RAMPUP_EPOCHS is set.\")\n\n        if self.SUSTAIN_EPOCHS is not None and self.RAMPUP_EPOCHS is not None:\n            self.SUSTAIN_EPOCHS = int(self.SUSTAIN_EPOCHS)\n\n        self.USE_ADJUSTED_LR = os.getenv(\"USE_ADJUSTED_LR\") == \"True\"\n        if self.USE_ADJUSTED_LR and not self.RAMPUP_EPOCHS and not self.SUSTAIN_EPOCHS:\n            raise ValueError(\"RAMPUP_EPOCHS and SUSTAIN_EPOCHS must be set if USE_ADJUSTED_LR is set.\")\n\n        if self.USE_ADJUSTED_LR:\n            try:\n                self.MAX_LR = float(os.getenv(\"MAX_LR\"))\n                self.MID_LR = float(os.getenv(\"MID_LR\"))\n                self.MIN_LR = float(os.getenv(\"MIN_LR\"))\n            except ValueError:\n                raise ValueError(\"MAX_LR, MID_LR, and MIN_LR must be set if USE_ADJUSTED_LR is set.\")\n\n        self.DROPOUT_RATE = os.getenv(\"DROPOUT_RATE\")\n        if self.DROPOUT_RATE is not None:\n            self.DROPOUT_RATE = float(self.DROPOUT_RATE)\n            if self.DROPOUT_RATE &lt; 0.:\n                self.DROPOUT_RATE = 0.\n            elif self.DROPOUT_RATE &gt; 1.:\n                self.DROPOUT_RATE = 1.\n        else:\n            self.DROPOUT_RATE = 0.\n\n        self.LOSS = os.getenv(\"LOSS\")\n\n        self.OPTIMIZER = os.getenv(\"OPTIMIZER\")\n\n        self.OUT_CLASS_NUM = int(os.getenv(\"OUT_CLASS_NUM\"))\n\n        self.USE_BEST_MODEL_FOR_INFERENCE = os.getenv(\"USE_BEST_MODEL_FOR_INFERENCE\") == \"True\"\n\n        self.ACTIVATION_FN = os.getenv(\"ACTIVATION_FN\")\n        if self.ACTIVATION_FN == \"sigmoid\" and self.OUT_CLASS_NUM &gt; 1:\n            raise ValueError(\"Sigmoid activation function is only for binary classification.\")\n        if self.ACTIVATION_FN == \"softmax\" and self.OUT_CLASS_NUM == 1:\n            raise ValueError(\"Softmax activation function is only for multi-class classification.\")\n\n        self.CALLBACK_PARAMETER = os.getenv(\"CALLBACK_PARAMETER\")\n        if self.CALLBACK_PARAMETER is None:\n            self.CALLBACK_PARAMETER = \"val_loss\"\n\n        self.EARLY_STOPPING = os.getenv(\"EARLY_STOPPING\") == \"True\"\n\n        # self.TRANSFORM_DATA = os.getenv(\"TRANSFORM_DATA\") == \"True\"\n        self.TRANSFORM_DATA = os.getenv(\"TRANSFORM_DATA\")\n        if self.TRANSFORM_DATA is None:\n            self.TRANSFORM_DATA = True\n        else:\n            self.TRANSFORM_DATA = self.TRANSFORM_DATA == \"True\"\n\n        # DERIVE_FEATURES &amp; ADDED_FEATURES are currently not available\n        # The original idea was if DERVIRE_FEATURES is True, then the ADDED_FEATURES are concatenated\n        # This is not fully implemented yet\n\n        # EE settings\n        self.USE_SERVICE_ACCOUNT = os.getenv(\"USE_SERVICE_ACCOUNT\") == \"True\"\n        self.EE_SERVICE_CREDENTIALS = os.getenv(\"EE_SERVICE_CREDENTIALS\")\n        self.EE_USER = os.getenv(\"EE_USER\")\n        self.EE_OUTPUT_ASSET = os.getenv(\"EE_OUTPUT_ASSET\")\n        self.OUTPUT_NAME = os.getenv(\"OUTPUT_NAME\")\n\n        # cloud stuff\n        self.GCS_PROJECT = os.getenv(\"GCS_PROJECT\")\n        self.GCS_BUCKET = os.getenv(\"GCS_BUCKET\")\n        self.GCS_IMAGE_DIR = os.getenv(\"GCS_IMAGE_DIR\")\n        self.GCS_IMAGE_PREFIX = os.getenv(\"GCS_IMAGE_PREFIX\")\n        self.GCS_VERTEX_MODEL_SAVE_DIR = os.getenv(\"GCS_VERTEX_MODEL_SAVE_DIR\")\n        self.GCS_REGION = os.getenv(\"GCS_REGION\")\n        self.GCS_VERTEX_CONTAINER_IMAGE = os.getenv(\"GCS_VERTEX_CONTAINER_IMAGE\")\n\n        self.USE_AI_PLATFORM = os.getenv(\"USE_AI_PLATFORM\") == \"True\"\n\n        self.GCP_MACHINE_TYPE = os.getenv(\"GCP_MACHINE_TYPE\")\n</code></pre>"},{"location":"config/#aces.config.Config.__init__","title":"<code>__init__(self, config_file, override=False)</code>  <code>special</code>","text":"<p>ACES Configuration Class Constructor</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>The path to the configuration file.</p> required <code>override</code> <code>bool</code> <p>Flag to override the configuration settings.</p> <code>False</code> Source code in <code>aces/config.py</code> <pre><code>def __init__(self, config_file, override=False) -&gt; None:\n    \"\"\"\n    ACES Configuration Class Constructor\n\n    Args:\n        config_file (str): The path to the configuration file.\n        override (bool): Flag to override the configuration settings.\n    \"\"\"\n    load_dotenv(config_file, override=override)\n\n    self.BASEDIR = Path(os.getenv(\"BASEDIR\"))\n    _DATADIR = os.getenv(\"DATADIR\")\n    if _DATADIR.startswith(\"gs://\"):\n        self.DATADIR = _DATADIR\n        self.TRAINING_DIR = f\"{self.DATADIR}/training\"\n        self.TESTING_DIR = f\"{self.DATADIR}/testing\"\n        self.VALIDATION_DIR = f\"{self.DATADIR}/validation\"\n    else:\n        self.DATADIR = self.BASEDIR / os.getenv(\"DATADIR\")\n        self.TRAINING_DIR = self.DATADIR / \"training\"\n        self.TESTING_DIR = self.DATADIR / \"testing\"\n        self.VALIDATION_DIR = self.DATADIR / \"validation\"\n\n    print(f\"BASEDIR: {self.BASEDIR}\")\n    print(f\"checking if the DATADIR exists at: {self.DATADIR}\")\n    if not Path(f\"{self.DATADIR}\").is_dir():\n        raise FileNotFoundError(f\"The directory '{self.DATADIR}' does not exist.\")\n\n    self.OUTPUT_DIR = self.BASEDIR / os.getenv(\"OUTPUT_DIR\")\n\n    self.MODEL_NAME = os.getenv(\"MODEL_NAME\")\n    if self.MODEL_NAME is None:\n        self.MODEL_NAME = \"aces_model\"\n\n    self.MODEL_CHECKPOINT_NAME = os.getenv(\"MODEL_CHECKPOINT_NAME\")\n\n    self.MODEL_DIR_NAME = os.getenv(\"MODEL_DIR_NAME\")\n\n    self.MODEL_DIR = self.OUTPUT_DIR / self.MODEL_DIR_NAME\n    # Ensure MODEL_DIR directory exists, create if it doesn't\n    # this takes care of creating OUTPUT_DIR as well\n    print(f\"creating if MODEL_DIR does not exist at: {self.MODEL_DIR}\")\n    self.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n    self.AUTO_MODEL_DIR_NAME = os.getenv(\"AUTO_MODEL_DIR_NAME\") == \"True\"\n\n    self.DATA_OUTPUT_DIR = os.getenv(\"DATA_OUTPUT_DIR\")\n\n    self.SCALE = int(os.getenv(\"SCALE\"))\n\n    # self.FEATURES = os.getenv(\"FEATURES\").split(\"\\n\")\n    self.FEATURES = Utils.parse_params(\"FEATURES\")\n\n    self.USE_ELEVATION = os.getenv(\"USE_ELEVATION\") == \"True\"\n    self.USE_S1 = os.getenv(\"USE_S1\") == \"True\"\n\n    # self.LABELS = os.getenv(\"LABELS\").split(\"\\n\")\n    self.LABELS = Utils.parse_params(\"LABELS\")\n\n    if self.USE_ELEVATION:\n            self.FEATURES.extend([\"elevation\", \"slope\"])\n\n    if self.USE_S1:\n        self.FEATURES.extend([\"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n                              \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"])\n\n    print(f\"using features: {self.FEATURES}\")\n    print(f\"using labels: {self.LABELS}\")\n\n    self.USE_SEED = os.getenv(\"USE_SEED\") == \"True\"\n    if self.USE_SEED:\n        self.SEED = int(os.getenv(\"SEED\"))\n\n    self.PRINT_INFO = os.getenv(\"PRINT_INFO\")\n    if self.PRINT_INFO is None:\n        self.PRINT_INFO = True\n    else:\n        self.PRINT_INFO = self.PRINT_INFO == \"True\"\n\n    # patch size for training\n    self.PATCH_SHAPE = ast.literal_eval(os.getenv(\"PATCH_SHAPE\"))\n    self.PATCH_SHAPE_SINGLE = self.PATCH_SHAPE[0]\n    KERNEL_BUFFER = os.getenv(\"KERNEL_BUFFER\")\n    if KERNEL_BUFFER == \"0\" or KERNEL_BUFFER is None:\n        self.KERNEL_BUFFER = None\n    else:\n        self.KERNEL_BUFFER = ast.literal_eval(KERNEL_BUFFER)\n\n    # Sizes of the testing, and evaluation datasets\n    self.TRAIN_SIZE = int(os.getenv(\"TRAIN_SIZE\"))\n    self.TEST_SIZE = int(os.getenv(\"TEST_SIZE\"))\n    self.VAL_SIZE = int(os.getenv(\"VAL_SIZE\"))\n\n    self.MODEL_TYPE = os.getenv(\"MODEL_TYPE\")\n    self.IS_DNN = True if self.MODEL_TYPE == \"dnn\" else False\n\n    self.BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\"))\n    self.EPOCHS = int(os.getenv(\"EPOCHS\"))\n\n    self.RAMPUP_EPOCHS = os.getenv(\"RAMPUP_EPOCHS\")\n    if self.RAMPUP_EPOCHS is not None:\n        self.RAMPUP_EPOCHS = int(self.RAMPUP_EPOCHS)\n\n    self.SUSTAIN_EPOCHS = os.getenv(\"SUSTAIN_EPOCHS\")\n    if self.RAMPUP_EPOCHS is not None and self.SUSTAIN_EPOCHS is None:\n        raise ValueError(\"SUSTAIN_EPOCHS must be set if RAMPUP_EPOCHS is set.\")\n\n    if self.SUSTAIN_EPOCHS is not None and self.RAMPUP_EPOCHS is not None:\n        self.SUSTAIN_EPOCHS = int(self.SUSTAIN_EPOCHS)\n\n    self.USE_ADJUSTED_LR = os.getenv(\"USE_ADJUSTED_LR\") == \"True\"\n    if self.USE_ADJUSTED_LR and not self.RAMPUP_EPOCHS and not self.SUSTAIN_EPOCHS:\n        raise ValueError(\"RAMPUP_EPOCHS and SUSTAIN_EPOCHS must be set if USE_ADJUSTED_LR is set.\")\n\n    if self.USE_ADJUSTED_LR:\n        try:\n            self.MAX_LR = float(os.getenv(\"MAX_LR\"))\n            self.MID_LR = float(os.getenv(\"MID_LR\"))\n            self.MIN_LR = float(os.getenv(\"MIN_LR\"))\n        except ValueError:\n            raise ValueError(\"MAX_LR, MID_LR, and MIN_LR must be set if USE_ADJUSTED_LR is set.\")\n\n    self.DROPOUT_RATE = os.getenv(\"DROPOUT_RATE\")\n    if self.DROPOUT_RATE is not None:\n        self.DROPOUT_RATE = float(self.DROPOUT_RATE)\n        if self.DROPOUT_RATE &lt; 0.:\n            self.DROPOUT_RATE = 0.\n        elif self.DROPOUT_RATE &gt; 1.:\n            self.DROPOUT_RATE = 1.\n    else:\n        self.DROPOUT_RATE = 0.\n\n    self.LOSS = os.getenv(\"LOSS\")\n\n    self.OPTIMIZER = os.getenv(\"OPTIMIZER\")\n\n    self.OUT_CLASS_NUM = int(os.getenv(\"OUT_CLASS_NUM\"))\n\n    self.USE_BEST_MODEL_FOR_INFERENCE = os.getenv(\"USE_BEST_MODEL_FOR_INFERENCE\") == \"True\"\n\n    self.ACTIVATION_FN = os.getenv(\"ACTIVATION_FN\")\n    if self.ACTIVATION_FN == \"sigmoid\" and self.OUT_CLASS_NUM &gt; 1:\n        raise ValueError(\"Sigmoid activation function is only for binary classification.\")\n    if self.ACTIVATION_FN == \"softmax\" and self.OUT_CLASS_NUM == 1:\n        raise ValueError(\"Softmax activation function is only for multi-class classification.\")\n\n    self.CALLBACK_PARAMETER = os.getenv(\"CALLBACK_PARAMETER\")\n    if self.CALLBACK_PARAMETER is None:\n        self.CALLBACK_PARAMETER = \"val_loss\"\n\n    self.EARLY_STOPPING = os.getenv(\"EARLY_STOPPING\") == \"True\"\n\n    # self.TRANSFORM_DATA = os.getenv(\"TRANSFORM_DATA\") == \"True\"\n    self.TRANSFORM_DATA = os.getenv(\"TRANSFORM_DATA\")\n    if self.TRANSFORM_DATA is None:\n        self.TRANSFORM_DATA = True\n    else:\n        self.TRANSFORM_DATA = self.TRANSFORM_DATA == \"True\"\n\n    # DERIVE_FEATURES &amp; ADDED_FEATURES are currently not available\n    # The original idea was if DERVIRE_FEATURES is True, then the ADDED_FEATURES are concatenated\n    # This is not fully implemented yet\n\n    # EE settings\n    self.USE_SERVICE_ACCOUNT = os.getenv(\"USE_SERVICE_ACCOUNT\") == \"True\"\n    self.EE_SERVICE_CREDENTIALS = os.getenv(\"EE_SERVICE_CREDENTIALS\")\n    self.EE_USER = os.getenv(\"EE_USER\")\n    self.EE_OUTPUT_ASSET = os.getenv(\"EE_OUTPUT_ASSET\")\n    self.OUTPUT_NAME = os.getenv(\"OUTPUT_NAME\")\n\n    # cloud stuff\n    self.GCS_PROJECT = os.getenv(\"GCS_PROJECT\")\n    self.GCS_BUCKET = os.getenv(\"GCS_BUCKET\")\n    self.GCS_IMAGE_DIR = os.getenv(\"GCS_IMAGE_DIR\")\n    self.GCS_IMAGE_PREFIX = os.getenv(\"GCS_IMAGE_PREFIX\")\n    self.GCS_VERTEX_MODEL_SAVE_DIR = os.getenv(\"GCS_VERTEX_MODEL_SAVE_DIR\")\n    self.GCS_REGION = os.getenv(\"GCS_REGION\")\n    self.GCS_VERTEX_CONTAINER_IMAGE = os.getenv(\"GCS_VERTEX_CONTAINER_IMAGE\")\n\n    self.USE_AI_PLATFORM = os.getenv(\"USE_AI_PLATFORM\") == \"True\"\n\n    self.GCP_MACHINE_TYPE = os.getenv(\"GCP_MACHINE_TYPE\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/SERVIR/servir-aces/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>servir-aces could always use more documentation, whether as part of the official servir-aces docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/SERVIR/servir-aces/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up aces for local development.</p> <ol> <li> <p>Fork the aces repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/servir-aces.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv servir-aces\n$ cd servir-aces/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.9 and later, and     for PyPy. Check https://github.com/SERVIR/servir-aces/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"data_processor/","title":"data_processor module","text":"<p> ACES Data Processor Module: </p> <p>This module provides functions for data input/output and preprocessing for the ACES project.</p>"},{"location":"data_processor/#aces.data_processor.DataProcessor","title":"<code> DataProcessor        </code>","text":"<p>ACES Data processor Class:</p> <p>This class provides functions for data input/output and preprocessing for the ACES project.</p> Source code in <code>aces/data_processor.py</code> <pre><code>class DataProcessor:\n    \"\"\"\n    ACES Data processor Class:\n\n    This class provides functions for data input/output and preprocessing for the ACES project.\n    \"\"\"\n\n    @staticmethod\n    @tf.autograph.experimental.do_not_convert\n    def create_tfrecord_from_file(filename: str) -&gt; tf.data.TFRecordDataset:\n        \"\"\"\n        Create a TensorFlow Dataset from a TFRecord file.\n\n        Parameters:\n\n        * filename (str): The filename of the TFRecord file.\n\n        Returns:\n\n        * tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.\n        \"\"\"\n        return tf.data.TFRecordDataset(filename, compression_type=\"GZIP\")\n\n    @staticmethod\n    @tf.autograph.experimental.do_not_convert\n    def get_sum_tensor(records):\n        \"\"\"\n        Gets the total number of tensor record by mapping through them.\n\n        Parameters:\n\n        * records: The input tensor records.\n\n        Returns:\n\n        * tf.Tensor: The total number of tensor records.\n        \"\"\"\n        dataset = records.map(lambda x, y: tf.constant(1, dtype=tf.int64), num_parallel_calls=tf.data.AUTOTUNE)\n        # ignores any error encountered while reading the records\n        # works with v2.x\n        dataset = dataset.apply(tf.data.experimental.ignore_errors())\n        n_tensors = dataset.reduce(np.int64(0), lambda x, y: x + y).numpy()\n        return n_tensors\n\n    @staticmethod\n    def calculate_n_samples(**config):\n        \"\"\"\n        Calculate the number of samples in the training, testing, and validation datasets.\n\n        Parameters:\n\n        **config: The configuration settings.\n\n        Returns:\n\n        * int: The number of training samples.\n\n        * int: The number of testing samples.\n\n        * int: The number of validation samples.\n\n        \"\"\"\n        parser_tupler = partial(DataProcessor.parse_tfrecord,\n                         patch_size=config.get(\"PATCH_SHAPE_SINGLE\"),\n                         features=config.get(\"FEATURES\"),\n                         labels=config.get(\"LABELS\"),\n                         depth=config.get(\"OUT_CLASS_NUM\"))\n\n        tf_training_records = tf.data.Dataset.list_files(f\"{str(config.get('TRAINING_DIR'))}/*\")\\\n                                             .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_training_records = tf_training_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n\n\n        if config.get(\"PRINT_DATASET\", False):\n            DataProcessor.print_dataset_info(tf_training_records, \"Training\")\n\n        n_training_records = DataProcessor.get_sum_tensor(tf_training_records)\n\n        tf_testing_records = tf.data.Dataset.list_files(f\"{str(config.get('TESTING_DIR'))}/*\")\\\n                                            .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_testing_records = tf_testing_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        n_testing_records = DataProcessor.get_sum_tensor(tf_testing_records)\n\n        tf_validation_records = tf.data.Dataset.list_files(f\"{str(config.get('VALIDATION_DIR'))}/*\")\\\n                                               .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n        tf_validation_records = tf_validation_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        n_validation_records = DataProcessor.get_sum_tensor(tf_validation_records)\n\n        return n_training_records, n_testing_records, n_validation_records\n\n    @staticmethod\n    def print_dataset_info(dataset: tf.data.Dataset, dataset_name: str) -&gt; None:\n        \"\"\"\n        Print information about a dataset.\n\n        Parameters:\n\n        * dataset (tf.data.Dataset): The dataset to print information about.\n\n        * dataset_name (str): The name of the dataset.\n        \"\"\"\n        print(dataset_name)\n        for inputs, outputs in dataset.take(1):\n            try:\n                print(f\"inputs: {inputs.dtype.name} {inputs.shape}\")\n                print(inputs)\n                print(f\"outputs: {outputs.dtype.name} {outputs.shape}\")\n                print(outputs)\n            except:\n                print(f\" &gt; inputs:\")\n                for name, values in inputs.items():\n                    print(f\"    {name}: {values.dtype.name} {values.shape}\")\n                # print(f\"    example \\n: {dataset.take(1)}\")\n                print(f\" &gt; outputs: {outputs.dtype.name} {outputs.shape}\")\n\n    @staticmethod\n    @tf.function\n    def random_transform(dataset: tf.Tensor, label: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Apply random transformations to a dataset.\n\n        Parameters:\n\n        * dataset (tf.Tensor): The input dataset.\n\n        Returns:\n\n        * tf.Tensor: The transformed dataset.\n        \"\"\"\n        x = tf.random.uniform(())\n        if x &lt; 0.10:\n            dataset = tf.image.flip_left_right(dataset)\n            label = tf.image.flip_left_right(label)\n        elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n            dataset = tf.image.flip_up_down(dataset)\n            label = tf.image.flip_up_down(label)\n        elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n            dataset = tf.image.flip_left_right(tf.image.flip_up_down(dataset))\n            label = tf.image.flip_left_right(tf.image.flip_up_down(label))\n        elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n            dataset = tf.image.rot90(dataset, k=1)\n            label = tf.image.rot90(label, k=1)\n        elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n            dataset = tf.image.rot90(dataset, k=2)\n            label = tf.image.rot90(label, k=2)\n        elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n            dataset = tf.image.rot90(dataset, k=3)\n            label = tf.image.rot90(label, k=3)\n        elif tf.math.logical_and(x &gt;= 0.60, x &lt; 0.70):\n            dataset = tf.image.flip_left_right(tf.image.rot90(dataset, k=2))\n            label = tf.image.flip_left_right(tf.image.rot90(label, k=2))\n        else:\n            dataset = dataset\n            label = label\n\n        return dataset, label\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None, depth: int = 1) -&gt; tf.data.Dataset:\n        \"\"\"\n        Parse a TFRecord example.\n\n        Parameters:\n        * example_proto (tf.Tensor): The example to parse.\n\n        * patch_size (int): The size of the patch.\n\n        * features (list, optional): The list of feature names to include. Default is None.\n\n        * labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n\n        * tf.data.Dataset: The parsed dataset.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        inputs = tf.io.parse_single_example(example_proto, proto_struct)\n        inputs_list = [inputs.get(key) for key in keys]\n        stacked = tf.stack(inputs_list, axis=0)\n        stacked = tf.transpose(stacked, [1, 2, 0])\n        label = stacked[:, :, len(features):]\n        y = tf.one_hot(tf.cast(label[:, :, -1], tf.uint8), depth)\n        return stacked[:, :, :len(features)], y\n\n    @staticmethod\n    @tf.function\n    def to_tuple(dataset: tf.Tensor, n_features: int = None, inverse_labels: bool = False) -&gt; tuple:\n        \"\"\"\n        Convert a dataset to a tuple of features and labels.\n\n        Parameters:\n\n        * dataset (tf.Tensor): The input dataset.\n\n        * n_features (int, optional): The number of features. Default is None.\n\n        * inverse_labels (bool, optional): Whether to inverse the labels. Default is False.\n\n        Returns:\n\n        * tuple: A tuple containing the features and labels.\n        \"\"\"\n        features = dataset[:, :, :, :n_features]\n        labels = dataset[:, :, :, n_features:]\n        if inverse_labels:\n            labels_inverse = tf.math.abs(labels - 1)\n            labels = tf.concat([labels_inverse, labels], axis=-1)\n        return features, labels\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_with_name(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n        \"\"\"\n        Parse a TFRecord example with named features.\n\n        Parameters:\n\n        * example_proto (tf.Tensor): The example to parse.\n\n        * patch_size (int): The size of the patch.\n\n        * features (list, optional): The list of feature names to include. Default is None.\n\n        * labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n\n        * tf.data.Dataset: The parsed dataset.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        return tf.io.parse_single_example(example_proto, proto_struct)\n\n    @staticmethod\n    @tf.function\n    def to_tuple_with_name(inputs: tf.Tensor, features: list = None, labels: list = None, n_classes: int = 1) -&gt; tuple:\n        \"\"\"\n        Convert inputs with named features to a tuple of features and one-hot encoded labels.\n\n        Parameters:\n\n        * inputs (tf.Tensor): The input dataset.\n\n        * features (list, optional): The list of feature names. Default is None.\n\n        * labels (list, optional): The list of label names. Default is None.\n\n        * n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.\n\n        Returns:\n\n        * tuple: A tuple containing the features and one-hot encoded labels.\n        \"\"\"\n        return (\n            {name: inputs[name] for name in features},\n            tf.one_hot(tf.cast(inputs[labels[0]], tf.uint8), n_classes)\n        )\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_dnn(example_proto: tf.Tensor, features: list = None, labels: list = None) -&gt; tuple:\n        \"\"\"\n        Parse a TFRecord example for DNN models.\n\n        Parameters:\n\n        * example_proto (tf.Tensor): The example to parse.\n\n        * features (list, optional): The list of feature names to include. Default is None.\n\n        * labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n\n        * tuple: A tuple containing the parsed features and labels.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n        label = parsed_features.pop(labels[0])\n        label = tf.cast(label, tf.int32)\n        return parsed_features, label\n\n    @staticmethod\n    @tf.function\n    def to_tuple_dnn(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n        \"\"\"\n        Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n        Parameters:\n\n        * dataset (dict): The input dataset.\n\n        * label (tf.Tensor): The label.\n\n        * depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n        Returns:\n\n        * tuple: A tuple containing the features and one-hot encoded labels.\n        \"\"\"\n        return tf.transpose(list(dataset.values())), tf.one_hot(indices=label, depth=depth)\n\n    @staticmethod\n    def to_tuple_dnn_ai_platform(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n        \"\"\"\n        Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n        Parameters:\n\n        * dataset (dict): The input dataset.\n\n        * label (tf.Tensor): The label.\n\n        * depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n        Returns:\n\n        * tuple: A tuple containing the features and one-hot encoded labels.\n        \"\"\"\n          # (1) -&gt; (1, 1, 1)\n        return ({k: [[v]] for k, v in dataset.items()}, tf.expand_dims(tf.one_hot(label, depth), axis=0))\n\n\n    @staticmethod\n    @tf.function\n    def parse_tfrecord_multi_label(example_proto: tf.data.Dataset, patch_size: int, features: list = None, labels: list = None) -&gt; tuple:\n        \"\"\"\n        Parse a TFRecord example with multiple labels.\n\n        Parameters:\n\n        * example_proto (tf.data.Dataset): The example to parse.\n\n        * patch_size (int): The size of the patch.\n\n        * features (list, optional): The list of feature names to include. Default is None.\n\n        * labels (list, optional): The list of label names to include. Default is None.\n\n        Returns:\n\n        * tuple: A tuple containing the parsed features and labels.\n        \"\"\"\n        keys = features + labels\n        columns = [\n            tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n        ]\n        proto_struct = dict(zip(keys, columns))\n        parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n        label = parsed_features.pop(labels[0])\n        return parsed_features, label\n\n    @staticmethod\n    @tf.function\n    def to_tuple_multi_label(dataset: dict, label: tf.Tensor, depth: int = 1, x_only: bool = False) -&gt; tuple:\n        \"\"\"\n        Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n        Parameters:\n\n        * dataset (tuple): The input dataset.\n\n        * n_labels (int, optional): The number of labels. Default is 1.\n\n        Returns:\n\n        * tuple: A tuple containing the features and multi-hot encoded labels.\n        \"\"\"\n        label = tf.cast(label, tf.uint8)\n        label = tf.one_hot(indices=label, depth=depth)\n        parsed_dataset = {k: tf.expand_dims(v, axis=2) for k, v in dataset.items()}\n        if x_only:\n            return parsed_dataset\n        return parsed_dataset, label\n\n\n    @staticmethod\n    @tf.function\n    def to_tuple_multi_label_ai_platform(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n        \"\"\"\n        Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n        Parameters:\n\n        * dataset (tuple): The input dataset.\n\n        * n_labels (int, optional): The number of labels. Default is 1.\n\n        Returns:\n\n        * tuple: A tuple containing the features and multi-hot encoded labels.\n        \"\"\"\n        label = tf.cast(label, tf.uint8)\n        label = tf.one_hot(indices=label, depth=depth)\n        parsed_dataset = {k: tf.expand_dims(v, axis=2) for k, v in dataset.items()}\n        return parsed_dataset, label\n\n\n    @staticmethod\n    def _get_dataset(files: list, features: list, labels: list, patch_shape: list, batch_size: int, buffer_size: int = 1000, training: bool = False, **kwargs) -&gt; tf.data.Dataset:\n        \"\"\"\n        Get a TFRecord dataset.\n\n        Parameters:\n        filenames (list): The list of file names.\n        patch_size (int): The size of the patch.\n        features (list, optional): The list of feature names to include. Default is None.\n        labels (list, optional): The list of label names to include. Default is None.\n        batch_size (int, optional): The batch size. Default is 1.\n        shuffle (bool, optional): Whether to shuffle the dataset. Default is False.\n        n_labels (int, optional): The number of labels. Default is 1.\n        num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n        drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n        cache (bool, optional): Whether to cache the dataset. Default is False.\n\n        Returns:\n\n        tf.data.Dataset: The TFRecord dataset.\n        \"\"\"\n        dnn = kwargs.get('dnn', False)\n        inverse_labels = kwargs.get('inverse_labels', False)\n        depth = kwargs.get('depth', len(labels))\n        multi_label_unet = kwargs.get('multi_label_unet', False)\n        dataset = tf.data.TFRecordDataset(files, compression_type='GZIP')\n\n        if dnn:\n            parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n            split_data = partial(DataProcessor.to_tuple_dnn, depth=depth)\n            dataset = dataset.map(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.batch(batch_size)\n            return dataset\n\n        if multi_label_unet:\n            parser = partial(DataProcessor.parse_tfrecord_multi_label, features=features, labels=labels, patch_shape=patch_shape)\n            split_data = partial(DataProcessor.to_tuple_multi_label, n_features=len(features), depth=depth)\n            dataset = dataset.interleave(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            if training:\n                dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True).batch(batch_size) \\\n                                 .map(DataProcessor.random_transform, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n                                 .map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            else:\n                dataset = dataset.batch(batch_size).map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n            return dataset\n\n        parser = partial(DataProcessor.parse_tfrecord, features=features, labels=labels)\n        split_data = partial(DataProcessor.to_tuple, n_features=len(features), inverse_labels=inverse_labels)\n        dataset = dataset.interleave(parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        if training:\n            dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True).batch(batch_size) \\\n                             .map(DataProcessor.random_transform, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n                             .map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        else:\n            dataset = dataset.batch(batch_size).map(split_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        return dataset\n\n    @staticmethod\n    def get_dataset(pattern: str, features: list, labels: list, patch_size: int, batch_size: int, n_classes: int = 1, **kwargs) -&gt; tf.data.Dataset:\n        \"\"\"\n        Get a TFRecord dataset.\n\n        Parameters:\n        * filenames (list): The list of file names.\n\n        * patch_size (int): The size of the patch.\n\n        * (list, optional): The list of feature names to include. Default is None.\n\n        * labels (list, optional): The list of label names to include. Default is None.\n\n        * batch_size (int, optional): The batch size. Default is 1.\n\n        * shuffle(bool, optional): Whether to shuffle the dataset. Default is False.\n\n        * n_labels (int, optional): The number of labels. Default is 1.\n\n        * num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n\n        * drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n\n        * cache (bool, optional): Whether to cache the dataset. Default is False.\n\n        Returns:\n        * tf.data.Dataset: The TFRecord dataset.\n        \"\"\"\n        print(f\"Loading dataset from {pattern}\")\n\n        dataset = tf.data.Dataset.list_files(pattern).interleave(DataProcessor.create_tfrecord_from_file)\n\n        if kwargs.get(\"IS_DNN\", False):\n            if kwargs.get(\"USE_AI_PLATFORM\", False):\n                parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n                tupler = partial(DataProcessor.to_tuple_dnn_ai_platform, depth=n_classes)\n            else:\n                parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n                tupler = partial(DataProcessor.to_tuple_dnn, depth=n_classes)\n\n            dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n            dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n            dataset = dataset.shuffle(512)\n            dataset = dataset.batch(batch_size)\n            dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n            return dataset\n\n        if kwargs.get(\"USE_AI_PLATFORM\", False):\n            parser = partial(DataProcessor.parse_tfrecord_multi_label, patch_size=patch_size, features=features, labels=labels)\n            tupler = partial(DataProcessor.to_tuple_multi_label_ai_platform, depth=n_classes)\n            parser_tupler = None\n        else:\n            parser_tupler = partial(DataProcessor.parse_tfrecord, patch_size=patch_size, features=features, labels=labels, depth=n_classes)\n\n        if parser_tupler is not None:\n            dataset = dataset.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        else:\n            dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n            dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n\n        dataset = dataset.shuffle(512)\n        # dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        if kwargs.get(\"training\", False) and kwargs.get(\"TRANSFORM_DATA\", True):\n            print(\"randomly transforming data\")\n            if kwargs.get(\"USE_AI_PLATFORM\", False):\n                dataset = dataset.map(RandomTransform(), num_parallel_calls=tf.data.AUTOTUNE)\n            else:\n                dataset = dataset.map(DataProcessor.random_transform, num_parallel_calls=tf.data.AUTOTUNE)\n\n        dataset = dataset.batch(batch_size)\n        # dataset = dataset.cache()\n        return dataset\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.calculate_n_samples","title":"<code>calculate_n_samples(**config)</code>  <code>staticmethod</code>","text":"<p>Calculate the number of samples in the training, testing, and validation datasets.</p> <p>**config: The configuration settings.</p> <ul> <li> <p>int: The number of training samples.</p> </li> <li> <p>int: The number of testing samples.</p> </li> <li> <p>int: The number of validation samples.</p> </li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef calculate_n_samples(**config):\n    \"\"\"\n    Calculate the number of samples in the training, testing, and validation datasets.\n\n    Parameters:\n\n    **config: The configuration settings.\n\n    Returns:\n\n    * int: The number of training samples.\n\n    * int: The number of testing samples.\n\n    * int: The number of validation samples.\n\n    \"\"\"\n    parser_tupler = partial(DataProcessor.parse_tfrecord,\n                     patch_size=config.get(\"PATCH_SHAPE_SINGLE\"),\n                     features=config.get(\"FEATURES\"),\n                     labels=config.get(\"LABELS\"),\n                     depth=config.get(\"OUT_CLASS_NUM\"))\n\n    tf_training_records = tf.data.Dataset.list_files(f\"{str(config.get('TRAINING_DIR'))}/*\")\\\n                                         .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_training_records = tf_training_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n\n\n    if config.get(\"PRINT_DATASET\", False):\n        DataProcessor.print_dataset_info(tf_training_records, \"Training\")\n\n    n_training_records = DataProcessor.get_sum_tensor(tf_training_records)\n\n    tf_testing_records = tf.data.Dataset.list_files(f\"{str(config.get('TESTING_DIR'))}/*\")\\\n                                        .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_testing_records = tf_testing_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    n_testing_records = DataProcessor.get_sum_tensor(tf_testing_records)\n\n    tf_validation_records = tf.data.Dataset.list_files(f\"{str(config.get('VALIDATION_DIR'))}/*\")\\\n                                           .interleave(DataProcessor.create_tfrecord_from_file, num_parallel_calls=tf.data.AUTOTUNE)\n    tf_validation_records = tf_validation_records.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    n_validation_records = DataProcessor.get_sum_tensor(tf_validation_records)\n\n    return n_training_records, n_testing_records, n_validation_records\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.create_tfrecord_from_file","title":"<code>create_tfrecord_from_file(filename)</code>  <code>staticmethod</code>","text":"<p>Create a TensorFlow Dataset from a TFRecord file.</p> <ul> <li>filename (str): The filename of the TFRecord file.</li> </ul> <ul> <li>tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.autograph.experimental.do_not_convert\ndef create_tfrecord_from_file(filename: str) -&gt; tf.data.TFRecordDataset:\n    \"\"\"\n    Create a TensorFlow Dataset from a TFRecord file.\n\n    Parameters:\n\n    * filename (str): The filename of the TFRecord file.\n\n    Returns:\n\n    * tf.data.TFRecordDataset: The TensorFlow Dataset created from the TFRecord file.\n    \"\"\"\n    return tf.data.TFRecordDataset(filename, compression_type=\"GZIP\")\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.get_dataset","title":"<code>get_dataset(pattern, features, labels, patch_size, batch_size, n_classes=1, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Get a TFRecord dataset.</p> <ul> <li> <p>filenames (list): The list of file names.</p> </li> <li> <p>patch_size (int): The size of the patch.</p> </li> <li> <p>(list, optional): The list of feature names to include. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names to include. Default is None.</p> </li> <li> <p>batch_size (int, optional): The batch size. Default is 1.</p> </li> <li> <p>shuffle(bool, optional): Whether to shuffle the dataset. Default is False.</p> </li> <li> <p>n_labels (int, optional): The number of labels. Default is 1.</p> </li> <li> <p>num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.</p> </li> <li> <p>drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.</p> </li> <li> <p>cache (bool, optional): Whether to cache the dataset. Default is False.</p> </li> </ul> <ul> <li>tf.data.Dataset: The TFRecord dataset.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef get_dataset(pattern: str, features: list, labels: list, patch_size: int, batch_size: int, n_classes: int = 1, **kwargs) -&gt; tf.data.Dataset:\n    \"\"\"\n    Get a TFRecord dataset.\n\n    Parameters:\n    * filenames (list): The list of file names.\n\n    * patch_size (int): The size of the patch.\n\n    * (list, optional): The list of feature names to include. Default is None.\n\n    * labels (list, optional): The list of label names to include. Default is None.\n\n    * batch_size (int, optional): The batch size. Default is 1.\n\n    * shuffle(bool, optional): Whether to shuffle the dataset. Default is False.\n\n    * n_labels (int, optional): The number of labels. Default is 1.\n\n    * num_parallel_calls (int, optional): The number of parallel calls. Default is tf.data.experimental.AUTOTUNE.\n\n    * drop_remainder (bool, optional): Whether to drop the remainder of batches. Default is False.\n\n    * cache (bool, optional): Whether to cache the dataset. Default is False.\n\n    Returns:\n    * tf.data.Dataset: The TFRecord dataset.\n    \"\"\"\n    print(f\"Loading dataset from {pattern}\")\n\n    dataset = tf.data.Dataset.list_files(pattern).interleave(DataProcessor.create_tfrecord_from_file)\n\n    if kwargs.get(\"IS_DNN\", False):\n        if kwargs.get(\"USE_AI_PLATFORM\", False):\n            parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n            tupler = partial(DataProcessor.to_tuple_dnn_ai_platform, depth=n_classes)\n        else:\n            parser = partial(DataProcessor.parse_tfrecord_dnn, features=features, labels=labels)\n            tupler = partial(DataProcessor.to_tuple_dnn, depth=n_classes)\n\n        dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.shuffle(512)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        return dataset\n\n    if kwargs.get(\"USE_AI_PLATFORM\", False):\n        parser = partial(DataProcessor.parse_tfrecord_multi_label, patch_size=patch_size, features=features, labels=labels)\n        tupler = partial(DataProcessor.to_tuple_multi_label_ai_platform, depth=n_classes)\n        parser_tupler = None\n    else:\n        parser_tupler = partial(DataProcessor.parse_tfrecord, patch_size=patch_size, features=features, labels=labels, depth=n_classes)\n\n    if parser_tupler is not None:\n        dataset = dataset.map(parser_tupler, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.map(tupler, num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset = dataset.shuffle(512)\n    # dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    if kwargs.get(\"training\", False) and kwargs.get(\"TRANSFORM_DATA\", True):\n        print(\"randomly transforming data\")\n        if kwargs.get(\"USE_AI_PLATFORM\", False):\n            dataset = dataset.map(RandomTransform(), num_parallel_calls=tf.data.AUTOTUNE)\n        else:\n            dataset = dataset.map(DataProcessor.random_transform, num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset = dataset.batch(batch_size)\n    # dataset = dataset.cache()\n    return dataset\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.get_sum_tensor","title":"<code>get_sum_tensor(records)</code>  <code>staticmethod</code>","text":"<p>Gets the total number of tensor record by mapping through them.</p> <ul> <li>records: The input tensor records.</li> </ul> <ul> <li>tf.Tensor: The total number of tensor records.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.autograph.experimental.do_not_convert\ndef get_sum_tensor(records):\n    \"\"\"\n    Gets the total number of tensor record by mapping through them.\n\n    Parameters:\n\n    * records: The input tensor records.\n\n    Returns:\n\n    * tf.Tensor: The total number of tensor records.\n    \"\"\"\n    dataset = records.map(lambda x, y: tf.constant(1, dtype=tf.int64), num_parallel_calls=tf.data.AUTOTUNE)\n    # ignores any error encountered while reading the records\n    # works with v2.x\n    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n    n_tensors = dataset.reduce(np.int64(0), lambda x, y: x + y).numpy()\n    return n_tensors\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord","title":"<code>parse_tfrecord(example_proto, patch_size, features=None, labels=None, depth=1)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example.</p> <ul> <li> <p>example_proto (tf.Tensor): The example to parse.</p> </li> <li> <p>patch_size (int): The size of the patch.</p> </li> <li> <p>features (list, optional): The list of feature names to include. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names to include. Default is None.</p> </li> </ul> <ul> <li>tf.data.Dataset: The parsed dataset.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None, depth: int = 1) -&gt; tf.data.Dataset:\n    \"\"\"\n    Parse a TFRecord example.\n\n    Parameters:\n    * example_proto (tf.Tensor): The example to parse.\n\n    * patch_size (int): The size of the patch.\n\n    * features (list, optional): The list of feature names to include. Default is None.\n\n    * labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n\n    * tf.data.Dataset: The parsed dataset.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    inputs = tf.io.parse_single_example(example_proto, proto_struct)\n    inputs_list = [inputs.get(key) for key in keys]\n    stacked = tf.stack(inputs_list, axis=0)\n    stacked = tf.transpose(stacked, [1, 2, 0])\n    label = stacked[:, :, len(features):]\n    y = tf.one_hot(tf.cast(label[:, :, -1], tf.uint8), depth)\n    return stacked[:, :, :len(features)], y\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_dnn","title":"<code>parse_tfrecord_dnn(example_proto, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example for DNN models.</p> <ul> <li> <p>example_proto (tf.Tensor): The example to parse.</p> </li> <li> <p>features (list, optional): The list of feature names to include. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names to include. Default is None.</p> </li> </ul> <ul> <li>tuple: A tuple containing the parsed features and labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_dnn(example_proto: tf.Tensor, features: list = None, labels: list = None) -&gt; tuple:\n    \"\"\"\n    Parse a TFRecord example for DNN models.\n\n    Parameters:\n\n    * example_proto (tf.Tensor): The example to parse.\n\n    * features (list, optional): The list of feature names to include. Default is None.\n\n    * labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n\n    * tuple: A tuple containing the parsed features and labels.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n    label = parsed_features.pop(labels[0])\n    label = tf.cast(label, tf.int32)\n    return parsed_features, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_multi_label","title":"<code>parse_tfrecord_multi_label(example_proto, patch_size, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example with multiple labels.</p> <ul> <li> <p>example_proto (tf.data.Dataset): The example to parse.</p> </li> <li> <p>patch_size (int): The size of the patch.</p> </li> <li> <p>features (list, optional): The list of feature names to include. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names to include. Default is None.</p> </li> </ul> <ul> <li>tuple: A tuple containing the parsed features and labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_multi_label(example_proto: tf.data.Dataset, patch_size: int, features: list = None, labels: list = None) -&gt; tuple:\n    \"\"\"\n    Parse a TFRecord example with multiple labels.\n\n    Parameters:\n\n    * example_proto (tf.data.Dataset): The example to parse.\n\n    * patch_size (int): The size of the patch.\n\n    * features (list, optional): The list of feature names to include. Default is None.\n\n    * labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n\n    * tuple: A tuple containing the parsed features and labels.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    parsed_features = tf.io.parse_single_example(example_proto, proto_struct)\n    label = parsed_features.pop(labels[0])\n    return parsed_features, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.parse_tfrecord_with_name","title":"<code>parse_tfrecord_with_name(example_proto, patch_size, features=None, labels=None)</code>  <code>staticmethod</code>","text":"<p>Parse a TFRecord example with named features.</p> <ul> <li> <p>example_proto (tf.Tensor): The example to parse.</p> </li> <li> <p>patch_size (int): The size of the patch.</p> </li> <li> <p>features (list, optional): The list of feature names to include. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names to include. Default is None.</p> </li> </ul> <ul> <li>tf.data.Dataset: The parsed dataset.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef parse_tfrecord_with_name(example_proto: tf.Tensor, patch_size: int, features: list = None, labels: list = None) -&gt; tf.data.Dataset:\n    \"\"\"\n    Parse a TFRecord example with named features.\n\n    Parameters:\n\n    * example_proto (tf.Tensor): The example to parse.\n\n    * patch_size (int): The size of the patch.\n\n    * features (list, optional): The list of feature names to include. Default is None.\n\n    * labels (list, optional): The list of label names to include. Default is None.\n\n    Returns:\n\n    * tf.data.Dataset: The parsed dataset.\n    \"\"\"\n    keys = features + labels\n    columns = [\n        tf.io.FixedLenFeature(shape=[patch_size, patch_size], dtype=tf.float32) for _ in keys\n    ]\n    proto_struct = dict(zip(keys, columns))\n    return tf.io.parse_single_example(example_proto, proto_struct)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.print_dataset_info","title":"<code>print_dataset_info(dataset, dataset_name)</code>  <code>staticmethod</code>","text":"<p>Print information about a dataset.</p> <ul> <li> <p>dataset (tf.data.Dataset): The dataset to print information about.</p> </li> <li> <p>dataset_name (str): The name of the dataset.</p> </li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef print_dataset_info(dataset: tf.data.Dataset, dataset_name: str) -&gt; None:\n    \"\"\"\n    Print information about a dataset.\n\n    Parameters:\n\n    * dataset (tf.data.Dataset): The dataset to print information about.\n\n    * dataset_name (str): The name of the dataset.\n    \"\"\"\n    print(dataset_name)\n    for inputs, outputs in dataset.take(1):\n        try:\n            print(f\"inputs: {inputs.dtype.name} {inputs.shape}\")\n            print(inputs)\n            print(f\"outputs: {outputs.dtype.name} {outputs.shape}\")\n            print(outputs)\n        except:\n            print(f\" &gt; inputs:\")\n            for name, values in inputs.items():\n                print(f\"    {name}: {values.dtype.name} {values.shape}\")\n            # print(f\"    example \\n: {dataset.take(1)}\")\n            print(f\" &gt; outputs: {outputs.dtype.name} {outputs.shape}\")\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.random_transform","title":"<code>random_transform(dataset, label)</code>  <code>staticmethod</code>","text":"<p>Apply random transformations to a dataset.</p> <ul> <li>dataset (tf.Tensor): The input dataset.</li> </ul> <ul> <li>tf.Tensor: The transformed dataset.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef random_transform(dataset: tf.Tensor, label: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Apply random transformations to a dataset.\n\n    Parameters:\n\n    * dataset (tf.Tensor): The input dataset.\n\n    Returns:\n\n    * tf.Tensor: The transformed dataset.\n    \"\"\"\n    x = tf.random.uniform(())\n    if x &lt; 0.10:\n        dataset = tf.image.flip_left_right(dataset)\n        label = tf.image.flip_left_right(label)\n    elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n        dataset = tf.image.flip_up_down(dataset)\n        label = tf.image.flip_up_down(label)\n    elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n        dataset = tf.image.flip_left_right(tf.image.flip_up_down(dataset))\n        label = tf.image.flip_left_right(tf.image.flip_up_down(label))\n    elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n        dataset = tf.image.rot90(dataset, k=1)\n        label = tf.image.rot90(label, k=1)\n    elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n        dataset = tf.image.rot90(dataset, k=2)\n        label = tf.image.rot90(label, k=2)\n    elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n        dataset = tf.image.rot90(dataset, k=3)\n        label = tf.image.rot90(label, k=3)\n    elif tf.math.logical_and(x &gt;= 0.60, x &lt; 0.70):\n        dataset = tf.image.flip_left_right(tf.image.rot90(dataset, k=2))\n        label = tf.image.flip_left_right(tf.image.rot90(label, k=2))\n    else:\n        dataset = dataset\n        label = label\n\n    return dataset, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple","title":"<code>to_tuple(dataset, n_features=None, inverse_labels=False)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset to a tuple of features and labels.</p> <ul> <li> <p>dataset (tf.Tensor): The input dataset.</p> </li> <li> <p>n_features (int, optional): The number of features. Default is None.</p> </li> <li> <p>inverse_labels (bool, optional): Whether to inverse the labels. Default is False.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple(dataset: tf.Tensor, n_features: int = None, inverse_labels: bool = False) -&gt; tuple:\n    \"\"\"\n    Convert a dataset to a tuple of features and labels.\n\n    Parameters:\n\n    * dataset (tf.Tensor): The input dataset.\n\n    * n_features (int, optional): The number of features. Default is None.\n\n    * inverse_labels (bool, optional): Whether to inverse the labels. Default is False.\n\n    Returns:\n\n    * tuple: A tuple containing the features and labels.\n    \"\"\"\n    features = dataset[:, :, :, :n_features]\n    labels = dataset[:, :, :, n_features:]\n    if inverse_labels:\n        labels_inverse = tf.math.abs(labels - 1)\n        labels = tf.concat([labels_inverse, labels], axis=-1)\n    return features, labels\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_dnn","title":"<code>to_tuple_dnn(dataset, label, depth=1)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.</p> <ul> <li> <p>dataset (dict): The input dataset.</p> </li> <li> <p>label (tf.Tensor): The label.</p> </li> <li> <p>depth (int, optional): The depth of one-hot encoding. Default is 1.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and one-hot encoded labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_dnn(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n    \"\"\"\n    Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n    Parameters:\n\n    * dataset (dict): The input dataset.\n\n    * label (tf.Tensor): The label.\n\n    * depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n    Returns:\n\n    * tuple: A tuple containing the features and one-hot encoded labels.\n    \"\"\"\n    return tf.transpose(list(dataset.values())), tf.one_hot(indices=label, depth=depth)\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_dnn_ai_platform","title":"<code>to_tuple_dnn_ai_platform(dataset, label, depth=1)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.</p> <ul> <li> <p>dataset (dict): The input dataset.</p> </li> <li> <p>label (tf.Tensor): The label.</p> </li> <li> <p>depth (int, optional): The depth of one-hot encoding. Default is 1.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and one-hot encoded labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\ndef to_tuple_dnn_ai_platform(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n    \"\"\"\n    Convert a dataset for DNN models to a tuple of features and one-hot encoded labels.\n\n    Parameters:\n\n    * dataset (dict): The input dataset.\n\n    * label (tf.Tensor): The label.\n\n    * depth (int, optional): The depth of one-hot encoding. Default is 1.\n\n    Returns:\n\n    * tuple: A tuple containing the features and one-hot encoded labels.\n    \"\"\"\n      # (1) -&gt; (1, 1, 1)\n    return ({k: [[v]] for k, v in dataset.items()}, tf.expand_dims(tf.one_hot(label, depth), axis=0))\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_multi_label","title":"<code>to_tuple_multi_label(dataset, label, depth=1, x_only=False)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.</p> <ul> <li> <p>dataset (tuple): The input dataset.</p> </li> <li> <p>n_labels (int, optional): The number of labels. Default is 1.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and multi-hot encoded labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_multi_label(dataset: dict, label: tf.Tensor, depth: int = 1, x_only: bool = False) -&gt; tuple:\n    \"\"\"\n    Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n    Parameters:\n\n    * dataset (tuple): The input dataset.\n\n    * n_labels (int, optional): The number of labels. Default is 1.\n\n    Returns:\n\n    * tuple: A tuple containing the features and multi-hot encoded labels.\n    \"\"\"\n    label = tf.cast(label, tf.uint8)\n    label = tf.one_hot(indices=label, depth=depth)\n    parsed_dataset = {k: tf.expand_dims(v, axis=2) for k, v in dataset.items()}\n    if x_only:\n        return parsed_dataset\n    return parsed_dataset, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_multi_label_ai_platform","title":"<code>to_tuple_multi_label_ai_platform(dataset, label, depth=1)</code>  <code>staticmethod</code>","text":"<p>Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.</p> <ul> <li> <p>dataset (tuple): The input dataset.</p> </li> <li> <p>n_labels (int, optional): The number of labels. Default is 1.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and multi-hot encoded labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_multi_label_ai_platform(dataset: dict, label: tf.Tensor, depth: int = 1) -&gt; tuple:\n    \"\"\"\n    Convert a dataset with multiple labels to a tuple of features and multi-hot encoded labels.\n\n    Parameters:\n\n    * dataset (tuple): The input dataset.\n\n    * n_labels (int, optional): The number of labels. Default is 1.\n\n    Returns:\n\n    * tuple: A tuple containing the features and multi-hot encoded labels.\n    \"\"\"\n    label = tf.cast(label, tf.uint8)\n    label = tf.one_hot(indices=label, depth=depth)\n    parsed_dataset = {k: tf.expand_dims(v, axis=2) for k, v in dataset.items()}\n    return parsed_dataset, label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.DataProcessor.to_tuple_with_name","title":"<code>to_tuple_with_name(inputs, features=None, labels=None, n_classes=1)</code>  <code>staticmethod</code>","text":"<p>Convert inputs with named features to a tuple of features and one-hot encoded labels.</p> <ul> <li> <p>inputs (tf.Tensor): The input dataset.</p> </li> <li> <p>features (list, optional): The list of feature names. Default is None.</p> </li> <li> <p>labels (list, optional): The list of label names. Default is None.</p> </li> <li> <p>n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.</p> </li> </ul> <ul> <li>tuple: A tuple containing the features and one-hot encoded labels.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@staticmethod\n@tf.function\ndef to_tuple_with_name(inputs: tf.Tensor, features: list = None, labels: list = None, n_classes: int = 1) -&gt; tuple:\n    \"\"\"\n    Convert inputs with named features to a tuple of features and one-hot encoded labels.\n\n    Parameters:\n\n    * inputs (tf.Tensor): The input dataset.\n\n    * features (list, optional): The list of feature names. Default is None.\n\n    * labels (list, optional): The list of label names. Default is None.\n\n    * n_classes (int, optional): The number of classes for one-hot encoding. Default is 1.\n\n    Returns:\n\n    * tuple: A tuple containing the features and one-hot encoded labels.\n    \"\"\"\n    return (\n        {name: inputs[name] for name in features},\n        tf.one_hot(tf.cast(inputs[labels[0]], tf.uint8), n_classes)\n    )\n</code></pre>"},{"location":"data_processor/#aces.data_processor.RandomTransform","title":"<code> RandomTransform            (Layer)         </code>","text":"Source code in <code>aces/data_processor.py</code> <pre><code>class RandomTransform(tf.keras.layers.Layer):\n    def __init__(self, seed=42, unit_range=True):\n        super().__init__()\n        self.seed = seed\n        self.flip_horizontal = tf.keras.layers.RandomFlip(\"horizontal\", seed=self.seed)\n        self.flip_vertical = tf.keras.layers.RandomFlip(\"vertical\", seed=self.seed)\n        self.flip_both = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=self.seed)\n        self.random_brightness = tf.keras.layers.RandomBrightness(0.2, value_range=(0, 1) if unit_range else (0, 255), seed=self.seed)\n        self.random_contrast = tf.keras.layers.RandomContrast(0.2, seed=self.seed)\n\n    @tf.function\n    def call(self, dataset, label):\n        \"\"\"\n        Apply random transformations to a dataset.\n\n        Parameters:\n\n        * dataset (tf.Tensor): The input dataset.\n\n        * label (tf.Tensor): The corresponding label.\n\n        Returns:\n\n        * tuple: The transformed dataset and label as a tuple.\n        \"\"\"\n        x = tf.random.uniform((), seed=self.seed)\n        transformed_features = {}\n\n        # Apply the same random transformation across all bands or features\n        for key, feature in dataset.items():\n            transformed_feature = feature  # Default to no change\n            if x &lt; 0.10:\n                transformed_feature = self.flip_horizontal(feature)\n            elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n                transformed_feature = self.flip_vertical(feature)\n            elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n                transformed_feature = self.flip_both(feature)\n            elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n                transformed_feature = tf.image.rot90(feature, k=1)\n            elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n                transformed_feature = tf.image.rot90(feature, k=2)\n            elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n                transformed_feature = tf.image.rot90(feature, k=3)\n            elif tf.math.logical_and(x &gt;= 0.60, x &lt; 0.70):\n                transformed_feature = self.random_brightness(feature)\n            elif tf.math.logical_and(x &gt;= 0.70, x &lt; 0.80):\n                transformed_feature = self.random_contrast(feature)\n\n            transformed_features[key] = transformed_feature\n\n        # Apply corresponding transformations to the label\n        transformed_label = label  # Default to no change\n        if x &lt; 0.10:\n            transformed_label = self.flip_horizontal(label)\n        elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n            transformed_label = self.flip_vertical(label)\n        elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n            transformed_label = self.flip_both(label)\n        elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n            transformed_label = tf.image.rot90(label, k=1)\n        elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n            transformed_label = tf.image.rot90(label, k=2)\n        elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n            transformed_label = tf.image.rot90(label, k=3)\n\n        return transformed_features, transformed_label\n</code></pre>"},{"location":"data_processor/#aces.data_processor.RandomTransform.call","title":"<code>call(self, dataset, label)</code>","text":"<p>Apply random transformations to a dataset.</p> <ul> <li> <p>dataset (tf.Tensor): The input dataset.</p> </li> <li> <p>label (tf.Tensor): The corresponding label.</p> </li> </ul> <ul> <li>tuple: The transformed dataset and label as a tuple.</li> </ul> Source code in <code>aces/data_processor.py</code> <pre><code>@tf.function\ndef call(self, dataset, label):\n    \"\"\"\n    Apply random transformations to a dataset.\n\n    Parameters:\n\n    * dataset (tf.Tensor): The input dataset.\n\n    * label (tf.Tensor): The corresponding label.\n\n    Returns:\n\n    * tuple: The transformed dataset and label as a tuple.\n    \"\"\"\n    x = tf.random.uniform((), seed=self.seed)\n    transformed_features = {}\n\n    # Apply the same random transformation across all bands or features\n    for key, feature in dataset.items():\n        transformed_feature = feature  # Default to no change\n        if x &lt; 0.10:\n            transformed_feature = self.flip_horizontal(feature)\n        elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n            transformed_feature = self.flip_vertical(feature)\n        elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n            transformed_feature = self.flip_both(feature)\n        elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n            transformed_feature = tf.image.rot90(feature, k=1)\n        elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n            transformed_feature = tf.image.rot90(feature, k=2)\n        elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n            transformed_feature = tf.image.rot90(feature, k=3)\n        elif tf.math.logical_and(x &gt;= 0.60, x &lt; 0.70):\n            transformed_feature = self.random_brightness(feature)\n        elif tf.math.logical_and(x &gt;= 0.70, x &lt; 0.80):\n            transformed_feature = self.random_contrast(feature)\n\n        transformed_features[key] = transformed_feature\n\n    # Apply corresponding transformations to the label\n    transformed_label = label  # Default to no change\n    if x &lt; 0.10:\n        transformed_label = self.flip_horizontal(label)\n    elif tf.math.logical_and(x &gt;= 0.10, x &lt; 0.20):\n        transformed_label = self.flip_vertical(label)\n    elif tf.math.logical_and(x &gt;= 0.20, x &lt; 0.30):\n        transformed_label = self.flip_both(label)\n    elif tf.math.logical_and(x &gt;= 0.30, x &lt; 0.40):\n        transformed_label = tf.image.rot90(label, k=1)\n    elif tf.math.logical_and(x &gt;= 0.40, x &lt; 0.50):\n        transformed_label = tf.image.rot90(label, k=2)\n    elif tf.math.logical_and(x &gt;= 0.50, x &lt; 0.60):\n        transformed_label = tf.image.rot90(label, k=3)\n\n    return transformed_features, transformed_label\n</code></pre>"},{"location":"ee_utils/","title":"ee_utils module","text":""},{"location":"ee_utils/#aces.ee_utils.EEUtils","title":"<code> EEUtils        </code>","text":"<p>This class provides utility functions to handle Earth Engine API information and make authenticated requests.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>class EEUtils:\n    \"\"\"\n    EEUtils: Earth Engine Utility Class\n\n    This class provides utility functions to handle Earth Engine API information and make authenticated requests.\n    \"\"\"\n    @staticmethod\n    def get_credentials_by_service_account_key(key):\n        \"\"\"\n        Helper function to retrieve credentials using a service account key.\n\n        Parameters:\n        key (str): The path to the service account key JSON file.\n\n        Returns:\n        ee.ServiceAccountCredentials: The authenticated credentials.\n        \"\"\"\n        import json\n        service_account = json.load(open(key))\n        credentials = ee.ServiceAccountCredentials(service_account[\"client_email\"], key)\n        return credentials\n\n    @staticmethod\n    def initialize_session(use_highvolume : bool = False, key : Union[str, None] = None, project: str = None):\n        \"\"\"\n        Initialize the Earth Engine session.\n        If use_highvolume is True, the high-volume Earth Engine API will be used.\n        If a project is provided, the session will be initialized with the project ID. Recommended to use project.\n        If a key is provided, the service account key will be used.\n\n        Parameters:\n        use_highvolume (bool): Whether to use the high-volume Earth Engine API.\n        key (str or None): The path to the service account key JSON file. If None, the default credentials will be used.\n        project (str): The Google Cloud project ID to use for the session.\n        \"\"\"\n        if key is None:\n            if use_highvolume and project:\n                ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\", project=project)\n            elif use_highvolume:\n                ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\")\n            elif project:\n                ee.Initialize(project=project)\n            else:\n                ee.Initialize()\n        else:\n            credentials = EEUtils.get_credentials_by_service_account_key(key)\n            if use_highvolume and project:\n                ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\", project=project)\n            elif use_highvolume:\n                ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n            elif project:\n                ee.Initialize(credentials, project=project)\n            else:\n                ee.Initialize(credentials)\n\n    @staticmethod\n    def calculate_avg_min_max_statistics(image: ee.Image, geometry: ee.FeatureCollection, scale: int = 30) -&gt; ee.Dictionary:\n        \"\"\"\n        Calculate min and max of an image over a specific region.\n\n        Parameters:\n        image (ee.Image): The image to calculate statistics on.\n        geometry (ee.FeatureCollection): The region to calculate statistics over.\n        scale (int, optional): The scale, in meters, of the projection to compute statistics in. Default is 30.\n\n        Returns:\n        ee.Dictionary: A dictionary containing the min and max of the image.\n        \"\"\"\n        reducers = ee.Reducer.mean() \\\n            .combine(reducer2=ee.Reducer.min(), sharedInputs=True) \\\n            .combine(reducer2=ee.Reducer.max(), sharedInputs=True)\n\n        stats = image.reduceRegion(\n            reducer=reducers,\n            geometry=geometry,\n            scale=scale,\n            maxPixels=1E13\n        )\n\n        return stats\n\n    @staticmethod\n    def export_collection_data(collection: ee.FeatureCollection, export_type: Union[list, str]=\"cloud\", start_training=True, **params) -&gt; None:\n        if isinstance(export_type, str):\n            export_type = [export_type]\n\n        for _type in export_type:\n            if _type == \"cloud\":\n                EEUtils._export_collection_to_cloud_storage(collection, start_training, **params)\n\n            if _type == \"asset\":\n                EEUtils._export_collection_to_asset(collection, start_training, **params)\n\n            if _type == \"drive\":\n                EEUtils._export_collection_to_drive(collection, start_training, **params)\n\n            if _type not in [\"cloud\", \"asset\", \"drive\"]:\n                raise NotImplementedError(f\"Currently supported export types are: {', '.join(_type)}\")\n\n    @staticmethod\n    def _export_collection_to_drive(collection, start_training, **kwargs) -&gt; None:\n        print(\"Exporting training data to Google Drive..\")\n        training_task = ee.batch.Export.table.toDrive(\n            collection=collection,\n            description=kwargs.get(\"description\", \"myExportTableTask\"),\n            folder=kwargs.get(\"folder\", \"myFolder\"),\n            fileNamePrefix=kwargs.get(\"file_prefix\", \"myExportTableTask\"),\n            fileFormat=kwargs.get(\"file_format\", \"CSV\"),\n            selectors=kwargs.get(\"selectors\", collection.first().propertyNames().getInfo()),\n        )\n        if start_training: training_task.start()\n\n    @staticmethod\n    def _export_collection_to_asset(collection, start_training, **kwargs) -&gt; None:\n        asset_id = kwargs.get(\"asset_id\", \"myAssetId\")\n        print(f\"Exporting training data to {asset_id}..\")\n        training_task = ee.batch.Export.table.toAsset(\n            collection=collection,\n            description=kwargs.get(\"description\", \"myExportTableTask\"),\n            assetId=asset_id,\n            selectors=kwargs.get(\"selectors\", collection.first().propertyNames().getInfo()),\n        )\n        if start_training: training_task.start()\n\n    @staticmethod\n    def _export_collection_to_cloud_storage(collection, start_training, **kwargs) -&gt; None:\n        description = kwargs.get(\"description\", \"myExportTableTask\")\n        bucket = kwargs.get(\"bucket\", \"myBucket\")\n        file_prefix = kwargs.get(\"file_prefix\") if kwargs.get(\"file_prefix\") is not None else description\n        print(f\"Exporting training data to gs://{bucket}/{file_prefix}..\")\n        training_task = ee.batch.Export.table.toCloudStorage(\n            collection=collection,\n            description=description,\n            fileNamePrefix=file_prefix,\n            bucket=bucket,\n            fileFormat=kwargs.get(\"file_format\", \"TFRecord\"),\n            selectors=kwargs.get(\"selectors\", collection.first().propertyNames().getInfo()),\n        )\n        if start_training: training_task.start()\n\n    @staticmethod\n    def beam_export_collection_to_cloud_storage(collection_index, start_training, **kwargs) -&gt; None:\n        from aces.ee_utils import EEUtils\n        import ee\n        EEUtils.initialize_session(use_highvolume=True)\n\n        collection = ee.FeatureCollection(collection_index[0])\n        index = collection_index[1]\n\n        description = kwargs.get(\"description\", \"myExportTableTask\")\n        bucket = kwargs.get(\"bucket\", \"myBucket\")\n        file_prefix = kwargs.get(\"file_prefix\") if kwargs.get(\"file_prefix\") is not None else description\n        file_prefix = f\"{file_prefix}_{index}\"\n        print(f\"Exporting training data to gs://{bucket}/{file_prefix}..\")\n        training_task = ee.batch.Export.table.toCloudStorage(\n            collection=collection,\n            description=f\"{description}__index_{index}\",\n            fileNamePrefix=file_prefix,\n            bucket=bucket,\n            fileFormat=kwargs.get(\"file_format\", \"TFRecord\"),\n            selectors=kwargs.get(\"selectors\", collection.first().propertyNames().getInfo()),\n        )\n        if start_training: training_task.start()\n\n    @staticmethod\n    def export_image(image: ee.Image, export_type: str=\"asset\", start_training=True, **params) -&gt; None:\n        if isinstance(export_type, str):\n            export_type = [export_type]\n\n        for _type in export_type:\n            if _type == \"cloud\":\n                EEUtils._export_image_to_cloud_storage(image, start_training, **params)\n\n            if _type == \"asset\":\n                EEUtils._export_image_to_asset(image, start_training, **params)\n\n            if _type not in [\"cloud\", \"asset\"]:\n                raise NotImplementedError(f\"Currently supported export types are: {', '.join(_type)}\")\n\n    @staticmethod\n    def _export_image_to_cloud_storage(image, start_training, **kwargs) -&gt; None:\n        description = kwargs.get(\"description\", \"myExportImageTask\")\n        bucket = kwargs.get(\"bucket\", \"myBucket\")\n        file_name_prefix = kwargs.get(\"file_name_prefix\") if kwargs.get(\"file_name_prefix\") is not None else description\n        region = kwargs.get(\"region\", None)\n        if region is not None:\n            if isinstance(region, ee.FeatureCollection):\n                region = region.geometry()\n            elif isinstance(region, ee.Geometry):\n                region = region\n            else:\n                raise ValueError(f\"region must be an ee.FeatureCollection or ee.Geometry object. Found {type(region)}\")\n\n        print(f\"Exporting training data to gs://{bucket}/{file_name_prefix}..\")\n\n        params = {\n            \"image\": image,\n            \"description\": description,\n            \"fileNamePrefix\": file_name_prefix,\n            \"bucket\": kwargs.get(\"bucket\", \"myBucket\"),\n            \"fileFormat\": kwargs.get(\"file_format\", \"GeoTIFF\"),\n            \"formatOptions\": kwargs.get(\"format_options\", None),\n            \"region\": region,\n            \"scale\": kwargs.get(\"scale\", 1000),\n            \"maxPixels\": kwargs.get(\"max_pixels\", 1e10),\n        }\n        keys = Utils.convert_camel_to_snake(list(params.keys()))\n        keys.remove(\"image\")\n\n        for key in list(kwargs.keys()):\n            if key not in keys:\n                warnings.warn(f\"Parameter {key} not found in kwargs. Double check your parameter name (camelCase vs snake_case)\")\n\n        not_none_params = {k:v for k, v in params.items() if v is not None}\n        training_task = ee.batch.Export.image.toCloudStorage(**not_none_params)\n        if start_training: training_task.start()\n\n    @staticmethod\n    def _export_image_to_asset(image, start_training, **kwargs) -&gt; None:\n        asset_id = kwargs.get(\"asset_id\", \"\")\n        print(f\"Exporting image to {asset_id}..\")\n\n        training_task = ee.batch.Export.image.toAsset(\n            image=image,\n            description=kwargs.get(\"description\", \"myExportImageTask\"),\n            assetId=asset_id,\n            region=kwargs.get(\"region\", None),\n            scale=kwargs.get(\"scale\", 30),\n            maxPixels=kwargs.get(\"max_pixels\", 1E13),\n        )\n        if start_training: training_task.start()\n\n    @staticmethod\n    def country_bbox(country_name, max_error=100):\n        \"\"\"Function to get a bounding box geometry of a country\n\n        args:\n            country_name (str): US-recognized country name\n            max_error (float,optional): The maximum amount of error tolerated when\n                performing any necessary reprojection. default = 100\n\n        returns:\n            ee.Geometry: geometry of country bounding box\n        \"\"\"\n\n        all_countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n        return all_countries.filter(ee.Filter.eq(\"country_na\", country_name))\\\n                            .geometry(max_error).bounds(max_error)\n\n    @staticmethod\n    def get_image_collection_statistics(image_collection: ee.ImageCollection) -&gt; ee.Image:\n        reducers = ee.Reducer.mean() \\\n            .combine(reducer2=ee.Reducer.min(), sharedInputs=True) \\\n                .combine(reducer2=ee.Reducer.max(), sharedInputs=True) \\\n                    .combine(reducer2=ee.Reducer.stdDev(), sharedInputs=True) \\\n                        .combine(reducer2=ee.Reducer.percentile([25, 50, 75], [\"Q1\", \"Q2\", \"Q3\"]), sharedInputs=True)\n        reducer = image_collection.reduce(reducer=reducers)\n        return reducer.float()\n\n    @staticmethod\n    def calculate_planet_indices(image: ee.Image) -&gt; ee.Image:\n        ndvi = image.normalizedDifference([\"N\", \"R\"]).rename(\"NDVI\")\n        evi = image.expression (\n            \"2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\", {\n                \"NIR\": image.select(\"N\"),\n                \"RED\": image.select(\"R\"),\n                \"BLUE\": image.select(\"B\")\n            }).rename(\"EVI\")\n        ndwi = image.normalizedDifference([\"G\", \"N\"]).rename(\"NDWI\")\n        savi = image.expression(\"((NIR - RED) / (NIR + RED + 0.5))*(1.5)\", {\n            \"NIR\": image.select(\"N\"),\n            \"RED\": image.select(\"R\")\n        }).rename(\"SAVI\")\n        msavi2 = image.expression(\"(( (2*NIR + 1) - sqrt( ((2*NIR + 1) * (2*NIR + 1)) - 8 * (NIR - R) ) )) / 2\", {\n            \"NIR\": image.select(\"N\"),\n            \"R\": image.select(\"R\")\n        }).rename(\"MSAVI2\")\n\n        mtvi2 = image.expression(\"( 1.5*(1.2*(NIR - GREEN) - 2.5*(RED - GREEN)) ) / ( sqrt( ((2*NIR + 1) * (2*NIR + 1)) - (6*NIR - 5*sqrt(RED)) - 0.5 ) )\", {\n            \"NIR\": image.select(\"N\"),\n            \"RED\": image.select(\"R\"),\n            \"GREEN\": image.select(\"G\"),\n        }).rename(\"MTVI2\")\n\n        vari = image.expression(\"(GREEN - RED) / (GREEN + RED - BLUE)\", {\n            \"GREEN\": image.select(\"G\"),\n            \"RED\": image.select(\"R\"),\n            \"BLUE\": image.select(\"B\"),\n        }).rename(\"VARI\")\n\n        tgi = image.expression(\"( (120*(RED - BLUE)) - (190*(RED - GREEN)) ) / 2\", {\n            \"GREEN\": image.select(\"G\"),\n            \"RED\": image.select(\"R\"),\n            \"BLUE\": image.select(\"B\"),\n        }).rename(\"TGI\")\n\n        return ndvi.addBands([evi, ndwi, savi, msavi2, mtvi2, vari, tgi]).float()\n\n    @staticmethod\n    def calculate_evi(image: ee.Image) -&gt; ee.Image:\n        evi = image.expression (\n            \"2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))\", {\n                \"NIR\": image.select(\"nir\"),\n                \"RED\": image.select(\"red\"),\n                \"BLUE\": image.select(\"blue\")\n            }).rename(\"EVI\")\n        return evi.float()\n\n    @staticmethod\n    def calculate_s1_indices(image: ee.Image) -&gt; ee.Image:\n        VV = image.select(\"VV\").rename(\"vv\")\n        VH = image.select(\"VH\").rename(\"vh\")\n        ratio = VV.divide(VH).rename(\"s1_ratio\")\n        ndratio = VV.subtract(VH).divide(VV.add(VH)).rename(\"s1_ndratio\")\n        return VV.addBands([VH, ratio, ndratio]).float()\n\n    @staticmethod\n    def generate_stratified_samples(image: ee.Image, region: ee.Geometry, numPoints: int = 500, classBand: str = None, scale: int=30, **kwargs) -&gt; ee.FeatureCollection:\n        # Add a latitude and longitude band.\n        return image.addBands(ee.Image.pixelLonLat()).stratifiedSample(\n            numPoints=numPoints,\n            classBand=classBand if classBand else \"label\",\n            scale=scale,\n            region=region,\n            seed=kwargs.get(\"seed\", Config.SEED),\n            classValues=kwargs.get(\"class_values\", None),\n            classPoints=kwargs.get(\"class_points\", None),# [2000, 600, 600, 600]\n        ).map(lambda f: f.setGeometry(ee.Geometry.Point([f.get(\"longitude\"), f.get(\"latitude\")])))\n\n    @staticmethod\n    def sample_image_by_collection(image: ee.Image, collection: ee.FeatureCollection, **kwargs: dict) -&gt; ee.FeatureCollection:\n        samples = image.sampleRegions(\n            collection=collection,\n            properties=kwargs.get(\"properties\", collection.first().propertyNames().getInfo()),\n            scale=kwargs.get(\"scale\", None),\n            geometries=kwargs.get(\"geometries\", False),\n            tileScale=kwargs.get(\"tile_scale\", 1),\n        )\n        return samples\n\n    @staticmethod\n    def sample_image(image: ee.Image, region: ee.FeatureCollection, **kwargs: dict) -&gt; ee.FeatureCollection:\n        sample = image.sample(region=region,\n                              scale=kwargs.get(\"SCALE\") or kwargs.get(\"scale\"),\n                              seed=kwargs.get(\"SEED\") or kwargs.get(\"seed\"),\n                              geometries=kwargs.get(\"geometries\", False))\n        return sample\n\n    @staticmethod\n    def beam_yield_sample_points_with_index(index, sample_locations: ee.List, use_service_account: bool = False) -&gt; List:\n        from aces.ee_utils import EEUtils\n        from aces.config import Config\n        import ee\n        EEUtils.initialize_session(use_highvolume=True, key=Config.EE_SERVICE_CREDENTIALS if use_service_account else None)\n        print(f\"Yielding Index: {index} of {sample_locations.size().getInfo() - 1}\")\n        point = ee.Feature(sample_locations.get(index)).geometry().getInfo()\n        return point[\"coordinates\"], index\n\n    @staticmethod\n    def beam_yield_sample_points(index, sample_locations: ee.List, use_service_account: bool = False) -&gt; List:\n        from aces.ee_utils import EEUtils\n        from aces.config import Config\n        import ee\n        EEUtils.initialize_session(use_highvolume=True, key=Config.EE_SERVICE_CREDENTIALS if use_service_account else None)\n        print(f\"Yielding Index: {index} of {sample_locations.size().getInfo() - 1}\")\n        point = ee.Feature(sample_locations.get(index)).geometry().getInfo()\n        return point[\"coordinates\"]\n\n    @staticmethod\n    def beam_sample_neighbourhood(coords_index, image, config: Union[Config, str] = \"config.env\", use_service_account: bool = False):\n        from aces.ee_utils import EEUtils\n        from aces.config import Config\n        import ee\n\n        if isinstance(config, str):\n            config = Config(config)\n        elif isinstance(config, Config):\n            config = config\n        else:\n            raise ValueError(\"config must be of type Config or str\")\n\n        EEUtils.initialize_session(use_highvolume=True, key=config.EE_SERVICE_CREDENTIALS if use_service_account else None)\n\n        coords = coords_index[0]\n        index = coords_index[1]\n\n        def get_kernel(kernel_size) -&gt; ee.Kernel:\n            eelist = ee.List.repeat(1, kernel_size)\n            lists = ee.List.repeat(eelist, kernel_size)\n            kernel = ee.Kernel.fixed(kernel_size, kernel_size, lists)\n            return kernel\n\n        def create_neighborhood(kernel) -&gt; ee.Image:\n            return image.neighborhoodToArray(kernel)\n\n        def sample_data(image, points) -&gt; ee.FeatureCollection:\n            return image.sample(\n                region=points,\n                scale=config.SCALE,\n                tileScale=16,\n                geometries=False\n            )\n\n        image_kernel = get_kernel(config.PATCH_SHAPE_SINGLE)\n        neighborhood = create_neighborhood(image_kernel)\n        training_data = sample_data(neighborhood, ee.Geometry.Point(coords))\n        return training_data, index\n\n\n    @staticmethod\n    def beam_get_training_patches(coords: List[float], image: ee.Image, bands: List[str] = [],\n                                  scale: int = 5, patch_size: int = 128, use_service_account: bool = False) -&gt; np.ndarray:\n        \"\"\"Get a training patch centered on the coordinates.\"\"\"\n        from aces.ee_utils import EEUtils\n        from aces.config import Config\n        import ee\n        EEUtils.initialize_session(use_highvolume=True, key=Config.EE_SERVICE_CREDENTIALS if use_service_account else None)\n        from google.api_core import exceptions, retry\n        import requests\n        import numpy as np\n        from typing import List\n        import io\n\n        # @retry.Retry(timeout=10*60) # seconds\n        @retry.Retry(deadline=10*60) # seconds\n        def get_patch(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int) -&gt; np.ndarray:\n            \"\"\"Get the patch of pixels in the geometry as a Numpy array.\"\"\"\n            # Create the URL to download the band values of the patch of pixels.\n            url = image.getDownloadURL({\n                \"region\": region,\n                \"dimensions\": [patch_size, patch_size],\n                \"format\": \"NPY\",\n                \"bands\": bands,\n            })\n            # Download the pixel data. If we get \"429: Too Many Requests\" errors,\n            # it\"s safe to retry the request.\n            response = requests.get(url)\n            if response.status_code == 429:\n                # The retry.Retry library only works with `google.api_core` exceptions.\n                raise exceptions.TooManyRequests(response.text)\n\n            if response.status_code == 503:\n                raise exceptions.ServiceUnavailable(response.text)\n\n                # Still raise any other exceptions to make sure we got valid data.\n            response.raise_for_status()\n            # Load the NumPy file data and return it as a NumPy array.\n            return np.load(io.BytesIO(response.content), allow_pickle=True)\n\n        # @retry.Retry(timeout=10*60) # seconds\n        @retry.Retry(deadline=10*60) # seconds\n        def compute_pixel(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int, scale_x: float, scale_y: float) -&gt; np.ndarray:\n            \"\"\"Get the patch of pixels in the geometry as a Numpy array.\"\"\"\n\n            # Make a request object.\n            request = {\n                \"expression\": image,\n                \"fileFormat\": \"NPY\",\n                \"bandIds\": bands,\n                \"grid\": {\n                    \"dimensions\": {\n                        \"width\": patch_size,\n                        \"height\": patch_size\n                    },\n                    \"affineTransform\": {\n                        \"scaleX\": scale_x,\n                        \"shearX\": 0,\n                        \"translateX\": coords[0],\n                        \"shearY\": 0,\n                        \"scaleY\": scale_y,\n                        \"translateY\": coords[1]\n                    },\n                    \"crsCode\": \"EPSG:4326\",\n                },\n            }\n            response = ee.data.computePixels(request)\n            # Load the NumPy file data and return it as a NumPy array.\n            return np.load(io.BytesIO(response.content), allow_pickle=True)\n\n        point = ee.Geometry.Point(coords)\n        region = point.buffer(scale * patch_size / 2, 1).bounds(1)\n        return get_patch(image, region, bands, patch_size)\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.beam_get_training_patches","title":"<code>beam_get_training_patches(coords, image, bands=[], scale=5, patch_size=128, use_service_account=False)</code>  <code>staticmethod</code>","text":"<p>Get a training patch centered on the coordinates.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef beam_get_training_patches(coords: List[float], image: ee.Image, bands: List[str] = [],\n                              scale: int = 5, patch_size: int = 128, use_service_account: bool = False) -&gt; np.ndarray:\n    \"\"\"Get a training patch centered on the coordinates.\"\"\"\n    from aces.ee_utils import EEUtils\n    from aces.config import Config\n    import ee\n    EEUtils.initialize_session(use_highvolume=True, key=Config.EE_SERVICE_CREDENTIALS if use_service_account else None)\n    from google.api_core import exceptions, retry\n    import requests\n    import numpy as np\n    from typing import List\n    import io\n\n    # @retry.Retry(timeout=10*60) # seconds\n    @retry.Retry(deadline=10*60) # seconds\n    def get_patch(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int) -&gt; np.ndarray:\n        \"\"\"Get the patch of pixels in the geometry as a Numpy array.\"\"\"\n        # Create the URL to download the band values of the patch of pixels.\n        url = image.getDownloadURL({\n            \"region\": region,\n            \"dimensions\": [patch_size, patch_size],\n            \"format\": \"NPY\",\n            \"bands\": bands,\n        })\n        # Download the pixel data. If we get \"429: Too Many Requests\" errors,\n        # it\"s safe to retry the request.\n        response = requests.get(url)\n        if response.status_code == 429:\n            # The retry.Retry library only works with `google.api_core` exceptions.\n            raise exceptions.TooManyRequests(response.text)\n\n        if response.status_code == 503:\n            raise exceptions.ServiceUnavailable(response.text)\n\n            # Still raise any other exceptions to make sure we got valid data.\n        response.raise_for_status()\n        # Load the NumPy file data and return it as a NumPy array.\n        return np.load(io.BytesIO(response.content), allow_pickle=True)\n\n    # @retry.Retry(timeout=10*60) # seconds\n    @retry.Retry(deadline=10*60) # seconds\n    def compute_pixel(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int, scale_x: float, scale_y: float) -&gt; np.ndarray:\n        \"\"\"Get the patch of pixels in the geometry as a Numpy array.\"\"\"\n\n        # Make a request object.\n        request = {\n            \"expression\": image,\n            \"fileFormat\": \"NPY\",\n            \"bandIds\": bands,\n            \"grid\": {\n                \"dimensions\": {\n                    \"width\": patch_size,\n                    \"height\": patch_size\n                },\n                \"affineTransform\": {\n                    \"scaleX\": scale_x,\n                    \"shearX\": 0,\n                    \"translateX\": coords[0],\n                    \"shearY\": 0,\n                    \"scaleY\": scale_y,\n                    \"translateY\": coords[1]\n                },\n                \"crsCode\": \"EPSG:4326\",\n            },\n        }\n        response = ee.data.computePixels(request)\n        # Load the NumPy file data and return it as a NumPy array.\n        return np.load(io.BytesIO(response.content), allow_pickle=True)\n\n    point = ee.Geometry.Point(coords)\n    region = point.buffer(scale * patch_size / 2, 1).bounds(1)\n    return get_patch(image, region, bands, patch_size)\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.calculate_avg_min_max_statistics","title":"<code>calculate_avg_min_max_statistics(image, geometry, scale=30)</code>  <code>staticmethod</code>","text":"<p>Calculate min and max of an image over a specific region.</p> <p>image (ee.Image): The image to calculate statistics on. geometry (ee.FeatureCollection): The region to calculate statistics over. scale (int, optional): The scale, in meters, of the projection to compute statistics in. Default is 30.</p> <p>ee.Dictionary: A dictionary containing the min and max of the image.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef calculate_avg_min_max_statistics(image: ee.Image, geometry: ee.FeatureCollection, scale: int = 30) -&gt; ee.Dictionary:\n    \"\"\"\n    Calculate min and max of an image over a specific region.\n\n    Parameters:\n    image (ee.Image): The image to calculate statistics on.\n    geometry (ee.FeatureCollection): The region to calculate statistics over.\n    scale (int, optional): The scale, in meters, of the projection to compute statistics in. Default is 30.\n\n    Returns:\n    ee.Dictionary: A dictionary containing the min and max of the image.\n    \"\"\"\n    reducers = ee.Reducer.mean() \\\n        .combine(reducer2=ee.Reducer.min(), sharedInputs=True) \\\n        .combine(reducer2=ee.Reducer.max(), sharedInputs=True)\n\n    stats = image.reduceRegion(\n        reducer=reducers,\n        geometry=geometry,\n        scale=scale,\n        maxPixels=1E13\n    )\n\n    return stats\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.country_bbox","title":"<code>country_bbox(country_name, max_error=100)</code>  <code>staticmethod</code>","text":"<p>Function to get a bounding box geometry of a country</p> <p>Parameters:</p> Name Type Description Default <code>country_name</code> <code>str</code> <p>US-recognized country name</p> required <code>max_error</code> <code>float,optional</code> <p>The maximum amount of error tolerated when performing any necessary reprojection. default = 100</p> <code>100</code> <p>Returns:</p> Type Description <code>ee.Geometry</code> <p>geometry of country bounding box</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef country_bbox(country_name, max_error=100):\n    \"\"\"Function to get a bounding box geometry of a country\n\n    args:\n        country_name (str): US-recognized country name\n        max_error (float,optional): The maximum amount of error tolerated when\n            performing any necessary reprojection. default = 100\n\n    returns:\n        ee.Geometry: geometry of country bounding box\n    \"\"\"\n\n    all_countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n    return all_countries.filter(ee.Filter.eq(\"country_na\", country_name))\\\n                        .geometry(max_error).bounds(max_error)\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.get_credentials_by_service_account_key","title":"<code>get_credentials_by_service_account_key(key)</code>  <code>staticmethod</code>","text":"<p>Helper function to retrieve credentials using a service account key.</p> <p>key (str): The path to the service account key JSON file.</p> <p>ee.ServiceAccountCredentials: The authenticated credentials.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef get_credentials_by_service_account_key(key):\n    \"\"\"\n    Helper function to retrieve credentials using a service account key.\n\n    Parameters:\n    key (str): The path to the service account key JSON file.\n\n    Returns:\n    ee.ServiceAccountCredentials: The authenticated credentials.\n    \"\"\"\n    import json\n    service_account = json.load(open(key))\n    credentials = ee.ServiceAccountCredentials(service_account[\"client_email\"], key)\n    return credentials\n</code></pre>"},{"location":"ee_utils/#aces.ee_utils.EEUtils.initialize_session","title":"<code>initialize_session(use_highvolume=False, key=None, project=None)</code>  <code>staticmethod</code>","text":"<p>Initialize the Earth Engine session. If use_highvolume is True, the high-volume Earth Engine API will be used. If a project is provided, the session will be initialized with the project ID. Recommended to use project. If a key is provided, the service account key will be used.</p> <p>use_highvolume (bool): Whether to use the high-volume Earth Engine API. key (str or None): The path to the service account key JSON file. If None, the default credentials will be used. project (str): The Google Cloud project ID to use for the session.</p> Source code in <code>aces/ee_utils.py</code> <pre><code>@staticmethod\ndef initialize_session(use_highvolume : bool = False, key : Union[str, None] = None, project: str = None):\n    \"\"\"\n    Initialize the Earth Engine session.\n    If use_highvolume is True, the high-volume Earth Engine API will be used.\n    If a project is provided, the session will be initialized with the project ID. Recommended to use project.\n    If a key is provided, the service account key will be used.\n\n    Parameters:\n    use_highvolume (bool): Whether to use the high-volume Earth Engine API.\n    key (str or None): The path to the service account key JSON file. If None, the default credentials will be used.\n    project (str): The Google Cloud project ID to use for the session.\n    \"\"\"\n    if key is None:\n        if use_highvolume and project:\n            ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\", project=project)\n        elif use_highvolume:\n            ee.Initialize(opt_url=\"https://earthengine-highvolume.googleapis.com\")\n        elif project:\n            ee.Initialize(project=project)\n        else:\n            ee.Initialize()\n    else:\n        credentials = EEUtils.get_credentials_by_service_account_key(key)\n        if use_highvolume and project:\n            ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\", project=project)\n        elif use_highvolume:\n            ee.Initialize(credentials, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n        elif project:\n            ee.Initialize(credentials, project=project)\n        else:\n            ee.Initialize(credentials)\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#earth-engine-account","title":"Earth Engine Account","text":"<p>Google Earth Engine (GEE) is used for data processing to get it ready in a usable TF Format. You can sign up for a Google Earth Engine account.</p> <p></p>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>servir-aces is available on PyPI. To install servir-aces, run this command in your terminal:</p> <pre><code>pip install servir-aces\n</code></pre> <p>The optional dependencies can be installed using one of the following:</p> <ul> <li><code>pip install \"servir-aces[extra]\"</code>: installing all optional dependencies.</li> </ul>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/SERVIR/servir-aces\n</code></pre>"},{"location":"metrics/","title":"metrics module","text":"<p>metrics.py: Custom Metrics for Model Evaluation and Utility Functions for Model Visualization</p> <p>This module provides a collection of custom metrics that can be used for evaluating model performance in tasks such as image segmentation. This module includes a wide variety of evaluation metrics listed below. Additionally, it contains utility functions for plotting and visualizing model metrics during training.</p>"},{"location":"metrics/#aces.metrics.Metrics","title":"<code> Metrics        </code>","text":"<p>A class containing various metrics functions for model evaluation.</p> <p>This class provides a collection of static methods, each representing a different evaluation metric. These metrics can be used to evaluate the performance of classification or segmentation models during training or testing.</p> <p>Methods</p> <p>precision():     Computes the precision metric. recall():     Computes the recall metric. f1_m():     Computes the F1 score metric. [van1979information, chicco2020advantages] one_hot_io_u(num_classes):     Computes the Intersection over Union (IoU) metric for each class. [jaccard1901etude] true_positives():     Computes the true positives metric also known as sensitivity.[yerushalmy1947statistical] false_positives():     Computes the false positives metric. [cohen2013statistical] true_negatives():     Computes the true negatives metric also known as specificity. [yerushalmy1947statistical] false_negatives():     Computes the false negatives metric. [cohen2013statistical] binary_accuracy():     Computes the binary accuracy metric. auc():     Computes the Area Under the Curve (AUC) metric. [provost2001robust] prc():     Computes the Precision-Recall Curve (PRC) metric. dice_coef():     Computes the Dice coefficient metric. [milletari2016v] dice_loss():     Computes the Dice loss metric. [sudre2017generalised] bce_loss():     Computes the Binary Cross-Entropy (BCE) loss metric. [zhu2018negative; yi2004automated] bce_dice_loss():     Computes the BCE and Dice loss metric. cite [taghanaki2019combo] tversky():     Computes the Tversky index metric.[tversky1977features] tversky_loss():     Computes the Tversky loss metric. [salehi2017tversky] focal_tversky_loss():     Computes the focal Tversky loss metric [abraham2019novel]</p> Source code in <code>aces/metrics.py</code> <pre><code>class Metrics:\n    \"\"\"\n    A class containing various metrics functions for model evaluation.\n\n    This class provides a collection of static methods, each representing a different evaluation metric. These metrics can be used to\n    evaluate the performance of classification or segmentation models during training or testing.\n\n    Methods:\n        precision():\n            Computes the precision metric.\n        recall():\n            Computes the recall metric.\n        f1_m():\n            Computes the F1 score metric. [van1979information, chicco2020advantages]\n        one_hot_io_u(num_classes):\n            Computes the Intersection over Union (IoU) metric for each class. [jaccard1901etude]\n        true_positives():\n            Computes the true positives metric also known as sensitivity.[yerushalmy1947statistical]\n        false_positives():\n            Computes the false positives metric. [cohen2013statistical]\n        true_negatives():\n            Computes the true negatives metric also known as specificity. [yerushalmy1947statistical]\n        false_negatives():\n            Computes the false negatives metric. [cohen2013statistical]\n        binary_accuracy():\n            Computes the binary accuracy metric.\n        auc():\n            Computes the Area Under the Curve (AUC) metric. [provost2001robust]\n        prc():\n            Computes the Precision-Recall Curve (PRC) metric.\n        dice_coef():\n            Computes the Dice coefficient metric. [milletari2016v]\n        dice_loss():\n            Computes the Dice loss metric. [sudre2017generalised]\n        bce_loss():\n            Computes the Binary Cross-Entropy (BCE) loss metric. [zhu2018negative; yi2004automated]\n        bce_dice_loss():\n            Computes the BCE and Dice loss metric. cite [taghanaki2019combo]\n        tversky():\n            Computes the Tversky index metric.[tversky1977features]\n        tversky_loss():\n            Computes the Tversky loss metric. [salehi2017tversky]\n        focal_tversky_loss():\n            Computes the focal Tversky loss metric [abraham2019novel]\n    \"\"\"\n\n    @staticmethod\n    def recall_m(y_true, y_pred):\n        \"\"\"\n        Calculate the recall metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Recall metric value.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    @staticmethod\n    def precision_m(y_true, y_pred):\n        \"\"\"\n        Calculate the precision metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Precision metric value.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    @staticmethod\n    def f1_m(y_true, y_pred):\n        \"\"\"\n        Calculate the F1-score metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            F1-score metric value.\n        \"\"\"\n        precision = Metrics.precision_m(y_true, y_pred)\n        recall = Metrics.recall_m(y_true, y_pred)\n        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n    @staticmethod\n    def dice_coef(y_true, y_pred, smooth=1):\n        \"\"\"\n        Calculate the Dice coefficient metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n\n        Returns:\n            Dice coefficient metric value.\n        \"\"\"\n        y_true_f = K.flatten(y_true)\n        y_pred_f = K.flatten(y_pred)\n        intersection = K.sum(y_true_f * y_pred_f)\n        return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n    @staticmethod\n    def dice_loss(y_true, y_pred, smooth=1):\n        \"\"\"\n        Calculate the Dice loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n\n        Returns:\n            Dice loss metric value.\n        \"\"\"\n        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n        true_sum = K.sum(K.square(y_true), -1)\n        pred_sum = K.sum(K.square(y_pred), -1)\n        return 1 - ((2. * intersection + smooth) / (true_sum + pred_sum + smooth))\n\n    @staticmethod\n    def bce_dice_loss(y_true, y_pred):\n        \"\"\"\n        Calculate the BCE-Dice loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            BCE-Dice loss metric value.\n        \"\"\"\n        return Metrics.bce_loss(y_true, y_pred) + Metrics.dice_loss(y_true, y_pred)\n\n    @staticmethod\n    def bce_loss(y_true, y_pred):\n        \"\"\"\n        Calculate the Binary Cross-Entropy (BCE) loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            BCE loss metric value.\n        \"\"\"\n        return keras.losses.binary_crossentropy(y_true, y_pred, label_smoothing=0.2)\n\n    @staticmethod\n    def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n        \"\"\"\n        Calculate the Tversky index metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            smooth: Smoothing parameter.\n            alpha: Weighting factor.\n\n        Returns:\n            Tversky index metric value.\n        \"\"\"\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n        return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n\n    @staticmethod\n    def tversky_loss(y_true, y_pred):\n        \"\"\"\n        Calculate the Tversky loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n\n        Returns:\n            Tversky loss metric value.\n        \"\"\"\n        return 1 - Metrics.tversky(y_true, y_pred)\n\n    @staticmethod\n    def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n        \"\"\"\n        Calculate the focal Tversky loss metric.\n\n        Args:\n            y_true: Ground truth labels.\n            y_pred: Predicted labels.\n            gamma: Focusing parameter.\n\n        Returns:\n            Focal Tversky loss metric value.\n        \"\"\"\n        return K.pow((1 - Metrics.tversky(y_true, y_pred)), gamma)\n\n    @staticmethod\n    def true_positives():\n        \"\"\"\n        Create a metric for counting true positives.\n\n        Returns:\n            True positives metric.\n        \"\"\"\n        return keras.metrics.TruePositives(name=\"tp\")\n\n    @staticmethod\n    def false_positives():\n        \"\"\"\n        Create a metric for counting false positives.\n\n        Returns:\n            False positives metric.\n        \"\"\"\n        return keras.metrics.FalsePositives(name=\"fp\")\n\n    @staticmethod\n    def true_negatives():\n        \"\"\"\n        Create a metric for counting true negatives.\n\n        Returns:\n            True negatives metric.\n        \"\"\"\n        return keras.metrics.TrueNegatives(name=\"tn\")\n\n    @staticmethod\n    def false_negatives():\n        \"\"\"\n        Create a metric for counting false negatives.\n\n        Returns:\n            False negatives metric.\n        \"\"\"\n        return keras.metrics.FalseNegatives(name=\"fn\")\n\n    @staticmethod\n    def binary_accuracy():\n        \"\"\"\n        Create a metric for calculating binary accuracy.\n\n        Returns:\n            Binary accuracy metric.\n        \"\"\"\n        return keras.metrics.BinaryAccuracy(name=\"accuracy\")\n\n    # check difference between this and precision_m output\n    @staticmethod\n    def precision():\n        \"\"\"\n        Create a metric for calculating precision.\n\n        Returns:\n            Precision metric.\n        \"\"\"\n        return keras.metrics.Precision(name=\"precision\")\n\n    @staticmethod\n    def recall():\n        \"\"\"\n        Create a metric for calculating recall.\n\n        Returns:\n            Recall metric.\n        \"\"\"\n        return keras.metrics.Recall(name=\"recall\")\n\n    @staticmethod\n    def auc():\n        \"\"\"\n        Create a metric for calculating Area Under the Curve (AUC).\n\n        Returns:\n            AUC metric.\n        \"\"\"\n        return keras.metrics.AUC(name=\"auc\")\n\n    @staticmethod\n    def prc():\n        \"\"\"\n        Create a metric for calculating Precision-Recall Curve (PRC).\n\n        Returns:\n            PRC metric.\n        \"\"\"\n        return keras.metrics.AUC(name=\"prc\", curve=\"PR\")\n\n    @staticmethod\n    def one_hot_io_u(num_classes, name=\"one_hot_io_u\"):\n        \"\"\"\n        Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.\n\n        Args:\n            num_classes: Number of classes.\n            name: Name of the metric.\n\n        Returns:\n            One-hot IoU metric.\n        \"\"\"\n        return keras.metrics.OneHotIoU(num_classes=num_classes, target_class_ids=list(range(num_classes)), name=name)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.auc","title":"<code>auc()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Area Under the Curve (AUC).</p> <p>Returns:</p> Type Description <p>AUC metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef auc():\n    \"\"\"\n    Create a metric for calculating Area Under the Curve (AUC).\n\n    Returns:\n        AUC metric.\n    \"\"\"\n    return keras.metrics.AUC(name=\"auc\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.bce_dice_loss","title":"<code>bce_dice_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the BCE-Dice loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>BCE-Dice loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef bce_dice_loss(y_true, y_pred):\n    \"\"\"\n    Calculate the BCE-Dice loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        BCE-Dice loss metric value.\n    \"\"\"\n    return Metrics.bce_loss(y_true, y_pred) + Metrics.dice_loss(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.bce_loss","title":"<code>bce_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the Binary Cross-Entropy (BCE) loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>BCE loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef bce_loss(y_true, y_pred):\n    \"\"\"\n    Calculate the Binary Cross-Entropy (BCE) loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        BCE loss metric value.\n    \"\"\"\n    return keras.losses.binary_crossentropy(y_true, y_pred, label_smoothing=0.2)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.binary_accuracy","title":"<code>binary_accuracy()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating binary accuracy.</p> <p>Returns:</p> Type Description <p>Binary accuracy metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef binary_accuracy():\n    \"\"\"\n    Create a metric for calculating binary accuracy.\n\n    Returns:\n        Binary accuracy metric.\n    \"\"\"\n    return keras.metrics.BinaryAccuracy(name=\"accuracy\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.dice_coef","title":"<code>dice_coef(y_true, y_pred, smooth=1)</code>  <code>staticmethod</code>","text":"<p>Calculate the Dice coefficient metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <p>Returns:</p> Type Description <p>Dice coefficient metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef dice_coef(y_true, y_pred, smooth=1):\n    \"\"\"\n    Calculate the Dice coefficient metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n\n    Returns:\n        Dice coefficient metric value.\n    \"\"\"\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.dice_loss","title":"<code>dice_loss(y_true, y_pred, smooth=1)</code>  <code>staticmethod</code>","text":"<p>Calculate the Dice loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <p>Returns:</p> Type Description <p>Dice loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef dice_loss(y_true, y_pred, smooth=1):\n    \"\"\"\n    Calculate the Dice loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n\n    Returns:\n        Dice loss metric value.\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    true_sum = K.sum(K.square(y_true), -1)\n    pred_sum = K.sum(K.square(y_pred), -1)\n    return 1 - ((2. * intersection + smooth) / (true_sum + pred_sum + smooth))\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.f1_m","title":"<code>f1_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the F1-score metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>F1-score metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef f1_m(y_true, y_pred):\n    \"\"\"\n    Calculate the F1-score metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        F1-score metric value.\n    \"\"\"\n    precision = Metrics.precision_m(y_true, y_pred)\n    recall = Metrics.recall_m(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.false_negatives","title":"<code>false_negatives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting false negatives.</p> <p>Returns:</p> Type Description <p>False negatives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef false_negatives():\n    \"\"\"\n    Create a metric for counting false negatives.\n\n    Returns:\n        False negatives metric.\n    \"\"\"\n    return keras.metrics.FalseNegatives(name=\"fn\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.false_positives","title":"<code>false_positives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting false positives.</p> <p>Returns:</p> Type Description <p>False positives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef false_positives():\n    \"\"\"\n    Create a metric for counting false positives.\n\n    Returns:\n        False positives metric.\n    \"\"\"\n    return keras.metrics.FalsePositives(name=\"fp\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.focal_tversky_loss","title":"<code>focal_tversky_loss(y_true, y_pred, gamma=0.75)</code>  <code>staticmethod</code>","text":"<p>Calculate the focal Tversky loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>gamma</code> <p>Focusing parameter.</p> <code>0.75</code> <p>Returns:</p> Type Description <p>Focal Tversky loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    \"\"\"\n    Calculate the focal Tversky loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        gamma: Focusing parameter.\n\n    Returns:\n        Focal Tversky loss metric value.\n    \"\"\"\n    return K.pow((1 - Metrics.tversky(y_true, y_pred)), gamma)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.one_hot_io_u","title":"<code>one_hot_io_u(num_classes, name='one_hot_io_u')</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <p>Number of classes.</p> required <code>name</code> <p>Name of the metric.</p> <code>'one_hot_io_u'</code> <p>Returns:</p> Type Description <p>One-hot IoU metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef one_hot_io_u(num_classes, name=\"one_hot_io_u\"):\n    \"\"\"\n    Create a metric for calculating Intersection over Union (IoU) using one-hot encoding.\n\n    Args:\n        num_classes: Number of classes.\n        name: Name of the metric.\n\n    Returns:\n        One-hot IoU metric.\n    \"\"\"\n    return keras.metrics.OneHotIoU(num_classes=num_classes, target_class_ids=list(range(num_classes)), name=name)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.prc","title":"<code>prc()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating Precision-Recall Curve (PRC).</p> <p>Returns:</p> Type Description <p>PRC metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef prc():\n    \"\"\"\n    Create a metric for calculating Precision-Recall Curve (PRC).\n\n    Returns:\n        PRC metric.\n    \"\"\"\n    return keras.metrics.AUC(name=\"prc\", curve=\"PR\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.precision","title":"<code>precision()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating precision.</p> <p>Returns:</p> Type Description <p>Precision metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef precision():\n    \"\"\"\n    Create a metric for calculating precision.\n\n    Returns:\n        Precision metric.\n    \"\"\"\n    return keras.metrics.Precision(name=\"precision\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.precision_m","title":"<code>precision_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the precision metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Precision metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef precision_m(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Precision metric value.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.recall","title":"<code>recall()</code>  <code>staticmethod</code>","text":"<p>Create a metric for calculating recall.</p> <p>Returns:</p> Type Description <p>Recall metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef recall():\n    \"\"\"\n    Create a metric for calculating recall.\n\n    Returns:\n        Recall metric.\n    \"\"\"\n    return keras.metrics.Recall(name=\"recall\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.recall_m","title":"<code>recall_m(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the recall metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Recall metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef recall_m(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Recall metric value.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.true_negatives","title":"<code>true_negatives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting true negatives.</p> <p>Returns:</p> Type Description <p>True negatives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef true_negatives():\n    \"\"\"\n    Create a metric for counting true negatives.\n\n    Returns:\n        True negatives metric.\n    \"\"\"\n    return keras.metrics.TrueNegatives(name=\"tn\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.true_positives","title":"<code>true_positives()</code>  <code>staticmethod</code>","text":"<p>Create a metric for counting true positives.</p> <p>Returns:</p> Type Description <p>True positives metric.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef true_positives():\n    \"\"\"\n    Create a metric for counting true positives.\n\n    Returns:\n        True positives metric.\n    \"\"\"\n    return keras.metrics.TruePositives(name=\"tp\")\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.tversky","title":"<code>tversky(y_true, y_pred, smooth=1, alpha=0.7)</code>  <code>staticmethod</code>","text":"<p>Calculate the Tversky index metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <code>smooth</code> <p>Smoothing parameter.</p> <code>1</code> <code>alpha</code> <p>Weighting factor.</p> <code>0.7</code> <p>Returns:</p> Type Description <p>Tversky index metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef tversky(y_true, y_pred, smooth=1, alpha=0.7):\n    \"\"\"\n    Calculate the Tversky index metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n        smooth: Smoothing parameter.\n        alpha: Weighting factor.\n\n    Returns:\n        Tversky index metric value.\n    \"\"\"\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n</code></pre>"},{"location":"metrics/#aces.metrics.Metrics.tversky_loss","title":"<code>tversky_loss(y_true, y_pred)</code>  <code>staticmethod</code>","text":"<p>Calculate the Tversky loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth labels.</p> required <code>y_pred</code> <p>Predicted labels.</p> required <p>Returns:</p> Type Description <p>Tversky loss metric value.</p> Source code in <code>aces/metrics.py</code> <pre><code>@staticmethod\ndef tversky_loss(y_true, y_pred):\n    \"\"\"\n    Calculate the Tversky loss metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        Tversky loss metric value.\n    \"\"\"\n    return 1 - Metrics.tversky(y_true, y_pred)\n</code></pre>"},{"location":"model_builder/","title":"model_builder module","text":"<p>model_builder.py:</p> <p>Model Builder Class for Creating and Compiling Neural Network Models</p> <p>This module provides a <code>ModelBuilder</code> class that is responsible for creating and compiling neural network models. It includes methods for building and compiling models of different types, such as Deep Neural Network (DNN), Convolutional Neural Network (CNN), and U-Net, based on the provided specifications. The class also contains utility methods for constructing custom layers and defining metrics.</p>"},{"location":"model_builder/#aces.model_builder.AddExtraFeatures","title":"<code> AddExtraFeatures            (Layer)         </code>","text":"Source code in <code>aces/model_builder.py</code> <pre><code>class AddExtraFeatures(tf.keras.layers.Layer):\n    def __init__(self, added_features):\n        super().__init__()\n        self.added_features = added_features\n\n    def call(self, features_dict, labels):\n        features_dict = rs.derive_features_for_dnn(features_dict, self.added_features)\n        return features_dict, labels\n</code></pre>"},{"location":"model_builder/#aces.model_builder.AddExtraFeatures.call","title":"<code>call(self, features_dict, labels)</code>","text":"<p>This is where the layer's logic lives.</p> <p>The <code>call()</code> method may not create state (except in its first invocation, wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).  It is recommended to create state, including <code>tf.Variable</code> instances and nested <code>Layer</code> instances,  in <code>__init__()</code>, or in the <code>build()</code> method that is called automatically before <code>call()</code> executes for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Input tensor, or dict/list/tuple of input tensors. The first positional <code>inputs</code> argument is subject to special rules: - <code>inputs</code> must be explicitly passed. A layer cannot have zero   arguments, and <code>inputs</code> cannot be provided via the default value   of a keyword argument. - NumPy array or Python scalar values in <code>inputs</code> get cast as   tensors. - Keras mask metadata is only collected from <code>inputs</code>. - Layers are built (<code>build(input_shape)</code> method)   using shape info from <code>inputs</code> only. - <code>input_spec</code> compatibility is only checked against <code>inputs</code>. - Mixed precision input casting is only applied to <code>inputs</code>.   If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their   casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using <code>inputs</code>   only. - Integration with various ecosystem packages like TFMOT, TFLite,   TF.js, etc is only supported for <code>inputs</code> and not for tensors in   positional and keyword arguments.</p> required <code>*args</code> <p>Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above.</p> required <code>**kwargs</code> <p>Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - <code>training</code>: Boolean scalar tensor of Python boolean indicating   whether the <code>call</code> is meant for training or inference. - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a   <code>mask</code> argument, its default value will be set to the mask   generated for <code>inputs</code> by the previous layer (if <code>input</code> did come   from a layer that generated a corresponding mask, i.e. if it came   from a Keras layer with masking support).</p> required <p>Returns:</p> Type Description <p>A tensor or list/tuple of tensors.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def call(self, features_dict, labels):\n    features_dict = rs.derive_features_for_dnn(features_dict, self.added_features)\n    return features_dict, labels\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder","title":"<code> ModelBuilder        </code>","text":"<p>ModelBuilder Class for Creating and Compiling Neural Network Models</p> <p>This class provides methods for building and compiling neural network models of different types, such as DNN, CNN, and U-Net, based on the provided specifications. It includes utility methods for constructing custom layers and defining metrics.</p> <p>Attributes:</p> Name Type Description <code>in_size</code> <code>int</code> <p>The input size of the models.</p> <code>out_classes</code> <code>int</code> <p>The number of output classes for classification models.</p> <code>optimizer</code> <code>tf.keras.optimizers.Optimizer</code> <p>The optimizer to use for model compilation.</p> <code>loss</code> <code>tf.keras.losses.Loss</code> <p>The loss function to use for model compilation.</p> <p>Methods</p> <p>build_model(model_type, kwargs):     Builds and compiles a neural network model based on the provided model type. build_and_compile_dnn_model(kwargs):     Builds and compiles a Deep Neural Network (DNN) model. build_and_compile_cnn_model(kwargs):     Builds and compiles a Convolutional Neural Network (CNN) model. build_and_compile_unet_model(kwargs):     Builds and compiles a U-Net model. _build_and_compile_unet_model(**kwargs):     Helper method for building and compiling a U-Net model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>class ModelBuilder:\n    \"\"\"\n    ModelBuilder Class for Creating and Compiling Neural Network Models\n\n    This class provides methods for building and compiling neural network models of different types, such as DNN, CNN, and U-Net,\n    based on the provided specifications. It includes utility methods for constructing custom layers and defining metrics.\n\n    Attributes:\n        in_size (int): The input size of the models.\n        out_classes (int): The number of output classes for classification models.\n        optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n        loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n\n    Methods:\n        build_model(model_type, **kwargs):\n            Builds and compiles a neural network model based on the provided model type.\n        build_and_compile_dnn_model(**kwargs):\n            Builds and compiles a Deep Neural Network (DNN) model.\n        build_and_compile_cnn_model(**kwargs):\n            Builds and compiles a Convolutional Neural Network (CNN) model.\n        build_and_compile_unet_model(**kwargs):\n            Builds and compiles a U-Net model.\n        _build_and_compile_unet_model(**kwargs):\n            Helper method for building and compiling a U-Net model.\n    \"\"\"\n\n    def __init__(self, features, out_classes, optimizer, loss):\n        \"\"\"\n        Initialize ModelBuilder with input size, output classes, optimizer, and loss.\n\n        Args:\n            in_size (int): The input size of the models.\n            out_classes (int): The number of output classes for classification models.\n            optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n            loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n        \"\"\"\n        self.features = features\n        self.in_size = len(features)\n        self.out_classes = out_classes\n        self.optimizer = optimizer\n        self.loss = loss\n\n    def build_model(self, model_type, **kwargs):\n        \"\"\"\n        Builds and compiles a neural network model based on the provided model type.\n\n        Args:\n            model_type (str): The type of the model to build (\"dnn\", \"cnn\", \"unet\").\n            **kwargs: Additional keyword arguments specific to the model type.\n\n        Returns:\n            keras.Model: The compiled neural network model.\n\n        Raises:\n            ValueError: If an invalid model type is provided.\n        \"\"\"\n        FOR_AI_PLATFORM = kwargs.get(\"FOR_AI_PLATFORM\", False)\n        if model_type == \"dnn\":\n            if FOR_AI_PLATFORM:\n                return self.build_and_compile_dnn_model_for_ai_platform(**kwargs)\n            else:\n                return self.build_and_compile_dnn_model(**kwargs)\n        elif model_type == \"cnn\":\n            return self.build_and_compile_cnn_model(**kwargs)\n        elif model_type == \"unet\":\n            if FOR_AI_PLATFORM:\n                return self.build_and_compile_unet_model_for_ai_platform(**kwargs)\n            else:\n                return self.build_and_compile_unet_model(**kwargs)\n        else:\n            raise ValueError(f\"Invalid model type: {model_type}\")\n\n    def build_and_compile_dnn_model(self, **kwargs):\n        \"\"\"\n        Builds and compiles a Deep Neural Network (DNN) model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled DNN model.\n        \"\"\"\n        INITIAL_BIAS = kwargs.get(\"INITIAL_BIAS\", None)\n        print(f\"INITIAL_BIAS: {INITIAL_BIAS}\")\n\n        if INITIAL_BIAS is not None:\n            INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n        else:\n            INITIAL_BIAS = \"zeros\"\n\n        inputs = keras.Input(shape=(None, self.in_size), name=\"input_layer\")\n\n        x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n        x = keras.layers.Dense(128, activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n        x = keras.layers.Dense(64, activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n        x = keras.layers.Dense(32, activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n        output = keras.layers.Dense(self.out_classes, activation=kwargs.get(\"ACTIVATION_FN\"), bias_initializer=INITIAL_BIAS)(x)\n\n        model = keras.models.Model(inputs=inputs, outputs=output)\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.CategoricalAccuracy(),\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n\n\n    def build_and_compile_dnn_model_for_ai_platform(self, **kwargs):\n        \"\"\"\n        Builds and compiles a Deep Neural Network (DNN) model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled DNN model.\n        \"\"\"\n        INITIAL_BIAS = kwargs.get(\"INITIAL_BIAS\", None)\n        print(f\"INITIAL_BIAS: {INITIAL_BIAS}\")\n        # DNN_DURING_ONLY = kwargs.get(\"DURING_ONLY\", False)\n\n        DERIVE_FEATURES = kwargs.get(\"DERIVE_FEATURES\", False)\n\n        if INITIAL_BIAS is not None:\n            INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n        else:\n            INITIAL_BIAS = \"zeros\"\n\n        inputs_main = keras.Input(shape=(None, None, self.in_size), name=\"input_layer\")\n\n        if DERIVE_FEATURES:\n            inputs = rs.concatenate_features_for_dnn(inputs_main)\n        else:\n            inputs = inputs_main\n\n        x = keras.layers.Conv2D(256, (1, 1), activation=\"relu\")(inputs)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(128, (1, 1), activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(64, (1, 1), activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(32, (1, 1), activation=\"relu\")(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        output = keras.layers.Conv2D(self.out_classes, (1, 1), activation=kwargs.get(\"ACTIVATION_FN\"), bias_initializer=INITIAL_BIAS)(x)\n\n        model = keras.models.Model(inputs=inputs_main, outputs=output)\n\n        wrapped_model = ModelWrapper(ModelPreprocess(self.features), model)\n\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.CategoricalAccuracy(),\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n        wrapped_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model, wrapped_model\n\n    def build_and_compile_cnn_model(self, **kwargs):\n        \"\"\"\n        Builds and compiles a Convolutional Neural Network (CNN) model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled CNN model.\n        \"\"\"\n        inputs = keras.Input(shape=(kwargs.get(\"PATCH_SHAPE\", 128)[0], kwargs.get(\"PATCH_SHAPE\", 128)[0], self.in_size))\n        x = keras.layers.Conv2D(32, 3, activation=\"relu\", name=\"convd-1\", padding=\"same\")(inputs)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(64, 3, activation=\"relu\", name=\"convd-2\", padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(128, 3, activation=\"relu\", name=\"convd-3\", padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        x = keras.layers.Conv2D(128, 3, activation=\"relu\", name=\"convd-4\", padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n        outputs = keras.layers.Conv2D(self.out_classes, (1, 1), activation=\"softmax\", name=\"final_conv\")(x)\n        model = keras.Model(inputs, outputs, name=\"cnn_model\")\n\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.CategoricalAccuracy(),\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n\n    def build_and_compile_unet_model_for_ai_platform(self, **kwargs):\n        \"\"\"\n        Builds and compiles a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        if len(kwargs.get(\"physical_devices\")) &gt; 0:\n            with tf.distribute.MirroredStrategy().scope():\n                return self._build_and_compile_unet_model_for_ai_plaform(**kwargs)\n        else:\n            print(\"No distributed strategy found.\")\n            return self._build_and_compile_unet_model(**kwargs)\n\n    def _build_and_compile_unet_model_for_ai_plaform(self, **kwargs):\n        \"\"\"\n        Helper method for building and compiling a U-Net model.\n        \"\"\"\n        model = self._build_and_compile_unet_model(**kwargs)\n        wrapped_model = ModelWrapper(ModelPreprocess(self.features), model)\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.CategoricalAccuracy(),\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n        wrapped_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model, wrapped_model\n\n    def build_and_compile_unet_model(self, **kwargs):\n        \"\"\"\n        Builds and compiles a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        if len(kwargs.get(\"physical_devices\")) &gt; 0:\n            with tf.distribute.MirroredStrategy().scope():\n                model = self._build_and_compile_unet_model(**kwargs)\n                metrics_list = [\n                    Metrics.precision(),\n                    Metrics.recall(),\n                    keras.metrics.CategoricalAccuracy(),\n                    Metrics.one_hot_io_u(self.out_classes),\n                ]\n                model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n                return model\n        else:\n            print(\"No distributed strategy found.\")\n            model = self._build_and_compile_unet_model(**kwargs)\n            metrics_list = [\n                Metrics.precision(),\n                Metrics.recall(),\n                keras.metrics.CategoricalAccuracy(),\n                Metrics.one_hot_io_u(self.out_classes),\n            ]\n            model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n            return model\n\n    def _build_and_compile_unet_model(self, **kwargs):\n        \"\"\"\n        Helper method for building and compiling a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        inputs = keras.Input(shape=(None, None, self.in_size))\n\n        DERIVE_FEATURES = kwargs.get(\"DERIVE_FEATURES\", False)\n\n        if DERIVE_FEATURES:\n            input_features = rs.concatenate_features_for_cnn(inputs)\n        else:\n            input_features = inputs\n\n\n        x = keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(input_features)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Activation(\"relu\")(x)\n\n        previous_block_activation = x\n\n        l2_regularizer = keras.regularizers.l2(0.001)\n\n        for filters in [64, 128, 256]:\n            x = keras.layers.Activation(\"relu\")(x)\n            x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.Activation(\"relu\")(x)\n            x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n            residual = keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n                previous_block_activation\n            )\n            x = keras.layers.add([x, residual])\n            previous_block_activation = x\n\n        for filters in [256, 128, 64, 32]:\n            x = keras.layers.Activation(\"relu\")(x)\n            x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.Activation(\"relu\")(x)\n            x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n            x = keras.layers.BatchNormalization()(x)\n\n            x = keras.layers.UpSampling2D(2)(x)\n\n            residual = keras.layers.UpSampling2D(2)(previous_block_activation)\n            residual = keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n            x = keras.layers.add([x, residual])\n            previous_block_activation = x\n\n        outputs = keras.layers.Conv2D(self.out_classes, 3, activation=kwargs.get(\"ACTIVATION_FN\"), padding=\"same\", name=\"final_conv\")(x)\n\n        model = keras.Model(inputs=inputs, outputs=outputs, name=\"unet\")\n\n        return model\n\n    def _build_and_compile_vanilla_unet_model(self, **kwargs):\n        \"\"\"\n        Helper method for building and compiling a U-Net model.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            keras.Model: The compiled U-Net model.\n        \"\"\"\n        inputs = keras.Input(shape=(None, None, self.in_size))\n\n        DERIVE_FEATURES = kwargs.get(\"DERIVE_FEATURES\", False)\n\n        print(f\"DERIVE_FEATURES: {DERIVE_FEATURES}\")\n\n        if DERIVE_FEATURES:\n            input_features = rs.concatenate_features_for_cnn(inputs)\n        else:\n            input_features = inputs\n\n        c1 = keras.layers.Conv2D(16, (3, 3), padding=\"same\")(input_features)\n        c1 = keras.layers.BatchNormalization()(c1)\n        c1 = keras.layers.Activation(\"relu\")(c1)\n        p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n\n        c2 = keras.layers.Conv2D(32, (3, 3), padding=\"same\")(p1)\n        c2 = keras.layers.BatchNormalization()(c2)\n        c2 = keras.layers.Activation(\"relu\")(c2)\n        p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n\n        c3 = keras.layers.Conv2D(64, (3, 3), padding=\"same\")(p2)\n        c3 = keras.layers.BatchNormalization()(c3)\n        c3 = keras.layers.Activation(\"relu\")(c3)\n        p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n\n        c4 = keras.layers.Conv2D(128, (3, 3), padding=\"same\")(p3)\n        c4 = keras.layers.BatchNormalization()(c4)\n        c4 = keras.layers.Activation(\"relu\")(c4)\n        p4 = keras.layers.MaxPooling2D((2, 2))(c4)\n\n        c5 = keras.layers.Conv2D(256, (3, 3), padding=\"same\")(p4)\n        c5 = keras.layers.BatchNormalization()(c5)\n        c5 = keras.layers.Activation(\"relu\")(c5)\n\n        # Decoder\n        u6 = keras.layers.UpSampling2D((2, 2))(c5)\n        u6 = keras.layers.Conv2D(128, (3, 3), padding=\"same\")(u6)\n        u6 = keras.layers.BatchNormalization()(u6)\n        u6 = keras.layers.Activation(\"relu\")(u6)\n        u6 = keras.layers.Add()([u6, c4])\n\n        u7 = keras.layers.UpSampling2D((2, 2))(u6)\n        u7 = keras.layers.Conv2D(64, (3, 3), padding=\"same\")(u7)\n        u7 = keras.layers.BatchNormalization()(u7)\n        u7 = keras.layers.Activation(\"relu\")(u7)\n        u7 = keras.layers.Add()([u7, c3])\n\n        u8 = keras.layers.UpSampling2D((2, 2))(u7)\n        u8 = keras.layers.Conv2D(32, (3, 3), padding=\"same\")(u8)\n        u8 = keras.layers.BatchNormalization()(u8)\n        u8 = keras.layers.Activation(\"relu\")(u8)\n        u8 = keras.layers.Add()([u8, c2])\n\n        u9 = keras.layers.UpSampling2D((2, 2))(u8)\n        u9 = keras.layers.Conv2D(16, (3, 3), padding=\"same\")(u9)\n        u9 = keras.layers.BatchNormalization()(u9)\n        u9 = keras.layers.Activation(\"relu\")(u9)\n        u9 = keras.layers.Add()([u9, c1])\n\n        # Output Layer\n        output_layer = keras.layers.Conv2D(self.out_classes, 3, activation=kwargs.get(\"ACTIVATION_FN\"), padding=\"same\", name=\"final_conv\")(u9)\n\n        # Build Model\n        model = keras.Model(inputs=inputs, outputs=output_layer, name=\"vanilla_unet\")\n\n        return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.__init__","title":"<code>__init__(self, features, out_classes, optimizer, loss)</code>  <code>special</code>","text":"<p>Initialize ModelBuilder with input size, output classes, optimizer, and loss.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>The input size of the models.</p> required <code>out_classes</code> <code>int</code> <p>The number of output classes for classification models.</p> required <code>optimizer</code> <code>tf.keras.optimizers.Optimizer</code> <p>The optimizer to use for model compilation.</p> required <code>loss</code> <code>tf.keras.losses.Loss</code> <p>The loss function to use for model compilation.</p> required Source code in <code>aces/model_builder.py</code> <pre><code>def __init__(self, features, out_classes, optimizer, loss):\n    \"\"\"\n    Initialize ModelBuilder with input size, output classes, optimizer, and loss.\n\n    Args:\n        in_size (int): The input size of the models.\n        out_classes (int): The number of output classes for classification models.\n        optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for model compilation.\n        loss (tf.keras.losses.Loss): The loss function to use for model compilation.\n    \"\"\"\n    self.features = features\n    self.in_size = len(features)\n    self.out_classes = out_classes\n    self.optimizer = optimizer\n    self.loss = loss\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_cnn_model","title":"<code>build_and_compile_cnn_model(self, **kwargs)</code>","text":"<p>Builds and compiles a Convolutional Neural Network (CNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled CNN model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_cnn_model(self, **kwargs):\n    \"\"\"\n    Builds and compiles a Convolutional Neural Network (CNN) model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled CNN model.\n    \"\"\"\n    inputs = keras.Input(shape=(kwargs.get(\"PATCH_SHAPE\", 128)[0], kwargs.get(\"PATCH_SHAPE\", 128)[0], self.in_size))\n    x = keras.layers.Conv2D(32, 3, activation=\"relu\", name=\"convd-1\", padding=\"same\")(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(64, 3, activation=\"relu\", name=\"convd-2\", padding=\"same\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(128, 3, activation=\"relu\", name=\"convd-3\", padding=\"same\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(128, 3, activation=\"relu\", name=\"convd-4\", padding=\"same\")(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    outputs = keras.layers.Conv2D(self.out_classes, (1, 1), activation=\"softmax\", name=\"final_conv\")(x)\n    model = keras.Model(inputs, outputs, name=\"cnn_model\")\n\n    metrics_list = [\n        Metrics.precision(),\n        Metrics.recall(),\n        keras.metrics.CategoricalAccuracy(),\n        Metrics.one_hot_io_u(self.out_classes),\n    ]\n\n    model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n    return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_dnn_model","title":"<code>build_and_compile_dnn_model(self, **kwargs)</code>","text":"<p>Builds and compiles a Deep Neural Network (DNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled DNN model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_dnn_model(self, **kwargs):\n    \"\"\"\n    Builds and compiles a Deep Neural Network (DNN) model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled DNN model.\n    \"\"\"\n    INITIAL_BIAS = kwargs.get(\"INITIAL_BIAS\", None)\n    print(f\"INITIAL_BIAS: {INITIAL_BIAS}\")\n\n    if INITIAL_BIAS is not None:\n        INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n    else:\n        INITIAL_BIAS = \"zeros\"\n\n    inputs = keras.Input(shape=(None, self.in_size), name=\"input_layer\")\n\n    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n    x = keras.layers.Dense(64, activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n    x = keras.layers.Dense(32, activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\", 0.))(x)\n    output = keras.layers.Dense(self.out_classes, activation=kwargs.get(\"ACTIVATION_FN\"), bias_initializer=INITIAL_BIAS)(x)\n\n    model = keras.models.Model(inputs=inputs, outputs=output)\n    metrics_list = [\n        Metrics.precision(),\n        Metrics.recall(),\n        keras.metrics.CategoricalAccuracy(),\n        Metrics.one_hot_io_u(self.out_classes),\n    ]\n\n    model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n    return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_dnn_model_for_ai_platform","title":"<code>build_and_compile_dnn_model_for_ai_platform(self, **kwargs)</code>","text":"<p>Builds and compiles a Deep Neural Network (DNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled DNN model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_dnn_model_for_ai_platform(self, **kwargs):\n    \"\"\"\n    Builds and compiles a Deep Neural Network (DNN) model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled DNN model.\n    \"\"\"\n    INITIAL_BIAS = kwargs.get(\"INITIAL_BIAS\", None)\n    print(f\"INITIAL_BIAS: {INITIAL_BIAS}\")\n    # DNN_DURING_ONLY = kwargs.get(\"DURING_ONLY\", False)\n\n    DERIVE_FEATURES = kwargs.get(\"DERIVE_FEATURES\", False)\n\n    if INITIAL_BIAS is not None:\n        INITIAL_BIAS = tf.keras.initializers.Constant(INITIAL_BIAS)\n    else:\n        INITIAL_BIAS = \"zeros\"\n\n    inputs_main = keras.Input(shape=(None, None, self.in_size), name=\"input_layer\")\n\n    if DERIVE_FEATURES:\n        inputs = rs.concatenate_features_for_dnn(inputs_main)\n    else:\n        inputs = inputs_main\n\n    x = keras.layers.Conv2D(256, (1, 1), activation=\"relu\")(inputs)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(128, (1, 1), activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(64, (1, 1), activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    x = keras.layers.Conv2D(32, (1, 1), activation=\"relu\")(x)\n    x = keras.layers.Dropout(kwargs.get(\"DROPOUT_RATE\"), 0.)(x)\n    output = keras.layers.Conv2D(self.out_classes, (1, 1), activation=kwargs.get(\"ACTIVATION_FN\"), bias_initializer=INITIAL_BIAS)(x)\n\n    model = keras.models.Model(inputs=inputs_main, outputs=output)\n\n    wrapped_model = ModelWrapper(ModelPreprocess(self.features), model)\n\n    metrics_list = [\n        Metrics.precision(),\n        Metrics.recall(),\n        keras.metrics.CategoricalAccuracy(),\n        Metrics.one_hot_io_u(self.out_classes),\n    ]\n    wrapped_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n    return model, wrapped_model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_unet_model","title":"<code>build_and_compile_unet_model(self, **kwargs)</code>","text":"<p>Builds and compiles a U-Net model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled U-Net model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_unet_model(self, **kwargs):\n    \"\"\"\n    Builds and compiles a U-Net model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled U-Net model.\n    \"\"\"\n    if len(kwargs.get(\"physical_devices\")) &gt; 0:\n        with tf.distribute.MirroredStrategy().scope():\n            model = self._build_and_compile_unet_model(**kwargs)\n            metrics_list = [\n                Metrics.precision(),\n                Metrics.recall(),\n                keras.metrics.CategoricalAccuracy(),\n                Metrics.one_hot_io_u(self.out_classes),\n            ]\n            model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n            return model\n    else:\n        print(\"No distributed strategy found.\")\n        model = self._build_and_compile_unet_model(**kwargs)\n        metrics_list = [\n            Metrics.precision(),\n            Metrics.recall(),\n            keras.metrics.CategoricalAccuracy(),\n            Metrics.one_hot_io_u(self.out_classes),\n        ]\n        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=metrics_list)\n        return model\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_and_compile_unet_model_for_ai_platform","title":"<code>build_and_compile_unet_model_for_ai_platform(self, **kwargs)</code>","text":"<p>Builds and compiles a U-Net model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled U-Net model.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_and_compile_unet_model_for_ai_platform(self, **kwargs):\n    \"\"\"\n    Builds and compiles a U-Net model.\n\n    Args:\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        keras.Model: The compiled U-Net model.\n    \"\"\"\n    if len(kwargs.get(\"physical_devices\")) &gt; 0:\n        with tf.distribute.MirroredStrategy().scope():\n            return self._build_and_compile_unet_model_for_ai_plaform(**kwargs)\n    else:\n        print(\"No distributed strategy found.\")\n        return self._build_and_compile_unet_model(**kwargs)\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelBuilder.build_model","title":"<code>build_model(self, model_type, **kwargs)</code>","text":"<p>Builds and compiles a neural network model based on the provided model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to build (\"dnn\", \"cnn\", \"unet\").</p> required <code>**kwargs</code> <p>Additional keyword arguments specific to the model type.</p> <code>{}</code> <p>Returns:</p> Type Description <code>keras.Model</code> <p>The compiled neural network model.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If an invalid model type is provided.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def build_model(self, model_type, **kwargs):\n    \"\"\"\n    Builds and compiles a neural network model based on the provided model type.\n\n    Args:\n        model_type (str): The type of the model to build (\"dnn\", \"cnn\", \"unet\").\n        **kwargs: Additional keyword arguments specific to the model type.\n\n    Returns:\n        keras.Model: The compiled neural network model.\n\n    Raises:\n        ValueError: If an invalid model type is provided.\n    \"\"\"\n    FOR_AI_PLATFORM = kwargs.get(\"FOR_AI_PLATFORM\", False)\n    if model_type == \"dnn\":\n        if FOR_AI_PLATFORM:\n            return self.build_and_compile_dnn_model_for_ai_platform(**kwargs)\n        else:\n            return self.build_and_compile_dnn_model(**kwargs)\n    elif model_type == \"cnn\":\n        return self.build_and_compile_cnn_model(**kwargs)\n    elif model_type == \"unet\":\n        if FOR_AI_PLATFORM:\n            return self.build_and_compile_unet_model_for_ai_platform(**kwargs)\n        else:\n            return self.build_and_compile_unet_model(**kwargs)\n    else:\n        raise ValueError(f\"Invalid model type: {model_type}\")\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelPreprocess","title":"<code> ModelPreprocess            (Layer)         </code>","text":"Source code in <code>aces/model_builder.py</code> <pre><code>class ModelPreprocess(keras.layers.Layer):\n    def __init__(self, in_features, **kwargs):\n        self.in_features = in_features\n        super(ModelPreprocess, self).__init__(**kwargs)\n\n    def call(self, features_dict):\n        # (None, 1, 1, 1) -&gt; (None, 1, 1, P) for dnn\n        # (None, H, W, 1) -&gt; (None, H, W, P) for cnn/unet\n        return tf.concat([features_dict[b] for b in self.in_features], axis=3)\n\n    def get_config(self):\n        config = super().get_config()\n        return config\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelPreprocess.call","title":"<code>call(self, features_dict)</code>","text":"<p>This is where the layer's logic lives.</p> <p>The <code>call()</code> method may not create state (except in its first invocation, wrapping the creation of variables or other resources in <code>tf.init_scope()</code>).  It is recommended to create state, including <code>tf.Variable</code> instances and nested <code>Layer</code> instances,  in <code>__init__()</code>, or in the <code>build()</code> method that is called automatically before <code>call()</code> executes for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Input tensor, or dict/list/tuple of input tensors. The first positional <code>inputs</code> argument is subject to special rules: - <code>inputs</code> must be explicitly passed. A layer cannot have zero   arguments, and <code>inputs</code> cannot be provided via the default value   of a keyword argument. - NumPy array or Python scalar values in <code>inputs</code> get cast as   tensors. - Keras mask metadata is only collected from <code>inputs</code>. - Layers are built (<code>build(input_shape)</code> method)   using shape info from <code>inputs</code> only. - <code>input_spec</code> compatibility is only checked against <code>inputs</code>. - Mixed precision input casting is only applied to <code>inputs</code>.   If a layer has tensor arguments in <code>*args</code> or <code>**kwargs</code>, their   casting behavior in mixed precision should be handled manually. - The SavedModel input specification is generated using <code>inputs</code>   only. - Integration with various ecosystem packages like TFMOT, TFLite,   TF.js, etc is only supported for <code>inputs</code> and not for tensors in   positional and keyword arguments.</p> required <code>*args</code> <p>Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above.</p> required <code>**kwargs</code> <p>Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: - <code>training</code>: Boolean scalar tensor of Python boolean indicating   whether the <code>call</code> is meant for training or inference. - <code>mask</code>: Boolean input mask. If the layer's <code>call()</code> method takes a   <code>mask</code> argument, its default value will be set to the mask   generated for <code>inputs</code> by the previous layer (if <code>input</code> did come   from a layer that generated a corresponding mask, i.e. if it came   from a Keras layer with masking support).</p> required <p>Returns:</p> Type Description <p>A tensor or list/tuple of tensors.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def call(self, features_dict):\n    # (None, 1, 1, 1) -&gt; (None, 1, 1, P) for dnn\n    # (None, H, W, 1) -&gt; (None, H, W, P) for cnn/unet\n    return tf.concat([features_dict[b] for b in self.in_features], axis=3)\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelPreprocess.get_config","title":"<code>get_config(self)</code>","text":"<p>Returns the config of the layer.</p> <p>A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration.</p> <p>The config of a layer does not include connectivity information, nor the layer class name. These are handled by <code>Network</code> (one layer of abstraction above).</p> <p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it.</p> <p>Returns:</p> Type Description <p>Python dictionary.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def get_config(self):\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelWrapper","title":"<code> ModelWrapper            (Model)         </code>","text":"Source code in <code>aces/model_builder.py</code> <pre><code>class ModelWrapper(keras.Model):\n    def __init__(self, preprocessing, backbone, **kwargs):\n        super().__init__(**kwargs)\n        self.preprocessing = preprocessing\n        self.backbone = backbone\n\n    def call(self, features_dict):\n        x = self.preprocessing(features_dict)\n        return self.backbone(x)\n\n    def get_config(self):\n        config = super().get_config()\n        return config\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelWrapper.call","title":"<code>call(self, features_dict)</code>","text":"<p>Calls the model on new inputs and returns the outputs as tensors.</p> <p>In this case <code>call()</code> just reapplies all ops in the graph to the new inputs (e.g. build a new computational graph from the provided inputs).</p> <p>Note: This method should not be called directly. It is only meant to be overridden when subclassing <code>tf.keras.Model</code>. To call a model on an input, always use the <code>__call__()</code> method, i.e. <code>model(inputs)</code>, which relies on the underlying <code>call()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Input tensor, or dict/list/tuple of input tensors.</p> required <code>training</code> <p>Boolean or boolean scalar tensor, indicating whether to run the <code>Network</code> in training mode or inference mode.</p> required <code>mask</code> <p>A mask or list of masks. A mask can be either a boolean tensor or None (no mask). For more details, check the guide here.</p> required <p>Returns:</p> Type Description <p>A tensor if there is a single output, or a list of tensors if there are more than one outputs.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def call(self, features_dict):\n    x = self.preprocessing(features_dict)\n    return self.backbone(x)\n</code></pre>"},{"location":"model_builder/#aces.model_builder.ModelWrapper.get_config","title":"<code>get_config(self)</code>","text":"<p>Returns the config of the <code>Model</code>.</p> <p>Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a <code>Model</code>. This allows the <code>Model</code> to be be reinstantiated later (without its trained weights) from this configuration.</p> <p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it.</p> <p>Developers of subclassed <code>Model</code> are advised to override this method, and continue to update the dict from <code>super(MyModel, self).get_config()</code> to provide the proper configuration of this <code>Model</code>. The default config will return config dict for init parameters if they are basic types. Raises <code>NotImplementedError</code> when in cases where a custom <code>get_config()</code> implementation is required for the subclassed model.</p> <p>Returns:</p> Type Description <p>Python dictionary containing the configuration of this <code>Model</code>.</p> Source code in <code>aces/model_builder.py</code> <pre><code>def get_config(self):\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"model_trainer/","title":"model_trainer module","text":""},{"location":"model_trainer/#aces.model_trainer.ModelTrainer","title":"<code> ModelTrainer        </code>","text":"<p>A class for training, buidling, compiling, and running specified deep learning models.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>An object containing the configuration settings for model training.</p> <code>model_builder</code> <p>An instance of ModelBuilder for building the model.</p> <code>build_model</code> <p>A partial function for building the model with the specified model type.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>class ModelTrainer:\n    \"\"\"\n    A class for training, buidling, compiling, and running specified deep learning models.\n\n    Attributes:\n        config: An object containing the configuration settings for model training.\n        model_builder: An instance of ModelBuilder for building the model.\n        build_model: A partial function for building the model with the specified model type.\n    \"\"\"\n    def __init__(self, config: Config):\n        \"\"\"\n        Initialize the ModelTrainer object.\n\n        Args:\n            config: An object containing the configuration settings for model training.\n\n        Attributes:\n            config: The configuration settings for model training.\n            model_builder: An instance of ModelBuilder for building the model.\n            build_model: A partial function for building the model with the specified model type.\n        \"\"\"\n        self.config = config\n        # @FIXME: This isn't producing reproducable results\n        if self.config.USE_SEED:\n            # producable results\n            import random\n            print(f\"Using seed: {self.config.SEED}\")\n            tf.random.set_seed(self.config.SEED)\n            np.random.seed(self.config.SEED)\n            random.seed(self.config.SEED)\n\n        # @ToDO: Create a way to autoload loss function from the list without if else\n        if self.config.LOSS == \"custom_focal_tversky_loss\":\n            self.config.LOSS = Metrics.focal_tversky_loss\n            self.LOSS_TXT = Metrics.focal_tversky_loss.__func__.__name__ # \"focal_tversky_loss\"\n        else:\n            self.config.LOSS_TXT = self.config.LOSS\n\n        self.model_builder = ModelBuilder(\n            features=self.config.FEATURES,\n            out_classes=self.config.OUT_CLASS_NUM,\n            optimizer=self.config.OPTIMIZER,\n            loss=self.config.LOSS\n        )\n        self.build_model = partial(self.model_builder.build_model, model_type=self.config.MODEL_TYPE,\n                                   **{\"FOR_AI_PLATFORM\": self.config.USE_AI_PLATFORM,\n                                      \"DERIVE_FEATURES\": self.config.DERIVE_FEATURES if hasattr(self.config, \"DERIVE_FEATURES\") else False,})\n\n    def train_model(self) -&gt; None:\n        \"\"\"\n        Train the model using the provided configuration settings.\n\n        This method performs the following steps:\n\n        1. Configures memory growth for TensorFlow.\n\n        2. Creates TensorFlow datasets for training, testing, and validation.\n\n        3. Builds and compiles the model.\n\n        4. Prepares the output directory for saving models and results.\n\n        5. Starts the training process.\n\n        6. Evaluates and prints validation metrics.\n\n        7. Saves training parameters, plots, and models.\n        \"\"\"\n        print(\"****************************************************************************\")\n        print(\"****************************** Clear Session... ****************************\")\n        keras.backend.clear_session()\n        print(\"****************************************************************************\")\n        print(f\"****************************** Configure memory growth... ************************\")\n        physical_devices = TFUtils.configure_memory_growth()\n        self.config.physical_devices = physical_devices\n        print(\"****************************************************************************\")\n        print(\"****************************** creating datasets... ************************\")\n        self.create_datasets(print_info=self.config.PRINT_INFO)\n\n        if self.config.USE_AI_PLATFORM:\n            print(\"****************************************************************************\")\n            print(\"******* building and compiling model for ai platform... ********************\")\n            self.build_and_compile_model_ai_platform()\n        else:\n            print(\"****************************************************************************\")\n            print(\"************************ building and compiling model... *******************\")\n            self.build_and_compile_model(print_model_summary=True)\n\n        print(\"****************************************************************************\")\n        print(\"************************ preparing output directory... *********************\")\n        self.prepare_output_dir()\n        print(\"****************************************************************************\")\n        print(\"****************************** training model... ***************************\")\n        self.start_training()\n\n        if self.config.USE_AI_PLATFORM:\n            print(self.model.summary())\n\n        print(\"****************************************************************************\")\n        print(\"****************************** evaluating model... *************************\")\n        self.evaluate_and_print_val()\n        print(\"****************************************************************************\")\n        print(\"****************************** saving parameters... ************************\")\n        ModelTrainer.save_parameters(**self.config.__dict__)\n        print(\"****************************************************************************\")\n        print(\"*************** saving model config and history object... ******************\")\n        self.save_history_object()\n        if self.config.USE_AI_PLATFORM:\n            ModelTrainer.save_model_config(self.config.MODEL_SAVE_DIR, **self._model.get_config())\n        else:\n            ModelTrainer.save_model_config(self.config.MODEL_SAVE_DIR, **self.model.get_config())\n        print(\"****************************************************************************\")\n        print(\"****************************** saving plots... *****************************\")\n        self.save_plots()\n        print(\"****************************************************************************\")\n        print(\"****************************** saving models... ****************************\")\n        self.save_models()\n        print(\"****************************************************************************\")\n\n    def prepare_output_dir(self) -&gt; None:\n        \"\"\"\n        Prepare the output directory for saving models and results.\n\n        Creates a directory with a timestamped name and increments the version number if necessary.\n        \"\"\"\n        if not self.config.AUTO_MODEL_DIR_NAME:\n            self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.config.MODEL_DIR_NAME\n            print(f\"&gt; Saving models and results at {self.config.MODEL_SAVE_DIR}...\")\n            if not os.path.exists(self.config.MODEL_SAVE_DIR):\n                os.mkdir(self.config.MODEL_SAVE_DIR)\n        else:\n            today = datetime.date.today().strftime(\"%Y_%m_%d\")\n            iterator = 1\n            while True:\n                model_dir_name = f\"trial_{self.config.MODEL_TYPE}_{today}_v{iterator}\"\n                self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / model_dir_name\n                try:\n                    os.mkdir(self.config.MODEL_SAVE_DIR)\n                except FileExistsError:\n                    print(f\"&gt; {self.config.MODEL_SAVE_DIR} exists, creating another version...\")\n                    iterator += 1\n                    continue\n                else:\n                    print(f\"&gt; Saving models and results at {self.config.MODEL_SAVE_DIR}...\")\n                    break\n\n    def create_datasets(self, print_info: bool = False) -&gt; None:\n        \"\"\"\n        Create TensorFlow datasets for training, testing, and validation.\n\n        Args:\n            print_info: Flag indicating whether to print dataset information.\n\n        Prints information about the created datasets if print_info is set to True.\n        \"\"\"\n        self.TRAINING_DATASET = DataProcessor.get_dataset(\n            f\"{str(self.config.TRAINING_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            self.config.BATCH_SIZE,\n            self.config.OUT_CLASS_NUM,\n            **{**self.config.__dict__, \"training\": True},\n        ).repeat()\n\n        self.VALIDATION_DATASET = DataProcessor.get_dataset(\n            f\"{str(self.config.VALIDATION_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            1,\n            self.config.OUT_CLASS_NUM,\n            **self.config.__dict__,\n        ).repeat()\n\n        self.TESTING_DATASET = DataProcessor.get_dataset(\n            f\"{str(self.config.TESTING_DIR)}/*\",\n            self.config.FEATURES,\n            self.config.LABELS,\n            self.config.PATCH_SHAPE[0],\n            1,\n            self.config.OUT_CLASS_NUM,\n            **self.config.__dict__,\n        )\n\n        if print_info:\n            print(\"Printing dataset info:\")\n            DataProcessor.print_dataset_info(self.TRAINING_DATASET, \"Training\")\n            DataProcessor.print_dataset_info(self.TESTING_DATASET, \"Testing\")\n            DataProcessor.print_dataset_info(self.VALIDATION_DATASET, \"Validation\")\n\n    def build_and_compile_model(self, print_model_summary: bool = True) -&gt; None:\n        \"\"\"\n        Build and compile the model.\n\n        Args:\n            print_model_summary: Flag indicating whether to print the model summary.\n\n        Builds and compiles the model using the provided configuration settings.\n\n        Prints the model summary if print_model_summary is set to True.\n        \"\"\"\n        self.model = self.build_model(**self.config.__dict__)\n        if print_model_summary:  print(self.model.summary())\n\n    def build_and_compile_model_ai_platform(self) -&gt; None:\n        \"\"\"\n        Build and compile the model.\n\n        Args:\n            print_model_summary: Flag indicating whether to print the model summary.\n\n        Builds and compiles the model using the provided configuration settings.\n\n        Prints the model summary if print_model_summary is set to True.\n        \"\"\"\n        model, wrapped_model = self.build_model(**self.config.__dict__)\n        print(model.summary())\n        self._model = model\n        self.model = wrapped_model\n\n    def start_training(self) -&gt; None:\n        \"\"\"\n        Start the training process.\n\n        Trains the model using the provided configuration settings and callbacks.\n        \"\"\"\n        model_checkpoint = callbacks.ModelCheckpoint(\n            f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}\",\n            monitor=self.config.CALLBACK_PARAMETER,\n            save_best_only=True,\n            mode=\"auto\",\n            verbose=1,\n            save_weights_only=False,\n        )  # save best model\n\n        tensorboard = callbacks.TensorBoard(log_dir=str(self.config.MODEL_SAVE_DIR / \"logs\"), write_images=True)\n\n        def lr_scheduler(epoch):\n            if epoch &lt; self.config.RAMPUP_EPOCHS:\n                return self.config.MAX_LR\n            elif epoch &lt; self.config.RAMPUP_EPOCHS + self.config.SUSTAIN_EPOCHS:\n                return self.config.MID_LR\n            else:\n                return self.config.MIN_LR\n\n        model_callbacks = [model_checkpoint, tensorboard]\n\n        if self.config.USE_ADJUSTED_LR:\n            lr_callback = callbacks.LearningRateScheduler(lambda epoch: lr_scheduler(epoch), verbose=True)\n            model_callbacks.append(lr_callback)\n\n        if self.config.EARLY_STOPPING:\n            early_stopping = callbacks.EarlyStopping(\n                monitor=self.config.CALLBACK_PARAMETER,\n                patience=int(0.3 * self.config.EPOCHS),\n                verbose=1,\n                mode=\"auto\",\n                restore_best_weights=True,\n            )\n            model_callbacks.append(early_stopping)\n\n        self.model_callbacks = model_callbacks\n\n        self.history = self.model.fit(\n            x=self.TRAINING_DATASET,\n            epochs=self.config.EPOCHS,\n            steps_per_epoch=(self.config.TRAIN_SIZE // self.config.BATCH_SIZE),\n            validation_data=self.VALIDATION_DATASET,\n            validation_steps=self.config.VAL_SIZE,\n            callbacks=model_callbacks,\n        )\n\n        # either save the wrapped model or the original model\n        # named as \"trained-model\" to avoid confusion\n        # self.model.save(f\"{self.config.MODEL_SAVE_DIR}/trained-wrapped-model\")\n        self.model.save(f\"{self.config.MODEL_SAVE_DIR}/trained-model\")\n\n    def evaluate_and_print_val(self) -&gt; None:\n        \"\"\"\n        Evaluate and print validation metrics.\n\n        Evaluates the model on the validation dataset and prints the metrics.\n        \"\"\"\n        print(\"************************************************\")\n        print(\"************************************************\")\n        print(\"Validation\")\n        # Tip: You can remove steps=self.config.TEST_SIZE and match the TEST_SIZE from the env\n        evaluate_results = self.model.evaluate(self.TESTING_DATASET) # , steps=self.config.TEST_SIZE\n        with open(f\"{self.config.MODEL_SAVE_DIR}/evaluation.txt\", \"w\") as evaluate:\n            evaluate.write(json.dumps(dict(zip(self.model.metrics_names, evaluate_results))))\n        for name, value in zip(self.model.metrics_names, evaluate_results):\n            print(f\"{name}: {value}\")\n        print(\"\\n\")\n\n    @staticmethod\n    def save_parameters(**config) -&gt; None:\n        \"\"\"\n        Save the training parameters to a text file.\n\n        Saves the training parameters used in the configuration settings to a text file.\n        \"\"\"\n        with open(f\"{str(config.get('MODEL_SAVE_DIR'))}/parameters.txt\", \"w\") as f:\n            f.write(f\"TRAIN_SIZE: {config.get('TRAIN_SIZE')}\\n\")\n            f.write(f\"TEST_SIZE: {config.get('TEST_SIZE')}\\n\")\n            f.write(f\"VAL_SIZE: {config.get('VAL_SIZE')}\\n\")\n            f.write(f\"BATCH_SIZE: {config.get('BATCH_SIZE')}\\n\")\n            f.write(f\"EPOCHS: {config.get('EPOCHS')}\\n\")\n            f.write(f\"LOSS: {config.get('LOSS_TXT')}\\n\")\n            f.write(f\"TRAINING_DIR: {config.get('TRAINING_DIR')}\\n\")\n            f.write(f\"TESTING_DIR: {config.get('TESTING_DIR')}\\n\")\n            f.write(f\"VALIDATION_DIR: {config.get('VALIDATION_DIR')}\\n\")\n            if config.get('USE_ADJUSTED_LR'):\n                f.write(f\"USE_ADJUSTED_LR: {config.get('USE_ADJUSTED_LR')}\\n\")\n                f.write(f\"MAX_LR: {config.get('MAX_LR')}\\n\")\n                f.write(f\"MID_LR: {config.get('MID_LR')}\\n\")\n                f.write(f\"MIN_LR: {config.get('MIN_LR')}\\n\")\n                f.write(f\"RAMPUP_EPOCHS: {config.get('RAMPUP_EPOCHS')}\\n\")\n                f.write(f\"SUSTAIN_EPOCHS: {config.get('SUSTAIN_EPOCHS')}\\n\")\n            f.write(f\"DROPOUT_RATE: {config.get('DROPOUT_RATE')}\\n\")\n            f.write(f\"ACTIVATION_FN: {config.get('ACTIVATION_FN')}\\n\")\n            f.write(f\"FEATURES: {config.get('FEATURES')}\\n\")\n            f.write(f\"LABELS: {config.get('LABELS')}\\n\")\n            f.write(f\"PATCH_SHAPE: {config.get('PATCH_SHAPE')}\\n\")\n            f.write(f\"CALLBACK_PARAMETER: {config.get('CALLBACK_PARAMETER')}\\n\")\n            f.write(f\"MODEL_TYPE: {config.get('MODEL_TYPE')}\\n\")\n            f.write(f\"TRANSFORM_DATA: {config.get('TRANSFORM_DATA')}\\n\")\n            f.write(f\"MODEL_NAME: {config.get('MODEL_NAME')}.h5\\n\")\n            f.write(f\"MODEL_CHECKPOINT_NAME: {config.get('MODEL_CHECKPOINT_NAME')}.h5\\n\")\n        f.close()\n\n    @staticmethod\n    def save_model_config(save_dir, **model_config) -&gt; None:\n        with open(f\"{save_dir}/config.json\", \"w\") as f:\n            json.dump(model_config, f, indent=4)\n        f.close()\n\n    def save_plots(self) -&gt; None:\n        \"\"\"\n        Save plots and model visualization.\n\n        Saves the model architecture plot, training history plot, and model object.\n        \"\"\"\n        print(f\"Saving plots and model visualization at {self.config.MODEL_SAVE_DIR}...\")\n\n        Utils.plot_metrics([key.replace(\"val_\", \"\") for key in self.history.history.keys() if key.startswith(\"val_\")],\n                           self.history.history, len(self.history.epoch), self.config.MODEL_SAVE_DIR)\n\n        if self.config.USE_AI_PLATFORM:\n            keras.utils.plot_model(self._model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True, rankdir=\"TB\")\n            keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/wrapped_model.png\", show_shapes=True, rankdir=\"LR\") # rankdir='TB'\n        else:\n            keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True, rankdir=\"TB\") # rankdir='TB'\n\n    def save_history_object(self) -&gt; None:\n        \"\"\"\n        Save the history object.\n        \"\"\"\n        with open(f\"{self.config.MODEL_SAVE_DIR}/model.pkl\", \"wb\") as f:\n            pickle.dump(self.history.history, f)\n\n        with open(f\"{self.config.MODEL_SAVE_DIR}/model.txt\", \"w\") as f:\n            f.write(json.dumps(self.history.history))\n\n\n    def load_and_save_models(self) -&gt; None:\n        \"\"\"\n        Load the trained models.\n\n        Loads the trained models from different formats: h5 and tf formats.\n        \"\"\"\n        self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.config.MODEL_DIR_NAME\n        self.model = tf.keras.models.load_model(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}.tf\")\n        updated_model = self.serialize_model()\n        # if not issubclass(self.model.__class__, keras.Model):\n        #     # Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model\n        #     updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n        updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n        updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n\n\n    def save_models(self) -&gt; None:\n        \"\"\"\n        Save the trained models.\n\n        Saves the trained models in different formats: h5 and tf formats.\n        \"\"\"\n        if self.config.USE_AI_PLATFORM:\n            updated_model = self.serialize_model()\n            # updated_model = self.model\n\n            # if not issubclass(self.model.__class__, keras.Model):\n            #     # Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model\n            #     updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n\n            # updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n            # updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n            updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}\")\n        else:\n            if not issubclass(self.model.__class__, keras.Model):\n                self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n            self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}\", save_format=\"tf\")\n\n    def serialize_model(self) -&gt; tf.keras.Model:\n        \"\"\"\n        Serialize and save the trained models.\n\n        Saves the trained models in different formats: h5 and tf formats.\n        \"\"\"\n        input_deserializer = DeSerializeInput(self.config.FEATURES)\n        output_deserializer = ReSerializeOutput()\n        serialized_inputs = {\n            b: tf.keras.Input(shape=[], dtype=\"string\", name=b) for b in self.config.FEATURES\n        }\n        updated_model_input = input_deserializer(serialized_inputs)\n        updated_model = self.model(updated_model_input)\n        updated_model = output_deserializer(updated_model, \"output\")\n        updated_model = tf.keras.Model(serialized_inputs, updated_model)\n        keras.utils.plot_model(updated_model, f\"{self.config.MODEL_SAVE_DIR}/serialized_model.png\", show_shapes=True, rankdir=\"LR\")\n        return updated_model\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.__init__","title":"<code>__init__(self, config)</code>  <code>special</code>","text":"<p>Initialize the ModelTrainer object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>An object containing the configuration settings for model training.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <p>The configuration settings for model training.</p> <code>model_builder</code> <p>An instance of ModelBuilder for building the model.</p> <code>build_model</code> <p>A partial function for building the model with the specified model type.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def __init__(self, config: Config):\n    \"\"\"\n    Initialize the ModelTrainer object.\n\n    Args:\n        config: An object containing the configuration settings for model training.\n\n    Attributes:\n        config: The configuration settings for model training.\n        model_builder: An instance of ModelBuilder for building the model.\n        build_model: A partial function for building the model with the specified model type.\n    \"\"\"\n    self.config = config\n    # @FIXME: This isn't producing reproducable results\n    if self.config.USE_SEED:\n        # producable results\n        import random\n        print(f\"Using seed: {self.config.SEED}\")\n        tf.random.set_seed(self.config.SEED)\n        np.random.seed(self.config.SEED)\n        random.seed(self.config.SEED)\n\n    # @ToDO: Create a way to autoload loss function from the list without if else\n    if self.config.LOSS == \"custom_focal_tversky_loss\":\n        self.config.LOSS = Metrics.focal_tversky_loss\n        self.LOSS_TXT = Metrics.focal_tversky_loss.__func__.__name__ # \"focal_tversky_loss\"\n    else:\n        self.config.LOSS_TXT = self.config.LOSS\n\n    self.model_builder = ModelBuilder(\n        features=self.config.FEATURES,\n        out_classes=self.config.OUT_CLASS_NUM,\n        optimizer=self.config.OPTIMIZER,\n        loss=self.config.LOSS\n    )\n    self.build_model = partial(self.model_builder.build_model, model_type=self.config.MODEL_TYPE,\n                               **{\"FOR_AI_PLATFORM\": self.config.USE_AI_PLATFORM,\n                                  \"DERIVE_FEATURES\": self.config.DERIVE_FEATURES if hasattr(self.config, \"DERIVE_FEATURES\") else False,})\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.build_and_compile_model","title":"<code>build_and_compile_model(self, print_model_summary=True)</code>","text":"<p>Build and compile the model.</p> <p>Parameters:</p> Name Type Description Default <code>print_model_summary</code> <code>bool</code> <p>Flag indicating whether to print the model summary.</p> <code>True</code> <p>Builds and compiles the model using the provided configuration settings.</p> <p>Prints the model summary if print_model_summary is set to True.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def build_and_compile_model(self, print_model_summary: bool = True) -&gt; None:\n    \"\"\"\n    Build and compile the model.\n\n    Args:\n        print_model_summary: Flag indicating whether to print the model summary.\n\n    Builds and compiles the model using the provided configuration settings.\n\n    Prints the model summary if print_model_summary is set to True.\n    \"\"\"\n    self.model = self.build_model(**self.config.__dict__)\n    if print_model_summary:  print(self.model.summary())\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.build_and_compile_model_ai_platform","title":"<code>build_and_compile_model_ai_platform(self)</code>","text":"<p>Build and compile the model.</p> <p>Parameters:</p> Name Type Description Default <code>print_model_summary</code> <p>Flag indicating whether to print the model summary.</p> required <p>Builds and compiles the model using the provided configuration settings.</p> <p>Prints the model summary if print_model_summary is set to True.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def build_and_compile_model_ai_platform(self) -&gt; None:\n    \"\"\"\n    Build and compile the model.\n\n    Args:\n        print_model_summary: Flag indicating whether to print the model summary.\n\n    Builds and compiles the model using the provided configuration settings.\n\n    Prints the model summary if print_model_summary is set to True.\n    \"\"\"\n    model, wrapped_model = self.build_model(**self.config.__dict__)\n    print(model.summary())\n    self._model = model\n    self.model = wrapped_model\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.create_datasets","title":"<code>create_datasets(self, print_info=False)</code>","text":"<p>Create TensorFlow datasets for training, testing, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>print_info</code> <code>bool</code> <p>Flag indicating whether to print dataset information.</p> <code>False</code> <p>Prints information about the created datasets if print_info is set to True.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def create_datasets(self, print_info: bool = False) -&gt; None:\n    \"\"\"\n    Create TensorFlow datasets for training, testing, and validation.\n\n    Args:\n        print_info: Flag indicating whether to print dataset information.\n\n    Prints information about the created datasets if print_info is set to True.\n    \"\"\"\n    self.TRAINING_DATASET = DataProcessor.get_dataset(\n        f\"{str(self.config.TRAINING_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        self.config.BATCH_SIZE,\n        self.config.OUT_CLASS_NUM,\n        **{**self.config.__dict__, \"training\": True},\n    ).repeat()\n\n    self.VALIDATION_DATASET = DataProcessor.get_dataset(\n        f\"{str(self.config.VALIDATION_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        1,\n        self.config.OUT_CLASS_NUM,\n        **self.config.__dict__,\n    ).repeat()\n\n    self.TESTING_DATASET = DataProcessor.get_dataset(\n        f\"{str(self.config.TESTING_DIR)}/*\",\n        self.config.FEATURES,\n        self.config.LABELS,\n        self.config.PATCH_SHAPE[0],\n        1,\n        self.config.OUT_CLASS_NUM,\n        **self.config.__dict__,\n    )\n\n    if print_info:\n        print(\"Printing dataset info:\")\n        DataProcessor.print_dataset_info(self.TRAINING_DATASET, \"Training\")\n        DataProcessor.print_dataset_info(self.TESTING_DATASET, \"Testing\")\n        DataProcessor.print_dataset_info(self.VALIDATION_DATASET, \"Validation\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.evaluate_and_print_val","title":"<code>evaluate_and_print_val(self)</code>","text":"<p>Evaluate and print validation metrics.</p> <p>Evaluates the model on the validation dataset and prints the metrics.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def evaluate_and_print_val(self) -&gt; None:\n    \"\"\"\n    Evaluate and print validation metrics.\n\n    Evaluates the model on the validation dataset and prints the metrics.\n    \"\"\"\n    print(\"************************************************\")\n    print(\"************************************************\")\n    print(\"Validation\")\n    # Tip: You can remove steps=self.config.TEST_SIZE and match the TEST_SIZE from the env\n    evaluate_results = self.model.evaluate(self.TESTING_DATASET) # , steps=self.config.TEST_SIZE\n    with open(f\"{self.config.MODEL_SAVE_DIR}/evaluation.txt\", \"w\") as evaluate:\n        evaluate.write(json.dumps(dict(zip(self.model.metrics_names, evaluate_results))))\n    for name, value in zip(self.model.metrics_names, evaluate_results):\n        print(f\"{name}: {value}\")\n    print(\"\\n\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.load_and_save_models","title":"<code>load_and_save_models(self)</code>","text":"<p>Load the trained models.</p> <p>Loads the trained models from different formats: h5 and tf formats.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def load_and_save_models(self) -&gt; None:\n    \"\"\"\n    Load the trained models.\n\n    Loads the trained models from different formats: h5 and tf formats.\n    \"\"\"\n    self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.config.MODEL_DIR_NAME\n    self.model = tf.keras.models.load_model(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}.tf\")\n    updated_model = self.serialize_model()\n    # if not issubclass(self.model.__class__, keras.Model):\n    #     # Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model\n    #     updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n    updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n    updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.prepare_output_dir","title":"<code>prepare_output_dir(self)</code>","text":"<p>Prepare the output directory for saving models and results.</p> <p>Creates a directory with a timestamped name and increments the version number if necessary.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def prepare_output_dir(self) -&gt; None:\n    \"\"\"\n    Prepare the output directory for saving models and results.\n\n    Creates a directory with a timestamped name and increments the version number if necessary.\n    \"\"\"\n    if not self.config.AUTO_MODEL_DIR_NAME:\n        self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / self.config.MODEL_DIR_NAME\n        print(f\"&gt; Saving models and results at {self.config.MODEL_SAVE_DIR}...\")\n        if not os.path.exists(self.config.MODEL_SAVE_DIR):\n            os.mkdir(self.config.MODEL_SAVE_DIR)\n    else:\n        today = datetime.date.today().strftime(\"%Y_%m_%d\")\n        iterator = 1\n        while True:\n            model_dir_name = f\"trial_{self.config.MODEL_TYPE}_{today}_v{iterator}\"\n            self.config.MODEL_SAVE_DIR = self.config.OUTPUT_DIR / model_dir_name\n            try:\n                os.mkdir(self.config.MODEL_SAVE_DIR)\n            except FileExistsError:\n                print(f\"&gt; {self.config.MODEL_SAVE_DIR} exists, creating another version...\")\n                iterator += 1\n                continue\n            else:\n                print(f\"&gt; Saving models and results at {self.config.MODEL_SAVE_DIR}...\")\n                break\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_history_object","title":"<code>save_history_object(self)</code>","text":"<p>Save the history object.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_history_object(self) -&gt; None:\n    \"\"\"\n    Save the history object.\n    \"\"\"\n    with open(f\"{self.config.MODEL_SAVE_DIR}/model.pkl\", \"wb\") as f:\n        pickle.dump(self.history.history, f)\n\n    with open(f\"{self.config.MODEL_SAVE_DIR}/model.txt\", \"w\") as f:\n        f.write(json.dumps(self.history.history))\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_models","title":"<code>save_models(self)</code>","text":"<p>Save the trained models.</p> <p>Saves the trained models in different formats: h5 and tf formats.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_models(self) -&gt; None:\n    \"\"\"\n    Save the trained models.\n\n    Saves the trained models in different formats: h5 and tf formats.\n    \"\"\"\n    if self.config.USE_AI_PLATFORM:\n        updated_model = self.serialize_model()\n        # updated_model = self.model\n\n        # if not issubclass(self.model.__class__, keras.Model):\n        #     # Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model\n        #     updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n\n        # updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n        # updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.tf\", save_format=\"tf\")\n        updated_model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}\")\n    else:\n        if not issubclass(self.model.__class__, keras.Model):\n            self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}.h5\", save_format=\"h5\")\n        self.model.save(f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_NAME}\", save_format=\"tf\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_parameters","title":"<code>save_parameters(**config)</code>  <code>staticmethod</code>","text":"<p>Save the training parameters to a text file.</p> <p>Saves the training parameters used in the configuration settings to a text file.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>@staticmethod\ndef save_parameters(**config) -&gt; None:\n    \"\"\"\n    Save the training parameters to a text file.\n\n    Saves the training parameters used in the configuration settings to a text file.\n    \"\"\"\n    with open(f\"{str(config.get('MODEL_SAVE_DIR'))}/parameters.txt\", \"w\") as f:\n        f.write(f\"TRAIN_SIZE: {config.get('TRAIN_SIZE')}\\n\")\n        f.write(f\"TEST_SIZE: {config.get('TEST_SIZE')}\\n\")\n        f.write(f\"VAL_SIZE: {config.get('VAL_SIZE')}\\n\")\n        f.write(f\"BATCH_SIZE: {config.get('BATCH_SIZE')}\\n\")\n        f.write(f\"EPOCHS: {config.get('EPOCHS')}\\n\")\n        f.write(f\"LOSS: {config.get('LOSS_TXT')}\\n\")\n        f.write(f\"TRAINING_DIR: {config.get('TRAINING_DIR')}\\n\")\n        f.write(f\"TESTING_DIR: {config.get('TESTING_DIR')}\\n\")\n        f.write(f\"VALIDATION_DIR: {config.get('VALIDATION_DIR')}\\n\")\n        if config.get('USE_ADJUSTED_LR'):\n            f.write(f\"USE_ADJUSTED_LR: {config.get('USE_ADJUSTED_LR')}\\n\")\n            f.write(f\"MAX_LR: {config.get('MAX_LR')}\\n\")\n            f.write(f\"MID_LR: {config.get('MID_LR')}\\n\")\n            f.write(f\"MIN_LR: {config.get('MIN_LR')}\\n\")\n            f.write(f\"RAMPUP_EPOCHS: {config.get('RAMPUP_EPOCHS')}\\n\")\n            f.write(f\"SUSTAIN_EPOCHS: {config.get('SUSTAIN_EPOCHS')}\\n\")\n        f.write(f\"DROPOUT_RATE: {config.get('DROPOUT_RATE')}\\n\")\n        f.write(f\"ACTIVATION_FN: {config.get('ACTIVATION_FN')}\\n\")\n        f.write(f\"FEATURES: {config.get('FEATURES')}\\n\")\n        f.write(f\"LABELS: {config.get('LABELS')}\\n\")\n        f.write(f\"PATCH_SHAPE: {config.get('PATCH_SHAPE')}\\n\")\n        f.write(f\"CALLBACK_PARAMETER: {config.get('CALLBACK_PARAMETER')}\\n\")\n        f.write(f\"MODEL_TYPE: {config.get('MODEL_TYPE')}\\n\")\n        f.write(f\"TRANSFORM_DATA: {config.get('TRANSFORM_DATA')}\\n\")\n        f.write(f\"MODEL_NAME: {config.get('MODEL_NAME')}.h5\\n\")\n        f.write(f\"MODEL_CHECKPOINT_NAME: {config.get('MODEL_CHECKPOINT_NAME')}.h5\\n\")\n    f.close()\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.save_plots","title":"<code>save_plots(self)</code>","text":"<p>Save plots and model visualization.</p> <p>Saves the model architecture plot, training history plot, and model object.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def save_plots(self) -&gt; None:\n    \"\"\"\n    Save plots and model visualization.\n\n    Saves the model architecture plot, training history plot, and model object.\n    \"\"\"\n    print(f\"Saving plots and model visualization at {self.config.MODEL_SAVE_DIR}...\")\n\n    Utils.plot_metrics([key.replace(\"val_\", \"\") for key in self.history.history.keys() if key.startswith(\"val_\")],\n                       self.history.history, len(self.history.epoch), self.config.MODEL_SAVE_DIR)\n\n    if self.config.USE_AI_PLATFORM:\n        keras.utils.plot_model(self._model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True, rankdir=\"TB\")\n        keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/wrapped_model.png\", show_shapes=True, rankdir=\"LR\") # rankdir='TB'\n    else:\n        keras.utils.plot_model(self.model, f\"{self.config.MODEL_SAVE_DIR}/model.png\", show_shapes=True, rankdir=\"TB\") # rankdir='TB'\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.serialize_model","title":"<code>serialize_model(self)</code>","text":"<p>Serialize and save the trained models.</p> <p>Saves the trained models in different formats: h5 and tf formats.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def serialize_model(self) -&gt; tf.keras.Model:\n    \"\"\"\n    Serialize and save the trained models.\n\n    Saves the trained models in different formats: h5 and tf formats.\n    \"\"\"\n    input_deserializer = DeSerializeInput(self.config.FEATURES)\n    output_deserializer = ReSerializeOutput()\n    serialized_inputs = {\n        b: tf.keras.Input(shape=[], dtype=\"string\", name=b) for b in self.config.FEATURES\n    }\n    updated_model_input = input_deserializer(serialized_inputs)\n    updated_model = self.model(updated_model_input)\n    updated_model = output_deserializer(updated_model, \"output\")\n    updated_model = tf.keras.Model(serialized_inputs, updated_model)\n    keras.utils.plot_model(updated_model, f\"{self.config.MODEL_SAVE_DIR}/serialized_model.png\", show_shapes=True, rankdir=\"LR\")\n    return updated_model\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.start_training","title":"<code>start_training(self)</code>","text":"<p>Start the training process.</p> <p>Trains the model using the provided configuration settings and callbacks.</p> Source code in <code>aces/model_trainer.py</code> <pre><code>def start_training(self) -&gt; None:\n    \"\"\"\n    Start the training process.\n\n    Trains the model using the provided configuration settings and callbacks.\n    \"\"\"\n    model_checkpoint = callbacks.ModelCheckpoint(\n        f\"{str(self.config.MODEL_SAVE_DIR)}/{self.config.MODEL_CHECKPOINT_NAME}\",\n        monitor=self.config.CALLBACK_PARAMETER,\n        save_best_only=True,\n        mode=\"auto\",\n        verbose=1,\n        save_weights_only=False,\n    )  # save best model\n\n    tensorboard = callbacks.TensorBoard(log_dir=str(self.config.MODEL_SAVE_DIR / \"logs\"), write_images=True)\n\n    def lr_scheduler(epoch):\n        if epoch &lt; self.config.RAMPUP_EPOCHS:\n            return self.config.MAX_LR\n        elif epoch &lt; self.config.RAMPUP_EPOCHS + self.config.SUSTAIN_EPOCHS:\n            return self.config.MID_LR\n        else:\n            return self.config.MIN_LR\n\n    model_callbacks = [model_checkpoint, tensorboard]\n\n    if self.config.USE_ADJUSTED_LR:\n        lr_callback = callbacks.LearningRateScheduler(lambda epoch: lr_scheduler(epoch), verbose=True)\n        model_callbacks.append(lr_callback)\n\n    if self.config.EARLY_STOPPING:\n        early_stopping = callbacks.EarlyStopping(\n            monitor=self.config.CALLBACK_PARAMETER,\n            patience=int(0.3 * self.config.EPOCHS),\n            verbose=1,\n            mode=\"auto\",\n            restore_best_weights=True,\n        )\n        model_callbacks.append(early_stopping)\n\n    self.model_callbacks = model_callbacks\n\n    self.history = self.model.fit(\n        x=self.TRAINING_DATASET,\n        epochs=self.config.EPOCHS,\n        steps_per_epoch=(self.config.TRAIN_SIZE // self.config.BATCH_SIZE),\n        validation_data=self.VALIDATION_DATASET,\n        validation_steps=self.config.VAL_SIZE,\n        callbacks=model_callbacks,\n    )\n\n    # either save the wrapped model or the original model\n    # named as \"trained-model\" to avoid confusion\n    # self.model.save(f\"{self.config.MODEL_SAVE_DIR}/trained-wrapped-model\")\n    self.model.save(f\"{self.config.MODEL_SAVE_DIR}/trained-model\")\n</code></pre>"},{"location":"model_trainer/#aces.model_trainer.ModelTrainer.train_model","title":"<code>train_model(self)</code>","text":"<p>Train the model using the provided configuration settings.</p> <p>This method performs the following steps:</p> <ol> <li> <p>Configures memory growth for TensorFlow.</p> </li> <li> <p>Creates TensorFlow datasets for training, testing, and validation.</p> </li> <li> <p>Builds and compiles the model.</p> </li> <li> <p>Prepares the output directory for saving models and results.</p> </li> <li> <p>Starts the training process.</p> </li> <li> <p>Evaluates and prints validation metrics.</p> </li> <li> <p>Saves training parameters, plots, and models.</p> </li> </ol> Source code in <code>aces/model_trainer.py</code> <pre><code>def train_model(self) -&gt; None:\n    \"\"\"\n    Train the model using the provided configuration settings.\n\n    This method performs the following steps:\n\n    1. Configures memory growth for TensorFlow.\n\n    2. Creates TensorFlow datasets for training, testing, and validation.\n\n    3. Builds and compiles the model.\n\n    4. Prepares the output directory for saving models and results.\n\n    5. Starts the training process.\n\n    6. Evaluates and prints validation metrics.\n\n    7. Saves training parameters, plots, and models.\n    \"\"\"\n    print(\"****************************************************************************\")\n    print(\"****************************** Clear Session... ****************************\")\n    keras.backend.clear_session()\n    print(\"****************************************************************************\")\n    print(f\"****************************** Configure memory growth... ************************\")\n    physical_devices = TFUtils.configure_memory_growth()\n    self.config.physical_devices = physical_devices\n    print(\"****************************************************************************\")\n    print(\"****************************** creating datasets... ************************\")\n    self.create_datasets(print_info=self.config.PRINT_INFO)\n\n    if self.config.USE_AI_PLATFORM:\n        print(\"****************************************************************************\")\n        print(\"******* building and compiling model for ai platform... ********************\")\n        self.build_and_compile_model_ai_platform()\n    else:\n        print(\"****************************************************************************\")\n        print(\"************************ building and compiling model... *******************\")\n        self.build_and_compile_model(print_model_summary=True)\n\n    print(\"****************************************************************************\")\n    print(\"************************ preparing output directory... *********************\")\n    self.prepare_output_dir()\n    print(\"****************************************************************************\")\n    print(\"****************************** training model... ***************************\")\n    self.start_training()\n\n    if self.config.USE_AI_PLATFORM:\n        print(self.model.summary())\n\n    print(\"****************************************************************************\")\n    print(\"****************************** evaluating model... *************************\")\n    self.evaluate_and_print_val()\n    print(\"****************************************************************************\")\n    print(\"****************************** saving parameters... ************************\")\n    ModelTrainer.save_parameters(**self.config.__dict__)\n    print(\"****************************************************************************\")\n    print(\"*************** saving model config and history object... ******************\")\n    self.save_history_object()\n    if self.config.USE_AI_PLATFORM:\n        ModelTrainer.save_model_config(self.config.MODEL_SAVE_DIR, **self._model.get_config())\n    else:\n        ModelTrainer.save_model_config(self.config.MODEL_SAVE_DIR, **self.model.get_config())\n    print(\"****************************************************************************\")\n    print(\"****************************** saving plots... *****************************\")\n    self.save_plots()\n    print(\"****************************************************************************\")\n    print(\"****************************** saving models... ****************************\")\n    self.save_models()\n    print(\"****************************************************************************\")\n</code></pre>"},{"location":"remote_sensing/","title":"remote_sensing module","text":""},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures","title":"<code> RemoteSensingFeatures        </code>","text":"<p>A class for generating remote sensing features using TensorFlow.</p> <p>This class provides static methods to compute various remote sensing indices and concatenate them into feature tensors.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>class RemoteSensingFeatures:\n    \"\"\"\n    A class for generating remote sensing features using TensorFlow.\n\n    This class provides static methods to compute various remote sensing indices and concatenate them into feature tensors.\n    \"\"\"\n    @staticmethod\n    def normalized_difference(c1: tf.Tensor, c2: tf.Tensor, name: str = \"nd\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the normalized difference index between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the normalized difference index.\n\n        \"\"\"\n        nd_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1])), name=name)([c1, c2])\n        nd_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f\"{name}_inf\")([c1, c2])\n        return tf.where(tf.math.is_finite(nd_f), nd_f, nd_inf, name=name)\n\n    @staticmethod\n    def evi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"evi\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the enhanced vegetation index (EVI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the EVI.\n\n        \"\"\"\n        _evi = keras.layers.Lambda(lambda x: 2.5 * ((x[0] - x[1]) / (x[0] + 6 * x[1] - 7.5 * x[2] + 1)), name=name)([c1, c2, c3])\n        return _evi\n\n    @staticmethod\n    def savi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"savi\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the SAVI.\n\n        \"\"\"\n        savi_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] + 0.5)) * 1.5, name=name)([c1, c2])\n        return savi_f\n\n    @staticmethod\n    def msavi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"msavi\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the MSAVI.\n\n        \"\"\"\n        msavi_f = keras.layers.Lambda(lambda x: (((2 * x[0] + 1) - tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - 8 * (x[0] - x[1]))) / 2), name=name)([c1, c2])\n        return msavi_f\n\n    @staticmethod\n    def mtvi2(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"mtvi2\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the MTVI2.\n\n        \"\"\"\n        mtvi2_f = keras.layers.Lambda(lambda x: (1.5 * (1.2 * (x[0] - x[2]) - 2.5 * (x[1] - x[2]))) / (tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - (6 * x[0] - 5 * tf.sqrt(x[1])) - 0.5)), name=name)([c1, c2, c3])\n        return mtvi2_f\n\n    @staticmethod\n    def vari(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"vari\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the visible atmospheric resistant index (VARI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the VARI.\n\n        \"\"\"\n        vari_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] - x[2])), name=name)([c1, c2, c3])\n        vari_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f\"{name}_inf\")([c1, c2, c3])\n        return tf.where(tf.math.is_finite(vari_f), vari_f, vari_inf, name=name)\n\n    @staticmethod\n    def tgi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"tgi\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the triangular greenness index (TGI) using three spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            c3: A TensorFlow tensor representing the third spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the TGI.\n\n        \"\"\"\n        tgi_f = keras.layers.Lambda(lambda x: ((120 * (x[1] - x[2])) - (190 * (x[1] - x[0]))) / 2, name=name)([c1, c2, c3])\n        return tgi_f\n\n    @staticmethod\n    def ratio(c1: tf.Tensor, c2: tf.Tensor, name: str = \"ratio\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the ratio between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the ratio between the spectral bands.\n\n        \"\"\"\n        ratio_f = keras.layers.Lambda(lambda x: x[0] / x[1], name=name)([c1, c2])\n        ratio_inf = keras.layers.Lambda(lambda x: x[0], name=f\"{name}_inf\")([c1, c2])\n        return tf.where(tf.math.is_finite(ratio_f), ratio_f, ratio_inf, name=name)\n\n    @staticmethod\n    def nvi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"nvi\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the normalized vegetation index (NVI) between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the NVI.\n\n        \"\"\"\n        nvi_f = keras.layers.Lambda(lambda x: x[0] / (x[0] + x[1]), name=name)([c1, c2])\n        nvi_inf = keras.layers.Lambda(lambda x: x[0], name=f\"{name}_inf\")([c1, c2])\n        return tf.where(tf.math.is_finite(nvi_f), nvi_f, nvi_inf, name=name)\n\n    @staticmethod\n    def diff_band(c1: tf.Tensor, c2: tf.Tensor, name: str = \"diff\") -&gt; tf.Tensor:\n        \"\"\"\n        Compute the difference between two spectral bands.\n\n        Args:\n            c1: A TensorFlow tensor representing the first spectral band.\n            c2: A TensorFlow tensor representing the second spectral band.\n            name: A string specifying the name for the operation.\n\n        Returns:\n            A TensorFlow tensor representing the difference between the spectral bands.\n\n        \"\"\"\n        diff = keras.layers.Lambda(lambda x: x[0] - x[1], name=name)([c1, c2])\n        return diff\n\n    @staticmethod\n    def concatenate_features_for_cnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Concatenate remote sensing features for Convolutional Neural Network (CNN) input.\n\n        Args:\n            input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n        Returns:\n            A TensorFlow tensor representing the concatenated features for CNN input.\n\n        \"\"\"\n        red_before = input_tensor[:, :, :, 0:1]\n        green_before = input_tensor[:, :, :, 1:2]\n        blue_before = input_tensor[:, :, :, 2:3]\n        nir_before = input_tensor[:, :, :, 3:4]\n        red_during = input_tensor[:, :, :, 4:5]\n        green_during = input_tensor[:, :, :, 5:6]\n        blue_during = input_tensor[:, :, :, 6:7]\n        nir_during = input_tensor[:, :, :, 7:8]\n\n        ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\")\n        ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\")\n        evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\")\n        evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\")\n        ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\")\n        ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\")\n        savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\")\n        savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\")\n        # msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\")\n        # msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\")\n        mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\")\n        mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\")\n        # vari is not used because the computation gave some nan values\n        # vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\")\n        # vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\")\n        # tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\")\n        # tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\")\n\n        red_diff1 = RemoteSensingFeatures.diff_band(red_before, red_during, name=\"diff1\")\n        green_diff1 = RemoteSensingFeatures.diff_band(green_before, green_during, name=\"diff2\")\n        blue_diff1 = RemoteSensingFeatures.diff_band(blue_before, blue_during, name=\"diff3\")\n        nir_diff1 = RemoteSensingFeatures.diff_band(nir_before, nir_during, name=\"diff4\")\n\n        return keras.layers.concatenate(\n            [input_tensor, ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n             mtvi2_before, mtvi2_during,\n             red_diff1, green_diff1, blue_diff1, nir_diff1],\n            name=\"input_features\"\n        )\n\n    @staticmethod\n    def concatenate_features_for_dnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n        Args:\n            input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n        Returns:\n            A TensorFlow tensor representing the concatenated features for DNN input.\n\n        \"\"\"\n        red_before = input_tensor[:, :, :, 0:1]\n        green_before = input_tensor[:, :, :, 1:2]\n        blue_before = input_tensor[:, :, :, 2:3]\n        nir_before = input_tensor[:, :, :, 3:4]\n        red_during = input_tensor[:, :, :, 4:5]\n        green_during = input_tensor[:, :, :, 5:6]\n        blue_during = input_tensor[:, :, :, 6:7]\n        nir_during = input_tensor[:, :, :, 7:8]\n\n        ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\")\n        ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\")\n        evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\")\n        evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\")\n        ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\")\n        ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\")\n        savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\")\n        savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\")\n        msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\")\n        msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\")\n        mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\")\n        mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\")\n        vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\")\n        vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\")\n        tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\")\n        tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\")\n\n        return keras.layers.concatenate(\n            [input_tensor, ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n             msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during],\n            name=\"input_features\"\n        )\n\n    @staticmethod\n    def derive_features_for_dnn(features_dict: dict, added_features: list = []) -&gt; dict:\n        \"\"\"\n        Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n        Args:\n            input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n        Returns:\n            A TensorFlow tensor representing the concatenated features for DNN input.\n\n        \"\"\"\n        red_before = features_dict[\"red_before\"]\n        green_before = features_dict[\"green_before\"]\n        blue_before = features_dict[\"blue_before\"]\n        nir_before = features_dict[\"nir_before\"]\n        red_during = features_dict[\"red_during\"]\n        green_during = features_dict[\"green_during\"]\n        blue_during = features_dict[\"blue_during\"]\n        nir_during = features_dict[\"nir_during\"]\n\n        feature_col = {\n            \"ndvi_before\": RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\"),\n            \"ndvi_during\": RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\"),\n            \"evi_before\": RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\"),\n            \"evi_during\": RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\"),\n            \"ndwi_before\": RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\"),\n            \"ndwi_during\": RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\"),\n            \"savi_before\": RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\"),\n            \"savi_during\": RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\"),\n            \"msavi_before\": RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\"),\n            \"msavi_during\": RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\"),\n            \"mtvi2_before\": RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\"),\n            \"mtvi2_during\": RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\"),\n            \"vari_before\": RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\"),\n            \"vari_during\": RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\"),\n            \"tgi_before\": RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\"),\n            \"tgi_during\": RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\"),\n        }\n\n        for feature in added_features:\n            features_dict[feature] = feature_col[feature]\n\n        return features_dict\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.concatenate_features_for_cnn","title":"<code>concatenate_features_for_cnn(input_tensor)</code>  <code>staticmethod</code>","text":"<p>Concatenate remote sensing features for Convolutional Neural Network (CNN) input.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>A TensorFlow tensor representing the input remote sensing data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the concatenated features for CNN input.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef concatenate_features_for_cnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Concatenate remote sensing features for Convolutional Neural Network (CNN) input.\n\n    Args:\n        input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n    Returns:\n        A TensorFlow tensor representing the concatenated features for CNN input.\n\n    \"\"\"\n    red_before = input_tensor[:, :, :, 0:1]\n    green_before = input_tensor[:, :, :, 1:2]\n    blue_before = input_tensor[:, :, :, 2:3]\n    nir_before = input_tensor[:, :, :, 3:4]\n    red_during = input_tensor[:, :, :, 4:5]\n    green_during = input_tensor[:, :, :, 5:6]\n    blue_during = input_tensor[:, :, :, 6:7]\n    nir_during = input_tensor[:, :, :, 7:8]\n\n    ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\")\n    ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\")\n    evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\")\n    evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\")\n    ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\")\n    ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\")\n    savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\")\n    savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\")\n    # msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\")\n    # msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\")\n    mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\")\n    mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\")\n    # vari is not used because the computation gave some nan values\n    # vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\")\n    # vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\")\n    # tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\")\n    # tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\")\n\n    red_diff1 = RemoteSensingFeatures.diff_band(red_before, red_during, name=\"diff1\")\n    green_diff1 = RemoteSensingFeatures.diff_band(green_before, green_during, name=\"diff2\")\n    blue_diff1 = RemoteSensingFeatures.diff_band(blue_before, blue_during, name=\"diff3\")\n    nir_diff1 = RemoteSensingFeatures.diff_band(nir_before, nir_during, name=\"diff4\")\n\n    return keras.layers.concatenate(\n        [input_tensor, ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n         mtvi2_before, mtvi2_during,\n         red_diff1, green_diff1, blue_diff1, nir_diff1],\n        name=\"input_features\"\n    )\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.concatenate_features_for_dnn","title":"<code>concatenate_features_for_dnn(input_tensor)</code>  <code>staticmethod</code>","text":"<p>Concatenate remote sensing features for Deep Neural Network (DNN) input.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>A TensorFlow tensor representing the input remote sensing data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the concatenated features for DNN input.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef concatenate_features_for_dnn(input_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n    Args:\n        input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n    Returns:\n        A TensorFlow tensor representing the concatenated features for DNN input.\n\n    \"\"\"\n    red_before = input_tensor[:, :, :, 0:1]\n    green_before = input_tensor[:, :, :, 1:2]\n    blue_before = input_tensor[:, :, :, 2:3]\n    nir_before = input_tensor[:, :, :, 3:4]\n    red_during = input_tensor[:, :, :, 4:5]\n    green_during = input_tensor[:, :, :, 5:6]\n    blue_during = input_tensor[:, :, :, 6:7]\n    nir_during = input_tensor[:, :, :, 7:8]\n\n    ndvi_before = RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\")\n    ndvi_during = RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\")\n    evi_before = RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\")\n    evi_during = RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\")\n    ndwi_before = RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\")\n    ndwi_during = RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\")\n    savi_before = RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\")\n    savi_during = RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\")\n    msavi_before = RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\")\n    msavi_during = RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\")\n    mtvi2_before = RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\")\n    mtvi2_during = RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\")\n    vari_before = RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\")\n    vari_during = RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\")\n    tgi_before = RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\")\n    tgi_during = RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\")\n\n    return keras.layers.concatenate(\n        [input_tensor, ndvi_before, ndvi_during, evi_before, evi_during, ndwi_before, ndwi_during, savi_before, savi_during,\n         msavi_before, msavi_during, mtvi2_before, mtvi2_during, vari_before, vari_during, tgi_before, tgi_during],\n        name=\"input_features\"\n    )\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.derive_features_for_dnn","title":"<code>derive_features_for_dnn(features_dict, added_features=[])</code>  <code>staticmethod</code>","text":"<p>Concatenate remote sensing features for Deep Neural Network (DNN) input.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <p>A TensorFlow tensor representing the input remote sensing data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A TensorFlow tensor representing the concatenated features for DNN input.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef derive_features_for_dnn(features_dict: dict, added_features: list = []) -&gt; dict:\n    \"\"\"\n    Concatenate remote sensing features for Deep Neural Network (DNN) input.\n\n    Args:\n        input_tensor: A TensorFlow tensor representing the input remote sensing data.\n\n    Returns:\n        A TensorFlow tensor representing the concatenated features for DNN input.\n\n    \"\"\"\n    red_before = features_dict[\"red_before\"]\n    green_before = features_dict[\"green_before\"]\n    blue_before = features_dict[\"blue_before\"]\n    nir_before = features_dict[\"nir_before\"]\n    red_during = features_dict[\"red_during\"]\n    green_during = features_dict[\"green_during\"]\n    blue_during = features_dict[\"blue_during\"]\n    nir_during = features_dict[\"nir_during\"]\n\n    feature_col = {\n        \"ndvi_before\": RemoteSensingFeatures.normalized_difference(nir_before, red_before, name=\"ndvi_before\"),\n        \"ndvi_during\": RemoteSensingFeatures.normalized_difference(nir_during, red_during, name=\"ndvi_during\"),\n        \"evi_before\": RemoteSensingFeatures.evi(nir_before, red_before, blue_before, name=\"evi_before\"),\n        \"evi_during\": RemoteSensingFeatures.evi(nir_during, red_during, blue_during, name=\"evi_during\"),\n        \"ndwi_before\": RemoteSensingFeatures.normalized_difference(green_before, nir_before, name=\"ndwi_before\"),\n        \"ndwi_during\": RemoteSensingFeatures.normalized_difference(green_during, nir_during, name=\"ndwi_during\"),\n        \"savi_before\": RemoteSensingFeatures.savi(nir_before, red_before, name=\"savi_before\"),\n        \"savi_during\": RemoteSensingFeatures.savi(nir_during, red_during, name=\"savi_during\"),\n        \"msavi_before\": RemoteSensingFeatures.msavi(nir_before, red_before, name=\"msavi_before\"),\n        \"msavi_during\": RemoteSensingFeatures.msavi(nir_during, red_during, name=\"msavi_during\"),\n        \"mtvi2_before\": RemoteSensingFeatures.mtvi2(nir_before, red_before, green_before, name=\"mtvi2_before\"),\n        \"mtvi2_during\": RemoteSensingFeatures.mtvi2(nir_during, red_during, green_during, name=\"mtvi2_during\"),\n        \"vari_before\": RemoteSensingFeatures.vari(green_before, red_before, blue_before, name=\"vari_before\"),\n        \"vari_during\": RemoteSensingFeatures.vari(green_during, red_during, blue_during, name=\"vari_during\"),\n        \"tgi_before\": RemoteSensingFeatures.tgi(green_before, red_before, blue_before, name=\"tgi_before\"),\n        \"tgi_during\": RemoteSensingFeatures.tgi(green_during, red_during, blue_during, name=\"tgi_during\"),\n    }\n\n    for feature in added_features:\n        features_dict[feature] = feature_col[feature]\n\n    return features_dict\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.diff_band","title":"<code>diff_band(c1, c2, name='diff')</code>  <code>staticmethod</code>","text":"<p>Compute the difference between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'diff'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the difference between the spectral bands.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef diff_band(c1: tf.Tensor, c2: tf.Tensor, name: str = \"diff\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the difference between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the difference between the spectral bands.\n\n    \"\"\"\n    diff = keras.layers.Lambda(lambda x: x[0] - x[1], name=name)([c1, c2])\n    return diff\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.evi","title":"<code>evi(c1, c2, c3, name='evi')</code>  <code>staticmethod</code>","text":"<p>Compute the enhanced vegetation index (EVI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'evi'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the EVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef evi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"evi\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the enhanced vegetation index (EVI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the EVI.\n\n    \"\"\"\n    _evi = keras.layers.Lambda(lambda x: 2.5 * ((x[0] - x[1]) / (x[0] + 6 * x[1] - 7.5 * x[2] + 1)), name=name)([c1, c2, c3])\n    return _evi\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.msavi","title":"<code>msavi(c1, c2, name='msavi')</code>  <code>staticmethod</code>","text":"<p>Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'msavi'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the MSAVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef msavi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"msavi\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the modified soil-adjusted vegetation index (MSAVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the MSAVI.\n\n    \"\"\"\n    msavi_f = keras.layers.Lambda(lambda x: (((2 * x[0] + 1) - tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - 8 * (x[0] - x[1]))) / 2), name=name)([c1, c2])\n    return msavi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.mtvi2","title":"<code>mtvi2(c1, c2, c3, name='mtvi2')</code>  <code>staticmethod</code>","text":"<p>Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'mtvi2'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the MTVI2.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef mtvi2(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"mtvi2\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the modified transformed vegetation index 2 (MTVI2) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the MTVI2.\n\n    \"\"\"\n    mtvi2_f = keras.layers.Lambda(lambda x: (1.5 * (1.2 * (x[0] - x[2]) - 2.5 * (x[1] - x[2]))) / (tf.sqrt(((2 * x[0] + 1) * (2 * x[0] + 1)) - (6 * x[0] - 5 * tf.sqrt(x[1])) - 0.5)), name=name)([c1, c2, c3])\n    return mtvi2_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.normalized_difference","title":"<code>normalized_difference(c1, c2, name='nd')</code>  <code>staticmethod</code>","text":"<p>Compute the normalized difference index between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'nd'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the normalized difference index.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef normalized_difference(c1: tf.Tensor, c2: tf.Tensor, name: str = \"nd\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the normalized difference index between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the normalized difference index.\n\n    \"\"\"\n    nd_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1])), name=name)([c1, c2])\n    nd_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f\"{name}_inf\")([c1, c2])\n    return tf.where(tf.math.is_finite(nd_f), nd_f, nd_inf, name=name)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.nvi","title":"<code>nvi(c1, c2, name='nvi')</code>  <code>staticmethod</code>","text":"<p>Compute the normalized vegetation index (NVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'nvi'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the NVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef nvi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"nvi\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the normalized vegetation index (NVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the NVI.\n\n    \"\"\"\n    nvi_f = keras.layers.Lambda(lambda x: x[0] / (x[0] + x[1]), name=name)([c1, c2])\n    nvi_inf = keras.layers.Lambda(lambda x: x[0], name=f\"{name}_inf\")([c1, c2])\n    return tf.where(tf.math.is_finite(nvi_f), nvi_f, nvi_inf, name=name)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.ratio","title":"<code>ratio(c1, c2, name='ratio')</code>  <code>staticmethod</code>","text":"<p>Compute the ratio between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'ratio'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the ratio between the spectral bands.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef ratio(c1: tf.Tensor, c2: tf.Tensor, name: str = \"ratio\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the ratio between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the ratio between the spectral bands.\n\n    \"\"\"\n    ratio_f = keras.layers.Lambda(lambda x: x[0] / x[1], name=name)([c1, c2])\n    ratio_inf = keras.layers.Lambda(lambda x: x[0], name=f\"{name}_inf\")([c1, c2])\n    return tf.where(tf.math.is_finite(ratio_f), ratio_f, ratio_inf, name=name)\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.savi","title":"<code>savi(c1, c2, name='savi')</code>  <code>staticmethod</code>","text":"<p>Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'savi'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the SAVI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef savi(c1: tf.Tensor, c2: tf.Tensor, name: str = \"savi\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the soil-adjusted vegetation index (SAVI) between two spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the SAVI.\n\n    \"\"\"\n    savi_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] + 0.5)) * 1.5, name=name)([c1, c2])\n    return savi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.tgi","title":"<code>tgi(c1, c2, c3, name='tgi')</code>  <code>staticmethod</code>","text":"<p>Compute the triangular greenness index (TGI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'tgi'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the TGI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef tgi(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"tgi\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the triangular greenness index (TGI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the TGI.\n\n    \"\"\"\n    tgi_f = keras.layers.Lambda(lambda x: ((120 * (x[1] - x[2])) - (190 * (x[1] - x[0]))) / 2, name=name)([c1, c2, c3])\n    return tgi_f\n</code></pre>"},{"location":"remote_sensing/#aces.remote_sensing.RemoteSensingFeatures.vari","title":"<code>vari(c1, c2, c3, name='vari')</code>  <code>staticmethod</code>","text":"<p>Compute the visible atmospheric resistant index (VARI) using three spectral bands.</p> <p>Parameters:</p> Name Type Description Default <code>c1</code> <code>Tensor</code> <p>A TensorFlow tensor representing the first spectral band.</p> required <code>c2</code> <code>Tensor</code> <p>A TensorFlow tensor representing the second spectral band.</p> required <code>c3</code> <code>Tensor</code> <p>A TensorFlow tensor representing the third spectral band.</p> required <code>name</code> <code>str</code> <p>A string specifying the name for the operation.</p> <code>'vari'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A TensorFlow tensor representing the VARI.</p> Source code in <code>aces/remote_sensing.py</code> <pre><code>@staticmethod\ndef vari(c1: tf.Tensor, c2: tf.Tensor, c3: tf.Tensor, name: str = \"vari\") -&gt; tf.Tensor:\n    \"\"\"\n    Compute the visible atmospheric resistant index (VARI) using three spectral bands.\n\n    Args:\n        c1: A TensorFlow tensor representing the first spectral band.\n        c2: A TensorFlow tensor representing the second spectral band.\n        c3: A TensorFlow tensor representing the third spectral band.\n        name: A string specifying the name for the operation.\n\n    Returns:\n        A TensorFlow tensor representing the VARI.\n\n    \"\"\"\n    vari_f = keras.layers.Lambda(lambda x: ((x[0] - x[1]) / (x[0] + x[1] - x[2])), name=name)([c1, c2, c3])\n    vari_inf = keras.layers.Lambda(lambda x: (x[0] - x[1]), name=f\"{name}_inf\")([c1, c2, c3])\n    return tf.where(tf.math.is_finite(vari_f), vari_f, vari_inf, name=name)\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Note: To make running things easier, we also have prepared <code>notebook</code>. You can find the detailed example on running things using this main notebook. This and several other notebooks are available on the <code>notebook</code> folder implementing DL methods. This notebook runs on colab, and most of the installation etc are taken care of.</p> <p>To use servir-aces, you need training datasets. If you want to use/produce produce your own datasets, you can follow this notebook which goes in detail on how to produce the datasets for training, testing, and validation.</p> <p>For quickly getting started to test the library, we have already prepared and exported the training datasets. They can be found at the google cloud storage and we will use <code>gsutil</code> to get the dataset in our workspace. Learn how you can install <code>gsutil</code> here. Let's start by downloading these datasets in our workspace. Navigate to your work folder and run.</p> <pre><code>gsutil -m cp -r gs://dl-book/chapter-1 .\n</code></pre> <p>Once you run this, you can find <code>unet_256x256_planet_wo_indices</code> and <code>dnn_planet_wo_indices</code> which are the prepared training data for running the U-Net and DNN model respectively. Each of these folder have sub-folder called <code>training</code>, <code>testing</code>, and <code>validation</code> which are the training, testing and validation dataset.</p> <p>All the settings are provided through the <code>.env</code> file. A sample example of the <code>.env</code> file can be found here. You can copy this and rename to <code>config.env</code> at your directory. There are several parameters that needs to be changed. Let's look at some of them here. Note these directory should exists before you can run them.</p> <pre><code>BASEDIR = \"/content/\"\nOUTPUT_DIR = \"/content/output\"\n</code></pre> <p>We will start by training a U-Net model using the <code>unet_256x256_planet_wo_indices</code> dataset that we just downloaded. Let's go ahead and change our DATADIR in the <code>config.env</code> file as below.</p> <pre><code>DATADIR = \"/content/datasets/unet_256x256_planet_wo_indices\"\n</code></pre> <p>These datasets have RGBN from Planetscope mosiac. For this example, we will use the library to map the rice fields, and we use growing season and pre-growing season information. Thus, we have 8 optical bands, namely <code>red_before</code>, <code>green_before</code>, <code>blue_before</code>, <code>nir_before</code>, <code>red_during</code>, <code>green_during</code>, <code>blue_during</code>, and  <code>nir_during</code>. This information is provided via the <code>FEATURES</code> variable which already defaults to what we want for this example; see here.</p> <p>In adidition, you can use <code>USE_ELEVATION</code> and <code>USE_S1</code> config to include the topographic and radar information if your dataset has any (or just put them in the <code>FEATURES</code> above). Since this datasets don't have topographic and radar features, so we won't be settting these config values. Similarly, these datasets are tiled to 256x256 pixels, so let's also change that.</p> <pre><code>USE_ELEVATION = False\nUSE_S1 = False\n\nPATCH_SHAPE = (256, 256)\n</code></pre> <p>You can then change the <code>MODEL_TYPE</code> which you are running. The default is \"unet\". You can change to \"dnn\" if you are using that model.</p> <p>Note current version does not expose all the model intracacies through the environment file but future version may include those depending on the need.</p> <pre><code>MODEL_TYPE = \"unet\"\n</code></pre> <p>Next define the <code>FEATURES</code> and the <code>LABELS</code> variables. This dataset was prepared for the rice mapping application that uses before and during growing season information. So for this dataset, here are the example <code>FEATURES</code>. Note the <code>FEATURES</code> should be in the same format as shown below</p> <pre><code>FEATURES = \"red_before\ngreen_before\nblue_before\nnir_before\nred_during\ngreen_during\nblue_during\nnir_during\"\n</code></pre> <p>Similarly, since the dataset has a single label, it is going to be</p> <pre><code>LABELS = [\"class\"]\n</code></pre> <p>Next, we need to calculate the size of the training, testing and validation dataset. For this, we know our size before hand. But <code>aces</code> also provides handful of functions that we can use to calculate this. See this notebook to learn more about how to do it. We will also change the <code>BATCH_SIZE</code> to 32; if you have larger memory available, you can increase the <code>BATCH_SIZE</code>. You can run for longer <code>EPOCHS</code> by changing the <code>EPOCHS</code> paramter; we will keep it to 30 for now.</p> <pre><code># Sizes of the training and evaluation datasets.\nTRAIN_SIZE = 8531\nTEST_SIZE = 1222\nVAL_SIZE = 2404\nBATCH_SIZE = 32\nEPOCHS = 30\n</code></pre> <p>Finally, you can change the <code>MODEL_DIR_NAME</code>. This is the directory name where you want to save the output of the training and other relevant files. This is used to construct the <code>MODEL_DIR</code>, which is constructed as <code>OUTPUT_DIR + MODEL_DIR_NAME</code>.</p> <pre><code>MODEL_DIR_NAME = \"unet_v1\"\n</code></pre> <p>These settings should be good enough to get started. To view the complete settings and what they meant, you can view them here.</p> <p>To use servir-aces in a project.</p> <pre><code>from aces import Config, ModelTrainer\n\nif __name__ == \"__main__\":\n    config_file = \"config.env\"\n    config = Config(config_file)\n</code></pre> <p>Most of the config in the <code>config.env</code> is now available via the config instance. Let's check few of them here.</p> <pre><code>print(config.TRAINING_DIR, config.OUTPUT_DIR, config.BATCH_SIZE, config.TRAIN_SIZE)\n</code></pre> <p>Next, let's make an instance of the <code>ModelTrainer</code> object. The <code>ModelTrainer</code> class provides various tools for training, buidling, compiling, and running specified deep learning models. (continue from above)</p> <pre><code>    trainer = ModelTrainer(config, seed=42)\n</code></pre> <p><code>ModelTrainer</code> class provides various functionality. We will use <code>train_model</code> function that helps to train the model using the provided configuration settings.</p> <p>This method performs the following steps: - Configures memory growth for TensorFlow. - Creates TensorFlow datasets for training, testing, and validation. - Builds and compiles the model. - Prepares the output directory for saving models and results. - Starts the training process. - Evaluates and prints validation metrics. - Saves training parameters, plots, and models.</p> <p>continue from above</p> <pre><code>    trainer.train_model()\n</code></pre> <p>Note: this takes a while to run and needs access to the GPU to run it faster. If you don't have access to GPU or don't want to wait while its running, we provide you with the trained model via the Google Cloud Storage. After you get the data via <code>gsutil</code> above, you will see a folder called <code>models</code> inside it. For the U-Net, you can download the <code>unet_v1</code> folder and put it inside the <code>MODEL_DIR</code>, and for DNN, you can download the <code>dnn_v1</code> folder and place it inside the <code>MODEL_DIR</code>.</p>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#aces.utils.TFUtils","title":"<code> TFUtils        </code>","text":"Source code in <code>aces/utils.py</code> <pre><code>class TFUtils:\n    @staticmethod\n    def beam_serialize(patch: np.ndarray) -&gt; bytes:\n        features = {\n            name: tf.train.Feature(\n                float_list=tf.train.FloatList(value=patch[name].flatten())\n            )\n            for name in patch.dtype.names\n        }\n        example = tf.train.Example(features=tf.train.Features(feature=features))\n        return example.SerializeToString()\n\n    @staticmethod\n    def configure_memory_growth() -&gt; List:\n        \"\"\"\n        Configure TensorFlow to allocate GPU memory dynamically.\n\n        If GPUs are found, this method enables memory growth for each GPU.\n        \"\"\"\n        physical_devices = tf.config.list_physical_devices(\"GPU\")\n        # self.config.physical_devices = physical_devices\n        if len(physical_devices):\n            print(f\" &gt; Found {len(physical_devices)} GPUs\")\n            try:\n                for device in physical_devices:\n                    tf.config.experimental.set_memory_growth(device, True)\n                return physical_devices\n            except Exception as err:\n                print(err)\n        else:\n            print(\" &gt; No GPUs found\")\n            return []\n</code></pre>"},{"location":"utils/#aces.utils.TFUtils.configure_memory_growth","title":"<code>configure_memory_growth()</code>  <code>staticmethod</code>","text":"<p>Configure TensorFlow to allocate GPU memory dynamically.</p> <p>If GPUs are found, this method enables memory growth for each GPU.</p> Source code in <code>aces/utils.py</code> <pre><code>@staticmethod\ndef configure_memory_growth() -&gt; List:\n    \"\"\"\n    Configure TensorFlow to allocate GPU memory dynamically.\n\n    If GPUs are found, this method enables memory growth for each GPU.\n    \"\"\"\n    physical_devices = tf.config.list_physical_devices(\"GPU\")\n    # self.config.physical_devices = physical_devices\n    if len(physical_devices):\n        print(f\" &gt; Found {len(physical_devices)} GPUs\")\n        try:\n            for device in physical_devices:\n                tf.config.experimental.set_memory_growth(device, True)\n            return physical_devices\n        except Exception as err:\n            print(err)\n    else:\n        print(\" &gt; No GPUs found\")\n        return []\n</code></pre>"},{"location":"utils/#aces.utils.Utils","title":"<code> Utils        </code>","text":"<p>This class provides utility functions for plotting, splitting data.</p> Source code in <code>aces/utils.py</code> <pre><code>class Utils:\n    \"\"\"\n    Utils: Utility Functions for ACES\n\n    This class provides utility functions for plotting, splitting data.\n    \"\"\"\n    @staticmethod\n    def split_dataset(element, num_partitions: int, validation_ratio: float = 0.2, test_ratio: float = 0.2) -&gt; int:\n        import random\n        weights = [1 - validation_ratio - test_ratio, validation_ratio, test_ratio]\n        return random.choices([0, 1, 2], weights)[0]\n\n    @staticmethod\n    def plot_metrics(metrics, history, epoch, model_save_dir):\n        \"\"\"\n        Plot the training and validation metrics over epochs.\n\n        Args:\n            metrics: List of metrics to plot.\n            history: Training history containing metric values.\n            epoch: Number of epochs.\n            model_save_dir: Directory to save the plot.\n\n        Returns:\n            None.\n        \"\"\"\n        fig, ax = plt.subplots(nrows=len(metrics), sharex=True, figsize=(15, len(metrics) * 6))\n        colors = [\"#1f77b4\", \"#ff7f0e\", \"red\", \"green\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n        for i, metric in enumerate(metrics):\n            try:\n                ax[i].plot(history[metric], color=colors[i], label=f\"Training {metric.upper()}\")\n                ax[i].plot(history[f\"val_{metric}\"], linestyle=\":\", marker=\"o\", markersize=3, color=colors[i], label=f\"Validation {metric.upper()}\")\n                ax[i].set_ylabel(metric.upper())\n                ax[i].legend()\n            except Exception as e:\n                print(f\"Exception: {e}\")\n                print(f\"Skipping {metric}.\")\n                continue\n\n        ax[i].set_xticks(range(1, epoch + 1, 4))\n        ax[i].set_xticklabels(range(1, epoch + 1, 4))\n        ax[i].set_xlabel(\"Epoch\")\n        fig.savefig(f\"{model_save_dir}/training.png\", dpi=1000)\n\n    @staticmethod\n    def filter_good_patches(patch):\n        \"\"\"\n        Filter patches to remove those with NaN or infinite values.\n\n        Parameters:\n        patch (np.ndarray): The patch to filter.\n\n        Returns:\n        bool: True if the patch has no NaN or infinite values, False otherwise.\n        \"\"\"\n        # the getdownload url has field names so we\"re using view here\n        has_nan = np.isnan(np.sum(patch.view(np.float32)))\n        has_inf = np.isinf(np.sum(patch.view(np.float32)))\n        if has_nan or has_inf:\n            return False\n        return True\n\n    @staticmethod\n    def convert_camel_to_snake(strings):\n        converted_strings = []\n        for string in strings:\n            converted_string = re.sub(r\"(?&lt;!^)(?=[A-Z])\", \"_\", string).lower()\n            converted_strings.append(converted_string)\n        return converted_strings\n\n    @staticmethod\n    def parse_params(feature_name):\n        # Fetch the feature_name string from the environment\n        params = os.getenv(feature_name)\n\n        # Normalize the string by replacing newlines with commas and stripping unwanted spaces\n        params = params.replace(\"\\n\", \",\").replace(\" \", \"\")\n\n        # Split the string into a list by commas\n        params_list = params.split(\",\")\n\n        # Optionally, you can remove any empty strings that may occur in the list\n        params_list = [feature for feature in params_list if feature]\n\n        return params_list\n</code></pre>"},{"location":"utils/#aces.utils.Utils.filter_good_patches","title":"<code>filter_good_patches(patch)</code>  <code>staticmethod</code>","text":"<p>Filter patches to remove those with NaN or infinite values.</p> <p>patch (np.ndarray): The patch to filter.</p> Source code in <code>aces/utils.py</code> <pre><code>@staticmethod\ndef filter_good_patches(patch):\n    \"\"\"\n    Filter patches to remove those with NaN or infinite values.\n\n    Parameters:\n    patch (np.ndarray): The patch to filter.\n\n    Returns:\n    bool: True if the patch has no NaN or infinite values, False otherwise.\n    \"\"\"\n    # the getdownload url has field names so we\"re using view here\n    has_nan = np.isnan(np.sum(patch.view(np.float32)))\n    has_inf = np.isinf(np.sum(patch.view(np.float32)))\n    if has_nan or has_inf:\n        return False\n    return True\n</code></pre>"},{"location":"utils/#aces.utils.Utils.plot_metrics","title":"<code>plot_metrics(metrics, history, epoch, model_save_dir)</code>  <code>staticmethod</code>","text":"<p>Plot the training and validation metrics over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>List of metrics to plot.</p> required <code>history</code> <p>Training history containing metric values.</p> required <code>epoch</code> <p>Number of epochs.</p> required <code>model_save_dir</code> <p>Directory to save the plot.</p> required <p>Returns:</p> Type Description <p>None.</p> Source code in <code>aces/utils.py</code> <pre><code>@staticmethod\ndef plot_metrics(metrics, history, epoch, model_save_dir):\n    \"\"\"\n    Plot the training and validation metrics over epochs.\n\n    Args:\n        metrics: List of metrics to plot.\n        history: Training history containing metric values.\n        epoch: Number of epochs.\n        model_save_dir: Directory to save the plot.\n\n    Returns:\n        None.\n    \"\"\"\n    fig, ax = plt.subplots(nrows=len(metrics), sharex=True, figsize=(15, len(metrics) * 6))\n    colors = [\"#1f77b4\", \"#ff7f0e\", \"red\", \"green\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n    for i, metric in enumerate(metrics):\n        try:\n            ax[i].plot(history[metric], color=colors[i], label=f\"Training {metric.upper()}\")\n            ax[i].plot(history[f\"val_{metric}\"], linestyle=\":\", marker=\"o\", markersize=3, color=colors[i], label=f\"Validation {metric.upper()}\")\n            ax[i].set_ylabel(metric.upper())\n            ax[i].legend()\n        except Exception as e:\n            print(f\"Exception: {e}\")\n            print(f\"Skipping {metric}.\")\n            continue\n\n    ax[i].set_xticks(range(1, epoch + 1, 4))\n    ax[i].set_xticklabels(range(1, epoch + 1, 4))\n    ax[i].set_xlabel(\"Epoch\")\n    fig.savefig(f\"{model_save_dir}/training.png\", dpi=1000)\n</code></pre>"}]}