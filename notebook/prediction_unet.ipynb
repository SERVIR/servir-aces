{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 NASA\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Rice mapping in Bhutan with U-Net using high resolution satellite imagery\n",
        "\n",
        "### This notebook shows an example of using the TFRecord images for prediction using saved U-Net Model.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/SERVIR/servir-aces/blob/main/notebooks/prediction_unet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/SERVIR/servir-aces/blob/main/notebooks/prediction_unet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "</table>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zk3R20LkKet"
      },
      "source": [
        "This notebook is also available in this github repo: https://github.com/SERVIR/servir-aces. Navigate to the `notebooks` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup environment"
      ],
      "metadata": {
        "id": "EuQkhSxRmiIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install servir-aces"
      ],
      "metadata": {
        "id": "FuDztFucmhpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30878f43-0be2-4981-d9af-32e1343115e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting servir-aces\n",
            "  Downloading servir_aces-0.0.14-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from servir-aces) (1.25.2)\n",
            "Requirement already satisfied: tensorflow>=2.9.3 in /usr/local/lib/python3.10/dist-packages (from servir-aces) (2.15.0)\n",
            "Requirement already satisfied: earthengine-api in /usr/local/lib/python3.10/dist-packages (from servir-aces) (0.1.399)\n",
            "Collecting python-dotenv>=1.0.0 (from servir-aces)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from servir-aces) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.8.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (0.1.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (0.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.9.3->servir-aces) (0.43.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->servir-aces) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->servir-aces) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (4.9)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->earthengine-api->servir-aces) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->earthengine-api->servir-aces) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->earthengine-api->servir-aces) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->earthengine-api->servir-aces) (2024.2.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->earthengine-api->servir-aces) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->earthengine-api->servir-aces) (2.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api->servir-aces) (1.63.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->earthengine-api->servir-aces) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api->servir-aces) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.2.2)\n",
            "Installing collected packages: python-dotenv, servir-aces\n",
            "Successfully installed python-dotenv-1.0.1 servir-aces-0.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SERVIR/servir-aces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PvoP9Sgqz4e",
        "outputId": "b3e310a7-7943-4904-c89d-9d8de4293eaf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'servir-aces'...\n",
            "remote: Enumerating objects: 731, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 731 (delta 55), reused 40 (delta 40), pack-reused 641\u001b[K\n",
            "Receiving objects: 100% (731/731), 3.35 MiB | 10.63 MiB/s, done.\n",
            "Resolving deltas: 100% (478/478), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the repo is downloaded. We will create an environment file file to place point to our training data and customize parameters for the model. To do this, we make a copy of the `.env.example` file provided.\n",
        "\n",
        "Under the hood, all the configuration provided via the environment file are parsed as a config object and can be accessed programatically.\n",
        "\n",
        "Note current version does not expose all the model intracacies through the environment file but future version may include those depending on the need."
      ],
      "metadata": {
        "id": "RukLD6eeq7Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp servir-aces/.env.example servir-aces/config.env"
      ],
      "metadata": {
        "id": "jpnfjXRYsYMk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgk98jsYF5xU",
        "outputId": "82828441-891a-4012-b332-89311cacce89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup config file variables"
      ],
      "metadata": {
        "id": "9wn3sMH9IXKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now we have the `config.env` file, we will use this to provide our environments and parameters.\n",
        "\n",
        "Note there are several parameters that can be changed. Let's start by changing the BASEDIR as below. Also since we already have the model, let's specify that path using the `OUTPUT_DIR`.\n",
        "\n",
        "```\n",
        "BASEDIR = \"/content/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output\"\n",
        "```"
      ],
      "metadata": {
        "id": "ooFqBUY3gS5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the prediction, we are using growing season and pre-growing season information. Thus, we have 8 optical bands, namely `red_before`, `green_before`, `blue_before`, `nir_before`, `red_during`, `green_during`, `blue_during`, and  `nir_during`. In adidition, you can use `USE_ELEVATION` and `USE_S1` config to include the topographic and radar information. Since currently we are not including these, so we won't be settting these config values.\n",
        "\n",
        "Similarly, we are using 256x256 pixels, so let's also change that. In addition, if you want to keep buffer on the export images buffer for prediction purpose, you can use `KERNEL_BUFFER` to specify that. Half this will extend on the sides of each patch. You can specify the size as tupe (e.g. 72 x 72). If zero is used; it will not buffer. I will keep this to zero one this one.\n",
        "\n",
        "```\n",
        "# For model training, USE_ELEVATION extends FEATURES with \"elevation\" & \"slope\"\n",
        "# USE_S1 extends FEATURES with \"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n",
        "# \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"\n",
        "# In case these are not useful and you have other bands in your training data, you can do set\n",
        "# USE_ELEVATION and USE_S1 to False and update FEATURES to include needed bands\n",
        "USE_ELEVATION = False\n",
        "USE_S1 = False\n",
        "\n",
        "PATCH_SHAPE = (256, 256)\n",
        "\n",
        "KERNEL_BUFFER = 0\n",
        "```"
      ],
      "metadata": {
        "id": "CEDUshfUi1OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will specify the `MODEL_DIR_NAME`. The `MODEL_DIR` is then constructed as\n",
        "MODEL_DIR = OUTPUT_DIR / MODEL_DIR_NAME. The `MODEL_DIR_NAME` in my case is `unet_v1`, so we will use that. Similarly, you can specify your output of the file from the prediction using `OUTPUT_NAME` variable. Other config to change are `GCS_PROJECT`, `GCS_BUCKET`, `GCS_IMAGE_DIR`, and `GCS_IMAGE_PREFIX` (for prediction image direction, see this [notebook](https://colab.research.google.com/drive/1MZexam3GZKsQySQO9Jk_RPNyyMLmciEq?usp=drive_link)). For exporting our prediction to the GEE Asset, we will use `EE_OUTPUT_ASSET` to update it.\n",
        "\n",
        "```\n",
        "MODEL_DIR_NAME = \"unet_v1\"\n",
        "OUTPUT_NAME = \"prediction_unet_v1\"\n",
        "GCS_BUCKET = \"dl-book\"\n",
        "\n",
        "# GCS settings\n",
        "GCS_PROJECT = \"servir-ee\"\n",
        "# prediction image directory\n",
        "GCS_IMAGE_DIR = \"chapter-1/images\"\n",
        "# prediction image prefix\n",
        "GCS_IMAGE_PREFIX = \"image_2021\"\n",
        "\n",
        "# where the prediction output will be stored\n",
        "EE_OUTPUT_ASSET = \"projects/servir-ee/assets/dl-book/chapter-1/prediction\"\n",
        "```"
      ],
      "metadata": {
        "id": "stwnTiy94pzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the config file programtically"
      ],
      "metadata": {
        "id": "0p6d8MQcqtO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a dictionary so we can change these config settings programatically."
      ],
      "metadata": {
        "id": "RMA7PH-jvvVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASEDIR = \"/content/\" # @param {type:\"string\"}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output\" # @param {type:\"string\"}\n",
        "# DATADIR = \"datasets/dnn_planet_wo_indices\" # @param {type:\"string\"}\n",
        "# PATCH_SHAPE, USE_ELEVATION, USE_S1\n",
        "# BATCH_SIZE, EPOCHS are converted to their appropriate type.\n",
        "USE_ELEVATION = \"False\" # @param {type:\"string\"}\n",
        "USE_S1 = \"False\" # @param {type:\"string\"}\n",
        "PATCH_SHAPE = \"(256, 256)\" # @param {type:\"string\"}\n",
        "KERNEL_BUFFER = \"0\" # @param {type:\"string\"}\n",
        "\n",
        "MODEL_DIR_NAME = \"unet_v1\" # @param {type:\"string\"}\n",
        "OUTPUT_NAME = \"prediction_unet_v1\" # @param {type:\"string\"}\n",
        "GCS_BUCKET = \"dl-book\" # @param {type:\"string\"}\n",
        "MODEL_TYPE = \"unet\" # @param {type:\"string\"}\n",
        "\n",
        "# GCS settings\n",
        "GCS_PROJECT = \"servir-ee\" # @param {type:\"string\"}\n",
        "# prediction image directory\n",
        "GCS_IMAGE_DIR = \"chapter-1/images\" # @param {type:\"string\"}\n",
        "# prediction image prefix\n",
        "GCS_IMAGE_PREFIX = \"image_2021\" # @param {type:\"string\"}\n",
        "\n",
        "# where the prediction output will be stored\n",
        "EE_OUTPUT_ASSET = \"projects/servir-ee/assets/dl-book/chapter-1/prediction\" # @param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "cFhU_kNQqu2b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_settings = {\n",
        "    \"BASEDIR\" : BASEDIR,\n",
        "    \"OUTPUT_DIR\": OUTPUT_DIR,\n",
        "    \"USE_ELEVATION\": USE_ELEVATION,\n",
        "    \"USE_S1\": USE_S1,\n",
        "    \"PATCH_SHAPE\": PATCH_SHAPE,\n",
        "    \"KERNEL_BUFFER\": KERNEL_BUFFER,\n",
        "    \"MODEL_DIR_NAME\": MODEL_DIR_NAME,\n",
        "    \"OUTPUT_NAME\": OUTPUT_NAME,\n",
        "    \"GCS_PROJECT\": GCS_PROJECT,\n",
        "    \"MODEL_TYPE\": MODEL_TYPE,\n",
        "    \"GCS_BUCKET\": GCS_BUCKET,\n",
        "    \"GCS_IMAGE_DIR\": GCS_IMAGE_DIR,\n",
        "    \"GCS_IMAGE_PREFIX\": GCS_IMAGE_PREFIX,\n",
        "    \"EE_OUTPUT_ASSET\": EE_OUTPUT_ASSET,\n",
        "}\n"
      ],
      "metadata": {
        "id": "j3_k_jmBv0xE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "\n",
        "config_file = \"servir-aces/config.env\"\n",
        "\n",
        "for config_key in config_settings:\n",
        "    dotenv.set_key(dotenv_path=config_file,\n",
        "                   key_to_set=config_key,\n",
        "                   value_to_set=config_settings[config_key]\n",
        "                   )\n"
      ],
      "metadata": {
        "id": "xMAmTIBvvRjW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load config file variables"
      ],
      "metadata": {
        "id": "lZvJKXATIf9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aces import Config, EEUtils\n",
        "\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n"
      ],
      "metadata": {
        "id": "id1pU4aCFjR-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(config_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5_vh3SFFe3n",
        "outputId": "b14199f9-c08b-49f6-e0f7-123c8e579450"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASEDIR: /content\n",
            "DATADIR: /content/data\n",
            "using features: ['red_before', 'green_before', 'blue_before', 'nir_before', 'red_during', 'green_during', 'blue_during', 'nir_during']\n",
            "using labels: ['class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_IMAGE_FILE = str(config.MODEL_DIR / \"prediction\" / f\"{config.OUTPUT_NAME}.TFRecord\")\n",
        "print(f\"OUTPUT_IMAGE_FILE: {OUTPUT_IMAGE_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M26Ze2ZNDP7R",
        "outputId": "5ef05099-aa02-411f-e455-923b5353106b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT_IMAGE_FILE: /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get files for export"
      ],
      "metadata": {
        "id": "GzrIzedTIv9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls = f\"gsutil ls gs://{config.GCS_BUCKET}/{config.GCS_IMAGE_DIR}\"\n",
        "print(f\"ls >> : {ls}\")\n",
        "\n",
        "files_list = subprocess.check_output(ls, shell=True)\n",
        "files_list = files_list.decode(\"utf-8\")\n",
        "files_list = files_list.split(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDJRxeeHDP91",
        "outputId": "a230db6a-3cfa-4e44-c105-57ccfd81b3d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls >> : gsutil ls gs://dl-book/chapter-1/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aCleCW6G1-L",
        "outputId": "0d6ea343-ed6c-4b23-e9fc-bf82991d43ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gs://dl-book/chapter-1/images/',\n",
              " 'gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_2021mixer.json',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get only the files generated by the image export.\n",
        "exported_files_list = [s for s in files_list if config.GCS_IMAGE_PREFIX in s]\n",
        "\n",
        "print(f\"exported_files_list: {exported_files_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12n86MsSIFip",
        "outputId": "d917e136-20da-4dc9-9fa8-5de143d97a11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exported_files_list: ['gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_2021mixer.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the list of image files and the JSON mixer file."
      ],
      "metadata": {
        "id": "2XDPadw0ILJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_files_list = []\n",
        "json_file = None\n",
        "for f in exported_files_list:\n",
        "    if f.endswith(\".tfrecord.gz\"):\n",
        "        image_files_list.append(f)\n",
        "    elif f.endswith(\".json\"):\n",
        "        json_file = f"
      ],
      "metadata": {
        "id": "WET_70R1DQA2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the files are in the right order.\n",
        "image_files_list.sort()"
      ],
      "metadata": {
        "id": "k3GNaHnsG7oV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"image_files_list: {image_files_list}\")\n",
        "\n",
        "print(f\"json_file: {json_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNcSm43FDQDp",
        "outputId": "c45b88e4-88f2-4b5c-d77a-f1e3fee4d691"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_files_list: ['gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz']\n",
            "json_file: gs://dl-book/chapter-1/images/image_2021mixer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading model from {str(config.MODEL_DIR)}/trained-model\")\n",
        "this_model = tf.keras.models.load_model(f\"{str(config.MODEL_DIR)}/trained-model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkrQCptGDQGd",
        "outputId": "ba26f327-61ab-4e72-9d70-63a6d0d30868"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/trained-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "this_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftVjQntTDQJD",
        "outputId": "34a5e9fa-c04a-4ae4-a559-28607f5e99d7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 8)]      0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, None, None, 32)       2336      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, None, None, 32)       128       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, None, None, 32)       0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, None, None, 32)       0         ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " separable_conv2d (Separabl  (None, None, None, 64)       2400      ['activation_1[0][0]']        \n",
            " eConv2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, None, None, 64)       256       ['separable_conv2d[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, None, None, 64)       0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (Separa  (None, None, None, 64)       4736      ['activation_2[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, None, None, 64)       256       ['separable_conv2d_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, None, None, 64)       0         ['batch_normalization_2[0][0]'\n",
            " D)                                                                 ]                             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)       2112      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " add (Add)                   (None, None, None, 64)       0         ['max_pooling2d[0][0]',       \n",
            "                                                                     'conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, None, None, 64)       0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (Separa  (None, None, None, 128)      8896      ['activation_3[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, None, None, 128)      512       ['separable_conv2d_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, None, None, 128)      0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (Separa  (None, None, None, 128)      17664     ['activation_4[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, None, None, 128)      512       ['separable_conv2d_3[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, None, None, 128)      0         ['batch_normalization_4[0][0]'\n",
            " g2D)                                                               ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, None, None, 128)      8320      ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, None, None, 128)      0         ['max_pooling2d_1[0][0]',     \n",
            "                                                                     'conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, None, None, 128)      0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (Separa  (None, None, None, 256)      34176     ['activation_5[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, None, None, 256)      1024      ['separable_conv2d_4[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, None, None, 256)      0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (Separa  (None, None, None, 256)      68096     ['activation_6[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, None, None, 256)      1024      ['separable_conv2d_5[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, None, None, 256)      0         ['batch_normalization_6[0][0]'\n",
            " g2D)                                                               ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, None, None, 256)      33024     ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, None, None, 256)      0         ['max_pooling2d_2[0][0]',     \n",
            "                                                                     'conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, None, None, 256)      0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, None, None, 256)      590080    ['activation_7[0][0]']        \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, None, None, 256)      1024      ['conv2d_transpose[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, None, None, 256)      0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, None, None, 256)      590080    ['activation_8[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, None, None, 256)      1024      ['conv2d_transpose_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSamplin  (None, None, None, 256)      0         ['add_2[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2  (None, None, None, 256)      0         ['batch_normalization_8[0][0]'\n",
            " D)                                                                 ]                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, None, None, 256)      65792     ['up_sampling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, None, None, 256)      0         ['up_sampling2d[0][0]',       \n",
            "                                                                     'conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, None, None, 256)      0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, None, None, 128)      295040    ['activation_9[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, None, None, 128)      512       ['conv2d_transpose_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, None, None, 128)      0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, None, None, 128)      147584    ['activation_10[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, None, None, 128)      512       ['conv2d_transpose_3[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSamplin  (None, None, None, 256)      0         ['add_3[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSamplin  (None, None, None, 128)      0         ['batch_normalization_10[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, None, None, 128)      32896     ['up_sampling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, None, None, 128)      0         ['up_sampling2d_2[0][0]',     \n",
            "                                                                     'conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, None, None, 128)      0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, None, None, 64)       73792     ['activation_11[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, None, None, 64)       256       ['conv2d_transpose_4[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, None, None, 64)       0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2D  (None, None, None, 64)       36928     ['activation_12[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, None, None, 64)       256       ['conv2d_transpose_5[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSamplin  (None, None, None, 128)      0         ['add_4[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSamplin  (None, None, None, 64)       0         ['batch_normalization_12[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, None, None, 64)       8256      ['up_sampling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, None, None, 64)       0         ['up_sampling2d_4[0][0]',     \n",
            "                                                                     'conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, None, None, 64)       0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2D  (None, None, None, 32)       18464     ['activation_13[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, None, None, 32)       128       ['conv2d_transpose_6[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, None, None, 32)       0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2D  (None, None, None, 32)       9248      ['activation_14[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, None, None, 32)       128       ['conv2d_transpose_7[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSamplin  (None, None, None, 64)       0         ['add_5[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSamplin  (None, None, None, 32)       0         ['batch_normalization_14[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, None, None, 32)       2080      ['up_sampling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, None, None, 32)       0         ['up_sampling2d_6[0][0]',     \n",
            "                                                                     'conv2d_7[0][0]']            \n",
            "                                                                                                  \n",
            " final_conv (Conv2D)         (None, None, None, 5)        1445      ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2060997 (7.86 MB)\n",
            "Trainable params: 2057221 (7.85 MB)\n",
            "Non-trainable params: 3776 (14.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get relevant info from the JSON mixer file."
      ],
      "metadata": {
        "id": "Ac6sL0faJmLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat = f\"gsutil cat {json_file}\"\n",
        "read_t = subprocess.check_output(cat, shell=True)\n",
        "read_t = read_t.decode(\"utf-8\")\n",
        "\n",
        "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "mixer = json.loads(read_t)\n",
        "\n",
        "# Get relevant info from the JSON mixer file.\n",
        "patch_width = mixer[\"patchDimensions\"][0]\n",
        "patch_height = mixer[\"patchDimensions\"][1]\n",
        "patches = mixer[\"totalPatches\"]\n",
        "patch_dimensions_flat = [patch_width * patch_height, 1]"
      ],
      "metadata": {
        "id": "o-Ns212ZDQLn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the buffer size for prediction"
      ],
      "metadata": {
        "id": "-tFJcYnpJ04y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.KERNEL_BUFFER:\n",
        "    x_buffer = config.KERNEL_BUFFER[0] // 2\n",
        "    y_buffer = config.KERNEL_BUFFER[1] // 2\n",
        "\n",
        "    buffered_shape = [\n",
        "        config.PATCH_SHAPE[0] + config.KERNEL_BUFFER[0],\n",
        "        config.PATCH_SHAPE[1] + config.KERNEL_BUFFER[1],\n",
        "    ]\n",
        "else:\n",
        "    x_buffer = 0\n",
        "    y_buffer = 0\n",
        "    buffered_shape = config.PATCH_SHAPE\n",
        "\n",
        "print(f\"buffered_shape: {buffered_shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieIGfMCrJ5ka",
        "outputId": "c6f8b400-b49a-4d2a-a312-7da05359dc47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffered_shape: (256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup features"
      ],
      "metadata": {
        "id": "3bXJaIDlKD3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.USE_ELEVATION:\n",
        "    config.FEATURES.extend([\"elevation\", \"slope\"])\n",
        "\n",
        "\n",
        "if config.USE_S1:\n",
        "    config.FEATURES.extend([\"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n",
        "                            \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"])\n",
        "\n",
        "print(f\"Config.FEATURES: {config.FEATURES}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUHbFYADKIhH",
        "outputId": "865da146-e652-470b-d852-3bf6b56795ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config.FEATURES: ['red_before', 'green_before', 'blue_before', 'nir_before', 'red_during', 'green_during', 'blue_during', 'nir_during']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some useful functions"
      ],
      "metadata": {
        "id": "WJv7lBJyKgSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_image(example_proto):\n",
        "    columns = [\n",
        "        tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) for k in config.FEATURES\n",
        "    ]\n",
        "    image_features_dict = dict(zip(config.FEATURES, columns))\n",
        "    return tf.io.parse_single_example(example_proto, image_features_dict)\n",
        "\n",
        "def to_tuple_image(inputs):\n",
        "    inputs_list = [inputs.get(key) for key in config.FEATURES]\n",
        "    stacked = tf.stack(inputs_list, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n"
      ],
      "metadata": {
        "id": "ovVaxx9YKjdP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a dataset"
      ],
      "metadata": {
        "id": "OZOpx1oWz7sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "image_dataset = tf.data.TFRecordDataset(image_files_list, compression_type=\"GZIP\")\n",
        "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
        "image_dataset = image_dataset.map(to_tuple_image).batch(1)\n"
      ],
      "metadata": {
        "id": "bdk8mvK7z_re"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Inference"
      ],
      "metadata": {
        "id": "bk6J6dtV0HKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = this_model.predict(image_dataset, steps=patches, verbose=1)\n",
        "print(f\"predictions shape: {predictions.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngBve5IK0I-j",
        "outputId": "0c721579-118c-4344-ba84-b1a1c2044d8d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "252/252 [==============================] - 20s 57ms/step\n",
            "predictions shape: (252, 256, 256, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Predictions"
      ],
      "metadata": {
        "id": "b0XXHcwW0SZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "Path(OUTPUT_IMAGE_FILE).parent.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Rs8yfBE3wW5o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Writing predictions to {OUTPUT_IMAGE_FILE} ...\")\n",
        "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n",
        "\n",
        "for i, prediction_patch in enumerate(predictions):\n",
        "    if i == 0:\n",
        "        print(f\"Starting with patch {i}...\")\n",
        "        print(f\"predictionPatch: {prediction_patch.shape}\")\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Writing patch {i}...\")\n",
        "\n",
        "    prediction_patch = prediction_patch[\n",
        "        x_buffer: x_buffer+config.PATCH_SHAPE[0],\n",
        "        y_buffer: y_buffer+config.PATCH_SHAPE[1]\n",
        "    ]\n",
        "\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "            \"prediction\": tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(\n",
        "                    value=np.argmax(prediction_patch, axis=-1).flatten())),\n",
        "            \"cropland_etc\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 0:1].flatten())),\n",
        "            \"rice\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 1:2].flatten())),\n",
        "            \"forest\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 2:3].flatten())),\n",
        "            \"urban\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 3:4].flatten())),\n",
        "            \"others_etc\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 4:5].flatten())),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    i += 1\n",
        "\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CSi_UFn0UzA",
        "outputId": "e703f94b-3688-41b6-9b69-dbabfdf6334e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predictions to /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord ...\n",
            "Starting with patch 0...\n",
            "predictionPatch: (256, 256, 5)\n",
            "Writing patch 0...\n",
            "Writing patch 50...\n",
            "Writing patch 100...\n",
            "Writing patch 150...\n",
            "Writing patch 200...\n",
            "Writing patch 250...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Google Earth Engine (GEE)"
      ],
      "metadata": {
        "id": "menoRbWc1WZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have write the prediction to the `OUTPUT_IMAGE_FILE`. You can upload this to GEE for visualization. To do this, you will need to upload to GCP and then to GEE."
      ],
      "metadata": {
        "id": "kVmtfYTe0w5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have proper permission"
      ],
      "metadata": {
        "id": "ek37oamcCbGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "rKnzfd0NXC1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_GCS_PATH = f\"gs://{config.GCS_BUCKET}/chapter-1/prediction/{config.OUTPUT_NAME}.TFRecord\"\n",
        "print(f\"OUTPUT_GCS_PATH: {OUTPUT_GCS_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eyow_AL9R6e",
        "outputId": "4bc9fcb9-174a-4817-f9d3-eb5bbfe10e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT_GCS_PATH: gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload to gcp\n",
        "upload_to_gcp = f'gsutil cp \"{OUTPUT_IMAGE_FILE}\" \"{OUTPUT_GCS_PATH}\"'\n",
        "print(f\"upload_to_gcp: {upload_to_gcp}\")\n",
        "result = subprocess.check_output(upload_to_gcp, shell=True)\n",
        "print(f\"uploading classified image to gcp: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDoyR9hS0U1_",
        "outputId": "ca648a31-1b24-4882-c1cf-365040288aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload_to_gcp: gsutil cp \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord\" \"gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord\"\n",
            "uploading classified image to gcp: b''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will upload this to the GEE asset."
      ],
      "metadata": {
        "id": "QdeLBXDfDyWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.EE_OUTPUT_ASSET, OUTPUT_GCS_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaabEpmuD5q2",
        "outputId": "9c8a6041-c112-4127-b3c2-dbfcc2d2c87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('projects/servir-ee/assets/dl-book/chapter-1/prediction',\n",
              " 'gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have proper permission."
      ],
      "metadata": {
        "id": "F3vVU9lUHZ6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "# ee.Initialize(project=f\"{config.GCS_PROJECT}\")\n",
        "EEUtils.initialize_session(use_highvolume=True, project=config.GCS_PROJECT)\n"
      ],
      "metadata": {
        "id": "UCxT0FLqYboq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!earthengine set_project {config.GCS_PROJECT}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8w9iCirYp5L",
        "outputId": "f79c8c1f-b690-4944-d7bd-afb6145459ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved project id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_image = f\"earthengine upload image --asset_id={config.EE_OUTPUT_ASSET}/{config.OUTPUT_NAME} --pyramiding_policy=mode {OUTPUT_GCS_PATH} {json_file}\"\n",
        "result = subprocess.check_output(upload_image, shell=True)\n",
        "print(f\"uploading classified image to earth engine: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLJOT49o0U7U",
        "outputId": "5f2f2da2-f257-42c7-bef4-15d20c0c12c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uploading classified image to earth engine: b'Started upload task with ID: EC4KXAWNTUH2MFR6YEWTPVE7\\n'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}