{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 NASA\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Rice mapping in Bhutan with U-Net using high resolution satellite imagery\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/SERVIR/servir-aces/blob/main/notebooks/prediction_unet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/SERVIR/servir-aces/blob/main/notebooks/prediction_unet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "</table>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "### This notebook shows an example of using the TFRecord images for prediction using saved U-Net Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zk3R20LkKet"
      },
      "source": [
        "This notebook is also available in this github repo: https://github.com/SERVIR/servir-aces. Navigate to the `notebooks` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup environment"
      ],
      "metadata": {
        "id": "EuQkhSxRmiIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install servir-aces"
      ],
      "metadata": {
        "id": "FuDztFucmhpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f29ee6-077b-40c0-c63d-361cccf96478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting servir-aces\n",
            "  Downloading servir_aces-0.0.6-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from servir-aces) (1.25.2)\n",
            "Requirement already satisfied: tensorflow>=2.9.3 in /usr/local/lib/python3.10/dist-packages (from servir-aces) (2.15.0)\n",
            "Collecting apache-beam>=2.38.0 (from servir-aces)\n",
            "  Downloading apache_beam-2.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: earthengine-api in /usr/local/lib/python3.10/dist-packages (from servir-aces) (0.1.397)\n",
            "Collecting python-dotenv>=1.0.0 (from servir-aces)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from servir-aces) (3.7.1)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (1.62.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (3.0.3)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (24.0)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading pymongo-4.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (676 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (2023.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (4.11.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<15.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam>=2.38.0->servir-aces) (0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (0.36.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.3->servir-aces) (2.15.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.8.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from earthengine-api->servir-aces) (0.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->servir-aces) (3.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.9.3->servir-aces) (0.43.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->servir-aces) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.12.1->earthengine-api->servir-aces) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->earthengine-api->servir-aces) (4.9)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam>=2.38.0->servir-aces) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.38.0->servir-aces) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.38.0->servir-aces) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.38.0->servir-aces) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.38.0->servir-aces) (0.18.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam>=2.38.0->servir-aces)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.38.0->servir-aces) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.38.0->servir-aces) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.38.0->servir-aces) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.38.0->servir-aces) (2024.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.0.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->earthengine-api->servir-aces) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->earthengine-api->servir-aces) (2.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api->servir-aces) (1.63.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->earthengine-api->servir-aces) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api->servir-aces) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.9.3->servir-aces) (3.2.2)\n",
            "Building wheels for collected packages: crcmod, dill, hdfs, pyjsparser, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31408 sha256=5af438cf11a0260c45fcdb74ea69f2ab6a42b59fb9a2b5413feec6cc9105732c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78541 sha256=b6510729f92ef33fe2609c91929f786e6f4d4b2a504fcb012150db55f51e604c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=cefbad269e16992d4af3a1a80924d84cfbb2b51746c1a2bd0a3d27b6a23669f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=30002932a8e3eda426e874abaa83e554dff6af1cdcf3e6dd7fe1cbf4b23b4b97\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=35ead22c6ea4cde12f8752a1f1e233e28003fa713d776acaab2e5287fe3ff66a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built crcmod dill hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, docopt, crcmod, zstandard, python-dotenv, orjson, objsize, js2py, fasteners, fastavro, dnspython, dill, pymongo, hdfs, apache-beam, servir-aces\n",
            "Successfully installed apache-beam-2.55.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 hdfs-2.7.3 js2py-0.74 objsize-0.7.0 orjson-3.10.1 pyjsparser-2.7.1 pymongo-4.6.3 python-dotenv-1.0.1 servir-aces-0.0.6 zstandard-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SERVIR/servir-aces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PvoP9Sgqz4e",
        "outputId": "12f146c4-61fb-4db6-913c-3e742778ca4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'servir-aces'...\n",
            "remote: Enumerating objects: 680, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 680 (delta 64), reused 62 (delta 49), pack-reused 573\u001b[K\n",
            "Receiving objects: 100% (680/680), 898.59 KiB | 11.67 MiB/s, done.\n",
            "Resolving deltas: 100% (384/384), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the repo is downloaded. We will create an environment file file to place point to our training data and customize parameters for the model. To do this, we make a copy of the `.env.example` file provided.\n",
        "\n",
        "Under the hood, all the configuration provided via the environment file are parsed as a config object and can be accessed programatically.\n",
        "\n",
        "Note current version does not expose all the model intracacies through the environment file but future version may include those depending on the need."
      ],
      "metadata": {
        "id": "RukLD6eeq7Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp servir-aces/.env.example servir-aces/config.env"
      ],
      "metadata": {
        "id": "jpnfjXRYsYMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgk98jsYF5xU",
        "outputId": "03e7e478-0482-46d4-fe3d-1149ee928e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup config file variables"
      ],
      "metadata": {
        "id": "9wn3sMH9IXKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now we have the `config.env` file, we will use this to provide our environments and parameters.\n",
        "\n",
        "Note there are several parameters that can be changed. Let's start by changing the BASEDIR as below. Also since we already have the model, let's specify that path using the `OUTPUT_DIR`.\n",
        "\n",
        "```\n",
        "BASEDIR = \"/content/\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output\"\n",
        "```"
      ],
      "metadata": {
        "id": "ooFqBUY3gS5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the prediction, we are using growing season and pre-growing season information. Thus, we have 8 optical bands, namely `red_before`, `green_before`, `blue_before`, `nir_before`, `red_during`, `green_during`, `blue_during`, and  `nir_during`. In adidition, you can use `USE_ELEVATION` and `USE_S1` config to include the topographic and radar information. Since currently we are not including these, so we won't be settting these config values.\n",
        "\n",
        "Similarly, we are using 256x256 pixels, so let's also change that. In addition, if you want to keep buffer on the export images buffer for prediction purpose, you can use `KERNEL_BUFFER` to specify that. Half this will extend on the sides of each patch. You can specify the size as tupe (e.g. 72 x 72). If zero is used; it will not buffer. I will keep this to zero one this one.\n",
        "\n",
        "```\n",
        "# For model training, USE_ELEVATION extends FEATURES with \"elevation\" & \"slope\"\n",
        "# USE_S1 extends FEATURES with \"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n",
        "# \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"\n",
        "# In case these are not useful and you have other bands in your training data, you can do set\n",
        "# USE_ELEVATION and USE_S1 to False and update FEATURES to include needed bands\n",
        "USE_ELEVATION = False\n",
        "USE_S1 = False\n",
        "\n",
        "PATCH_SHAPE = (256, 256)\n",
        "\n",
        "KERNEL_BUFFER = 0\n",
        "```"
      ],
      "metadata": {
        "id": "CEDUshfUi1OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will specify the `MODEL_DIR_NAME`. The `MODEL_DIR` is then constructed as\n",
        "MODEL_DIR = OUTPUT_DIR / MODEL_DIR_NAME. The `MODEL_DIR_NAME` in my case is `unet_v1`, so we will use that. Similarly, you can specify your output of the file from the prediction using `OUTPUT_NAME` variable. Other config to change are `GCS_PROJECT`, `GCS_BUCKET`, `GCS_IMAGE_DIR`, and `GCS_IMAGE_PREFIX` (for prediction image direction, see this [notebook](https://colab.research.google.com/drive/1MZexam3GZKsQySQO9Jk_RPNyyMLmciEq?usp=drive_link)). For exporting our prediction to the GEE Asset, we will use `EE_OUTPUT_ASSET` to update it.\n",
        "\n",
        "```\n",
        "MODEL_DIR_NAME = \"unet_v1\"\n",
        "OUTPUT_NAME = \"prediction_unet_v1\"\n",
        "GCS_BUCKET = \"dl-book\"\n",
        "\n",
        "# GCS settings\n",
        "GCS_PROJECT = \"servir-ee\"\n",
        "# prediction image directory\n",
        "GCS_IMAGE_DIR = \"chapter-1/images\"\n",
        "# prediction image prefix\n",
        "GCS_IMAGE_PREFIX = \"image_2021\"\n",
        "\n",
        "# where the prediction output will be stored\n",
        "EE_OUTPUT_ASSET = \"projects/servir-ee/assets/dl-book/chapter-1/prediction\"\n",
        "```"
      ],
      "metadata": {
        "id": "stwnTiy94pzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the config file programtically"
      ],
      "metadata": {
        "id": "0p6d8MQcqtO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a dictionary so we can change these config settings programatically."
      ],
      "metadata": {
        "id": "RMA7PH-jvvVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASEDIR = \"/content/\" # @param {type:\"string\"}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output\" # @param {type:\"string\"}\n",
        "# DATADIR = \"datasets/dnn_planet_wo_indices\" # @param {type:\"string\"}\n",
        "# PATCH_SHAPE, USE_ELEVATION, USE_S1, TRAIN_SIZE, TEST_SIZE, VAL_SIZE\n",
        "# BATCH_SIZE, EPOCHS are converted to their appropriate type.\n",
        "USE_ELEVATION = \"False\" # @param {type:\"string\"}\n",
        "USE_S1 = \"False\" # @param {type:\"string\"}\n",
        "PATCH_SHAPE = \"(256, 256)\" # @param {type:\"string\"}\n",
        "KERNEL_BUFFER = \"0\" # @param {type:\"string\"}\n",
        "\n",
        "MODEL_DIR_NAME = \"unet_v1\" # @param {type:\"string\"}\n",
        "OUTPUT_NAME = \"prediction_unet_v1\" # @param {type:\"string\"}\n",
        "GCS_BUCKET = \"dl-book\" # @param {type:\"string\"}\n",
        "MODEL_TYPE = \"unet\" # @param {type:\"string\"}\n",
        "\n",
        "# GCS settings\n",
        "GCS_PROJECT = \"servir-ee\" # @param {type:\"string\"}\n",
        "# prediction image directory\n",
        "GCS_IMAGE_DIR = \"chapter-1/images\" # @param {type:\"string\"}\n",
        "# prediction image prefix\n",
        "GCS_IMAGE_PREFIX = \"image_2021\" # @param {type:\"string\"}\n",
        "\n",
        "# where the prediction output will be stored\n",
        "EE_OUTPUT_ASSET = \"projects/servir-ee/assets/dl-book/chapter-1/prediction\" # @param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "cFhU_kNQqu2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_settings = {\n",
        "    \"BASEDIR\" : BASEDIR,\n",
        "    \"OUTPUT_DIR\": OUTPUT_DIR,\n",
        "    \"USE_ELEVATION\": USE_ELEVATION,\n",
        "    \"USE_S1\": USE_S1,\n",
        "    \"PATCH_SHAPE\": PATCH_SHAPE,\n",
        "    \"KERNEL_BUFFER\": KERNEL_BUFFER,\n",
        "    \"MODEL_DIR_NAME\": MODEL_DIR_NAME,\n",
        "    \"OUTPUT_NAME\": OUTPUT_NAME,\n",
        "    \"GCS_PROJECT\": GCS_PROJECT,\n",
        "    \"MODEL_TYPE\": MODEL_TYPE,\n",
        "    \"GCS_BUCKET\": GCS_BUCKET,\n",
        "    \"GCS_IMAGE_DIR\": GCS_IMAGE_DIR,\n",
        "    \"GCS_IMAGE_PREFIX\": GCS_IMAGE_PREFIX,\n",
        "    \"EE_OUTPUT_ASSET\": EE_OUTPUT_ASSET,\n",
        "}\n"
      ],
      "metadata": {
        "id": "j3_k_jmBv0xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "\n",
        "config_file = \"servir-aces/config.env\"\n",
        "\n",
        "for config_key in config_settings:\n",
        "    dotenv.set_key(dotenv_path=config_file,\n",
        "                   key_to_set=config_key,\n",
        "                   value_to_set=config_settings[config_key]\n",
        "                   )\n"
      ],
      "metadata": {
        "id": "xMAmTIBvvRjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load config file variables"
      ],
      "metadata": {
        "id": "lZvJKXATIf9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aces import Config\n",
        "\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n"
      ],
      "metadata": {
        "id": "id1pU4aCFjR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(config_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5_vh3SFFe3n",
        "outputId": "46874201-8cd9-41a7-9ab7-67c48a2af33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASEDIR: /content\n",
            "DATADIR: /content/data\n",
            "using features: ['red_before', 'green_before', 'blue_before', 'nir_before', 'red_during', 'green_during', 'blue_during', 'nir_during']\n",
            "using labels: ['class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_IMAGE_FILE = str(config.MODEL_DIR / \"prediction\" / f\"{config.OUTPUT_NAME}.TFRecord\")\n",
        "print(f\"OUTPUT_IMAGE_FILE: {OUTPUT_IMAGE_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M26Ze2ZNDP7R",
        "outputId": "f1787c99-0ef8-4c01-f88b-d158c4808507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT_IMAGE_FILE: /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get files for export"
      ],
      "metadata": {
        "id": "GzrIzedTIv9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls = f\"gsutil ls gs://{config.GCS_BUCKET}/{config.GCS_IMAGE_DIR}\"\n",
        "print(f\"ls >> : {ls}\")\n",
        "\n",
        "files_list = subprocess.check_output(ls, shell=True)\n",
        "files_list = files_list.decode(\"utf-8\")\n",
        "files_list = files_list.split(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDJRxeeHDP91",
        "outputId": "af42f53c-cf98-43d6-aad5-91f70078eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls >> : gsutil ls gs://dl-book/chapter-1/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aCleCW6G1-L",
        "outputId": "459c2003-1753-430c-994f-92a21a4b8293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gs://dl-book/chapter-1/images/',\n",
              " 'gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz',\n",
              " 'gs://dl-book/chapter-1/images/image_2021mixer.json',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get only the files generated by the image export.\n",
        "exported_files_list = [s for s in files_list if config.GCS_IMAGE_PREFIX in s]\n",
        "\n",
        "print(f\"exported_files_list: {exported_files_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12n86MsSIFip",
        "outputId": "17845cee-67ec-40fa-e454-5bc8b64fbb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exported_files_list: ['gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_2021mixer.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the list of image files and the JSON mixer file."
      ],
      "metadata": {
        "id": "2XDPadw0ILJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_files_list = []\n",
        "json_file = None\n",
        "for f in exported_files_list:\n",
        "    if f.endswith(\".tfrecord.gz\"):\n",
        "        image_files_list.append(f)\n",
        "    elif f.endswith(\".json\"):\n",
        "        json_file = f"
      ],
      "metadata": {
        "id": "WET_70R1DQA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the files are in the right order.\n",
        "image_files_list.sort()"
      ],
      "metadata": {
        "id": "k3GNaHnsG7oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"image_files_list: {image_files_list}\")\n",
        "\n",
        "print(f\"json_file: {json_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNcSm43FDQDp",
        "outputId": "8885177e-0166-4bb9-8063-317d4d77676e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_files_list: ['gs://dl-book/chapter-1/images/image_202100000.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100001.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100002.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100003.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100004.tfrecord.gz', 'gs://dl-book/chapter-1/images/image_202100005.tfrecord.gz']\n",
            "json_file: gs://dl-book/chapter-1/images/image_2021mixer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading model from {str(config.MODEL_DIR)}/trained-model\")\n",
        "this_model = tf.keras.models.load_model(f\"{str(config.MODEL_DIR)}/trained-model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkrQCptGDQGd",
        "outputId": "8a193144-256f-4100-e769-84aeacb0db46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/trained-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "this_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftVjQntTDQJD",
        "outputId": "509bafda-2263-46b7-eabb-7529eb7165f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 8)]      0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, None, None, 32)       2336      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, None, None, 32)       128       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, None, None, 32)       0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, None, None, 32)       0         ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " separable_conv2d (Separabl  (None, None, None, 64)       2400      ['activation_1[0][0]']        \n",
            " eConv2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, None, None, 64)       256       ['separable_conv2d[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, None, None, 64)       0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (Separa  (None, None, None, 64)       4736      ['activation_2[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, None, None, 64)       256       ['separable_conv2d_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, None, None, 64)       0         ['batch_normalization_2[0][0]'\n",
            " D)                                                                 ]                             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)       2112      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " add (Add)                   (None, None, None, 64)       0         ['max_pooling2d[0][0]',       \n",
            "                                                                     'conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, None, None, 64)       0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (Separa  (None, None, None, 128)      8896      ['activation_3[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, None, None, 128)      512       ['separable_conv2d_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, None, None, 128)      0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (Separa  (None, None, None, 128)      17664     ['activation_4[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, None, None, 128)      512       ['separable_conv2d_3[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, None, None, 128)      0         ['batch_normalization_4[0][0]'\n",
            " g2D)                                                               ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, None, None, 128)      8320      ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, None, None, 128)      0         ['max_pooling2d_1[0][0]',     \n",
            "                                                                     'conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, None, None, 128)      0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (Separa  (None, None, None, 256)      34176     ['activation_5[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, None, None, 256)      1024      ['separable_conv2d_4[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, None, None, 256)      0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (Separa  (None, None, None, 256)      68096     ['activation_6[0][0]']        \n",
            " bleConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, None, None, 256)      1024      ['separable_conv2d_5[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, None, None, 256)      0         ['batch_normalization_6[0][0]'\n",
            " g2D)                                                               ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, None, None, 256)      33024     ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, None, None, 256)      0         ['max_pooling2d_2[0][0]',     \n",
            "                                                                     'conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, None, None, 256)      0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, None, None, 256)      590080    ['activation_7[0][0]']        \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, None, None, 256)      1024      ['conv2d_transpose[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, None, None, 256)      0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, None, None, 256)      590080    ['activation_8[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, None, None, 256)      1024      ['conv2d_transpose_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSamplin  (None, None, None, 256)      0         ['add_2[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2  (None, None, None, 256)      0         ['batch_normalization_8[0][0]'\n",
            " D)                                                                 ]                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, None, None, 256)      65792     ['up_sampling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, None, None, 256)      0         ['up_sampling2d[0][0]',       \n",
            "                                                                     'conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, None, None, 256)      0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, None, None, 128)      295040    ['activation_9[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, None, None, 128)      512       ['conv2d_transpose_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, None, None, 128)      0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, None, None, 128)      147584    ['activation_10[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, None, None, 128)      512       ['conv2d_transpose_3[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSamplin  (None, None, None, 256)      0         ['add_3[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSamplin  (None, None, None, 128)      0         ['batch_normalization_10[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, None, None, 128)      32896     ['up_sampling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, None, None, 128)      0         ['up_sampling2d_2[0][0]',     \n",
            "                                                                     'conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, None, None, 128)      0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, None, None, 64)       73792     ['activation_11[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, None, None, 64)       256       ['conv2d_transpose_4[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, None, None, 64)       0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2D  (None, None, None, 64)       36928     ['activation_12[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, None, None, 64)       256       ['conv2d_transpose_5[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSamplin  (None, None, None, 128)      0         ['add_4[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSamplin  (None, None, None, 64)       0         ['batch_normalization_12[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, None, None, 64)       8256      ['up_sampling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, None, None, 64)       0         ['up_sampling2d_4[0][0]',     \n",
            "                                                                     'conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, None, None, 64)       0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2D  (None, None, None, 32)       18464     ['activation_13[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, None, None, 32)       128       ['conv2d_transpose_6[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, None, None, 32)       0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2D  (None, None, None, 32)       9248      ['activation_14[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, None, None, 32)       128       ['conv2d_transpose_7[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSamplin  (None, None, None, 64)       0         ['add_5[0][0]']               \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSamplin  (None, None, None, 32)       0         ['batch_normalization_14[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, None, None, 32)       2080      ['up_sampling2d_7[0][0]']     \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, None, None, 32)       0         ['up_sampling2d_6[0][0]',     \n",
            "                                                                     'conv2d_7[0][0]']            \n",
            "                                                                                                  \n",
            " final_conv (Conv2D)         (None, None, None, 5)        1445      ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2060997 (7.86 MB)\n",
            "Trainable params: 2057221 (7.85 MB)\n",
            "Non-trainable params: 3776 (14.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get relevant info from the JSON mixer file."
      ],
      "metadata": {
        "id": "Ac6sL0faJmLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat = f\"gsutil cat {json_file}\"\n",
        "read_t = subprocess.check_output(cat, shell=True)\n",
        "read_t = read_t.decode(\"utf-8\")\n",
        "\n",
        "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "mixer = json.loads(read_t)\n",
        "\n",
        "# Get relevant info from the JSON mixer file.\n",
        "patch_width = mixer[\"patchDimensions\"][0]\n",
        "patch_height = mixer[\"patchDimensions\"][1]\n",
        "patches = mixer[\"totalPatches\"]\n",
        "patch_dimensions_flat = [patch_width * patch_height, 1]"
      ],
      "metadata": {
        "id": "o-Ns212ZDQLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the buffer size for prediction"
      ],
      "metadata": {
        "id": "-tFJcYnpJ04y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.KERNEL_BUFFER:\n",
        "    x_buffer = config.KERNEL_BUFFER[0] // 2\n",
        "    y_buffer = config.KERNEL_BUFFER[1] // 2\n",
        "\n",
        "    buffered_shape = [\n",
        "        config.PATCH_SHAPE[0] + config.KERNEL_BUFFER[0],\n",
        "        config.PATCH_SHAPE[1] + config.KERNEL_BUFFER[1],\n",
        "    ]\n",
        "else:\n",
        "    x_buffer = 0\n",
        "    y_buffer = 0\n",
        "    buffered_shape = config.PATCH_SHAPE\n",
        "\n",
        "print(f\"buffered_shape: {buffered_shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieIGfMCrJ5ka",
        "outputId": "f8a1b0b1-b9c2-4887-f17b-d0278f1172cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffered_shape: (256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup features"
      ],
      "metadata": {
        "id": "3bXJaIDlKD3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.USE_ELEVATION:\n",
        "    config.FEATURES.extend([\"elevation\", \"slope\"])\n",
        "\n",
        "\n",
        "if config.USE_S1:\n",
        "    config.FEATURES.extend([\"vv_asc_before\", \"vh_asc_before\", \"vv_asc_during\", \"vh_asc_during\",\n",
        "                            \"vv_desc_before\", \"vh_desc_before\", \"vv_desc_during\", \"vh_desc_during\"])\n",
        "\n",
        "print(f\"Config.FEATURES: {config.FEATURES}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUHbFYADKIhH",
        "outputId": "813a79bc-0b31-4caf-f600-8d84ab4654ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config.FEATURES: ['red_before', 'green_before', 'blue_before', 'nir_before', 'red_during', 'green_during', 'blue_during', 'nir_during']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some useful functions"
      ],
      "metadata": {
        "id": "WJv7lBJyKgSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_image(example_proto):\n",
        "    columns = [\n",
        "        tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) for k in config.FEATURES\n",
        "    ]\n",
        "    image_features_dict = dict(zip(config.FEATURES, columns))\n",
        "    return tf.io.parse_single_example(example_proto, image_features_dict)\n",
        "\n",
        "def to_tuple_image(inputs):\n",
        "    inputs_list = [inputs.get(key) for key in config.FEATURES]\n",
        "    stacked = tf.stack(inputs_list, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n"
      ],
      "metadata": {
        "id": "ovVaxx9YKjdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a dataset"
      ],
      "metadata": {
        "id": "OZOpx1oWz7sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "image_dataset = tf.data.TFRecordDataset(image_files_list, compression_type=\"GZIP\")\n",
        "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
        "image_dataset = image_dataset.map(to_tuple_image).batch(1)\n"
      ],
      "metadata": {
        "id": "bdk8mvK7z_re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Inference"
      ],
      "metadata": {
        "id": "bk6J6dtV0HKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = this_model.predict(image_dataset, steps=patches, verbose=1)\n",
        "print(f\"predictions shape: {predictions.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngBve5IK0I-j",
        "outputId": "e1d67088-6756-44c3-e340-7801a5a64a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "252/252 [==============================] - 45s 153ms/step\n",
            "predictions shape: (252, 256, 256, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Predictions"
      ],
      "metadata": {
        "id": "b0XXHcwW0SZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "Path(OUTPUT_IMAGE_FILE).parent.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Rs8yfBE3wW5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Writing predictions to {OUTPUT_IMAGE_FILE} ...\")\n",
        "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n",
        "\n",
        "for i, prediction_patch in enumerate(predictions):\n",
        "    if i == 0:\n",
        "        print(f\"Starting with patch {i}...\")\n",
        "        print(f\"predictionPatch: {prediction_patch.shape}\")\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f\"Writing patch {i}...\")\n",
        "\n",
        "    prediction_patch = prediction_patch[\n",
        "        x_buffer: x_buffer+config.PATCH_SHAPE[0],\n",
        "        y_buffer: y_buffer+config.PATCH_SHAPE[1]\n",
        "    ]\n",
        "\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "            \"prediction\": tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(\n",
        "                    value=np.argmax(prediction_patch, axis=-1).flatten())),\n",
        "            \"cropland_etc\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 0:1].flatten())),\n",
        "            \"rice\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 1:2].flatten())),\n",
        "            \"forest\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 2:3].flatten())),\n",
        "            \"urban\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 3:4].flatten())),\n",
        "            \"others_etc\": tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=prediction_patch[:, :, 4:5].flatten())),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    i += 1\n",
        "\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CSi_UFn0UzA",
        "outputId": "11cf30c3-6396-49ea-9926-721c5a052f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predictions to /content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord ...\n",
            "Starting with patch 0...\n",
            "predictionPatch: (256, 256, 5)\n",
            "Writing patch 0...\n",
            "Writing patch 50...\n",
            "Writing patch 100...\n",
            "Writing patch 150...\n",
            "Writing patch 200...\n",
            "Writing patch 250...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Google Earth Engine (GEE)"
      ],
      "metadata": {
        "id": "menoRbWc1WZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have write the prediction to the `OUTPUT_IMAGE_FILE`. You can upload this to GEE for visualization. To do this, you will need to upload to GCP and then to GEE."
      ],
      "metadata": {
        "id": "kVmtfYTe0w5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have proper permission"
      ],
      "metadata": {
        "id": "ek37oamcCbGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "rKnzfd0NXC1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_GCS_PATH = f\"gs://{config.GCS_BUCKET}/chapter-1/prediction/{config.OUTPUT_NAME}.TFRecord\"\n",
        "print(f\"OUTPUT_GCS_PATH: {OUTPUT_GCS_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eyow_AL9R6e",
        "outputId": "241dffc8-8bd9-4f8d-8ea4-9014d187ce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT_GCS_PATH: gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload to gcp\n",
        "upload_to_gcp = f'gsutil cp \"{OUTPUT_IMAGE_FILE}\" \"{OUTPUT_GCS_PATH}\"'\n",
        "print(f\"upload_to_gcp: {upload_to_gcp}\")\n",
        "result = subprocess.check_output(upload_to_gcp, shell=True)\n",
        "print(f\"uploading classified image to gcp: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDoyR9hS0U1_",
        "outputId": "ad796653-52cd-491a-9d7e-64c863b2c886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload_to_gcp: gsutil cp \"/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord\" \"gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord\"\n",
            "uploading classified image to gcp: b''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will upload this to the GEE asset."
      ],
      "metadata": {
        "id": "QdeLBXDfDyWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.EE_OUTPUT_ASSET, OUTPUT_GCS_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaabEpmuD5q2",
        "outputId": "ab264c5f-b479-4065-d362-43cfc920f857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('projects/servir-ee/assets/dl-book/chapter-1/prediction',\n",
              " 'gs://dl-book/chapter-1/prediction/prediction_unet_v1.TFRecord')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have proper permission."
      ],
      "metadata": {
        "id": "F3vVU9lUHZ6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=f\"{config.GCS_PROJECT}\")"
      ],
      "metadata": {
        "id": "UCxT0FLqYboq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!earthengine set_project {config.GCS_PROJECT}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8w9iCirYp5L",
        "outputId": "e57a8a38-d5bb-4b1c-8b88-0dc8a3a46fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved project id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_image = f\"earthengine upload image --asset_id={config.EE_OUTPUT_ASSET}/{config.OUTPUT_NAME} --pyramiding_policy=mode {OUTPUT_GCS_PATH} {json_file}\"\n",
        "result = subprocess.check_output(upload_image, shell=True)\n",
        "print(f\"uploading classified image to earth engine: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLJOT49o0U7U",
        "outputId": "3f76540f-30df-4435-be4a-f70e8fdb1b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uploading classified image to earth engine: b'Started upload task with ID: XLMWAIQS2LKVZYR3VTUUUXS3\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test (Do not run)"
      ],
      "metadata": {
        "id": "j_OniN5mDpCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get relevant info from the JSON mixer file.\n",
        "affine_transform = mixer[\"projection\"][\"affine\"][\"doubleMatrix\"]\n",
        "patch_dims = mixer[\"patchDimensions\"]\n",
        "patches_per_row = mixer[\"patchesPerRow\"]\n",
        "total_patches = mixer[\"totalPatches\"]"
      ],
      "metadata": {
        "id": "9BWBMiBd0VA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI3DZXi8Po4j"
      },
      "outputs": [],
      "source": [
        "# Path to your TFRecord file\n",
        "tfrecord_file = '/content/drive/MyDrive/Colab Notebooks/DL_Book/Chapter_1/output/unet_v1/prediction/prediction_unet_v1.TFRecord'\n",
        "\n",
        "# Define the feature description for deserialization\n",
        "feature_description = {\n",
        "    # Create a dictionary describing the features.\n",
        "    'prediction': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'cropland_etc': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'rice': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'forest': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'urban': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'others_etc': tf.io.FixedLenFeature([], tf.float32),\n",
        "}\n",
        "\n",
        "def _parse_function(proto):\n",
        "    return tf.io.parse_single_example(proto, feature_description)\n",
        "\n",
        "# Create a dataset from the TFRecord file\n",
        "raw_dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "parsed_dataset = raw_dataset.map(_parse_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal, osr\n",
        "import cv2\n",
        "\n",
        "# Initialize an empty array for the entire image\n",
        "full_image = np.zeros((patch_dims[0] * (total_patches // patches_per_row),\n",
        "                       patch_dims[1] * patches_per_row, 3), dtype=np.uint8)\n",
        "\n",
        "# Iterate over each image in the parsed dataset\n",
        "for i, features in enumerate(parsed_dataset):\n",
        "    img = tf.image.decode_image(features['prediction']).numpy()\n",
        "    row = i // patches_per_row\n",
        "    col = i % patches_per_row\n",
        "    full_image[row * patch_dims[0]:(row + 1) * patch_dims[0],\n",
        "               col * patch_dims[1]:(col + 1) * patch_dims[1]] = img\n",
        "\n",
        "# Create a GeoTIFF file\n",
        "driver = gdal.GetDriverByName('GTiff')\n",
        "outRaster = driver.Create('output.tif', full_image.shape[1], full_image.shape[0], 3, gdal.GDT_Byte)\n",
        "outRaster.SetGeoTransform([affine_transform[2], affine_transform[0], 0,\n",
        "                           affine_transform[5], 0, affine_transform[4]])\n",
        "\n",
        "# Set the projection\n",
        "outRasterSRS = osr.SpatialReference()\n",
        "outRasterSRS.ImportFromEPSG(4326)\n",
        "outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
        "\n",
        "# Write the data\n",
        "outband = outRaster.GetRasterBand(1)\n",
        "outband.WriteArray(full_image[:,:,0])\n",
        "outband = outRaster.GetRasterBand(2)\n",
        "outband.WriteArray(full_image[:,:,1])\n",
        "outband = outRaster.GetRasterBand(3)\n",
        "outband.WriteArray(full_image[:,:,2])\n",
        "\n",
        "# Flush data\n",
        "outRaster.FlushCache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "x5OYxU-j3o8f",
        "outputId": "5a6cd3ea-0a2a-42d6-e53e-a267e8248ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: urban.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-16e5abd9ab0c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate over each image in the parsed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mpatches_per_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    774\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_6_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: urban.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9r1Necj03u4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "j_OniN5mDpCF"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}